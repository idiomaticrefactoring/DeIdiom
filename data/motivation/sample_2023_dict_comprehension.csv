repo_name,file_html,cl,me,stmt_code,parent_code,code,complicate_code,temporary_flag,create_fun_flag,temprorary,func
baserow,https://github.com/bram2w/baserow/tree/master/backend/src/baserow/contrib/database/table/models.py,TableModelQuerySet,order_by_fields_string$136,field_object_dict = {o['field'].name: o for o in self.model._field_objects.values()},field_object_dict = {o['field'].name: o for o in self.model._field_objects.values()},field_object_dict = {o['field'].name: o for o in self.model._field_objects.values()},"field_object_dict = dict()
for o in self.model._field_objects.values():
    field_object_dict[o['field'].name] = o
",0,0,0,0
swift,https://github.com/openstack/swift/tree/master/test/unit/account/test_server.py,TestAccountController,test_GET_multichar_delimiter$1780,"self.assertEqual([{k: v for (k, v) in item.items() if k in ('subdir', 'name')} for item in json.loads(resp.body)], [{'subdir': 'US~~UT~~~'}, {'name': 'US~~UT~~A'}])","[{k: v for (k, v) in item.items() if k in ('subdir', 'name')} for item in json.loads(resp.body)]","[{k: v for (k, v) in item.items() if k in ('subdir', 'name')} for item in json.loads(resp.body)]","def my_comprehension_func(item):
    tmp_DictComp0 = dict()
    for (k, v) in item.items():
        if k in ('subdir', 'name'):
            tmp_DictComp0[k] = v
    return tmp_DictComp0
self.assertEqual([{k: v for (k, v) in item.items() if k in ('subdir', 'name')} for item in json.loads(resp.body)], [{'subdir': 'US~~UT~~~'}, {'name': 'US~~UT~~A'}])",1,1,1,1
evalml,https://github.com/alteryx/evalml/tree/master/evalml/tests/component_tests/test_standard_scaler.py,,test_standard_scaler_woodwork_custom_overrides_returned_by_components$17,"assert {k: type(v) for (k, v) in transformed.ww.logical_types.items()} == {0: Double}","{k: type(v) for (k, v) in transformed.ww.logical_types.items()} == {0: Double}","{k: type(v) for (k, v) in transformed.ww.logical_types.items()} == {0: Double}","tmp_DictComp0 = dict()
for (k, v) in transformed.ww.logical_types.items():
    tmp_DictComp0[k] = type(v)
assert tmp_DictComp0 == {0: Double}",1,0,1,0
fiftyone,https://github.com/voxel51/fiftyone/tree/master/fiftyone/core/dataset.py,,_merge_samples_pipeline$6854,"frame_fields = {k: v for (k, v) in frame_fields.items() if k not in omit_frame_fields}","frame_fields = {k: v for (k, v) in frame_fields.items() if k not in omit_frame_fields}","frame_fields = {k: v for (k, v) in frame_fields.items() if k not in omit_frame_fields}","tmp_DictComp0 = dict()
for (k, v) in frame_fields.items():
    if k not in omit_frame_fields:
        tmp_DictComp0[k] = v
frame_fields = tmp_DictComp0",1,0,1,0
PettingZoo,https://github.com/Farama-Foundation/PettingZoo/tree/master/pettingzoo/classic/hanabi/hanabi.py,raw_env,__init__$231,self.action_spaces = {name: spaces.Discrete(self.hanabi_env.num_moves()) for name in self.agents},self.action_spaces = {name: spaces.Discrete(self.hanabi_env.num_moves()) for name in self.agents},self.action_spaces = {name: spaces.Discrete(self.hanabi_env.num_moves()) for name in self.agents},"tmp_DictComp0 = dict()
for name in self.agents:
    tmp_DictComp0[name] = spaces.Discrete(self.hanabi_env.num_moves())
self.action_spaces = tmp_DictComp0",1,0,1,0
mindmeld,https://github.com/cisco/mindmeld/tree/master/mindmeld/path.py,,get_labeled_query_tree$232,found_pattern = {pattern: False for pattern in patterns} if patterns else {},{pattern: False for pattern in patterns} if patterns else {},{pattern: False for pattern in patterns} if patterns else {},"def my_comprehension_func(patterns):
    tmp_DictComp0 = dict()
    for pattern in patterns:
        tmp_DictComp0[pattern] = False
    return tmp_DictComp0
found_pattern = my_comprehension_func(patterns) if patterns else {}",1,1,1,1
cloud-inquisitor,https://github.com/RiotGames/cloud-inquisitor/tree/master/backend/cloud_inquisitor/data/migrations/versions/cfb0ed4cced9_new_accounts_table.py,,migrate_data$84,account_types = {x['account_type']: x['account_type_id'] for x in conn.execute(text(select_acct_types))},account_types = {x['account_type']: x['account_type_id'] for x in conn.execute(text(select_acct_types))},account_types = {x['account_type']: x['account_type_id'] for x in conn.execute(text(select_acct_types))},"account_types = dict()
for x in conn.execute(text(select_acct_types)):
    account_types[x['account_type']] = x['account_type_id']
",0,0,0,0
taiga-back,https://github.com/taigaio/taiga-back/tree/master/taiga/export_import/services/store.py,,store_epic$485,statuses = {s.name: s.id for s in project.epic_statuses.all()},statuses = {s.name: s.id for s in project.epic_statuses.all()},statuses = {s.name: s.id for s in project.epic_statuses.all()},"statuses = dict()
for s in project.epic_statuses.all():
    statuses[s.name] = s.id
",0,0,0,0
awslambda-psycopg2,https://github.com/jkehler/awslambda-psycopg2/tree/master/with_ssl_support/psycopg2-3.7/extensions.py,,make_dsn$145,"kwargs = {k: v for (k, v) in kwargs.items() if v is not None}","kwargs = {k: v for (k, v) in kwargs.items() if v is not None}","kwargs = {k: v for (k, v) in kwargs.items() if v is not None}","tmp_DictComp0 = dict()
for (k, v) in kwargs.items():
    if v is not None:
        tmp_DictComp0[k] = v
kwargs = tmp_DictComp0",1,0,1,0
edx-platform,https://github.com/edx/edx-platform/tree/master/lms/djangoapps/program_enrollments/management/commands/migrate_saml_uids.py,Command,handle$68,"email_map = {m['email']: {'uid': m['student_key'], 'updated': False, 'counted': False} for m in uid_mappings}","email_map = {m['email']: {'uid': m['student_key'], 'updated': False, 'counted': False} for m in uid_mappings}","email_map = {m['email']: {'uid': m['student_key'], 'updated': False, 'counted': False} for m in uid_mappings}","email_map = dict()
for m in uid_mappings:
    email_map[m['email']] = {'uid': m['student_key'], 'updated': False, 'counted': False}
",0,0,0,0
holoviews,https://github.com/holoviz/holoviews/tree/master/holoviews/tests/plotting/matplotlib/test_spikeplot.py,TestSpikesPlot,test_op_ndoverlay_value$178,"overlay = NdOverlay({color: Spikes(np.arange(i + 2)) for (i, color) in enumerate(colors)}, 'Color').opts('Spikes', color='Color')","NdOverlay({color: Spikes(np.arange(i + 2)) for (i, color) in enumerate(colors)}, 'Color')","NdOverlay({color: Spikes(np.arange(i + 2)) for (i, color) in enumerate(colors)}, 'Color')","tmp_DictComp0 = dict()
for (i, color) in enumerate(colors):
    tmp_DictComp0[color] = Spikes(np.arange(i + 2))
overlay = NdOverlay(tmp_DictComp0, 'Color').opts('Spikes', color='Color')",1,0,1,0
fklearn,https://github.com/nubank/fklearn/tree/master/src/fklearn/training/transformation.py,,custom_transformer$892,return df.assign(**{col: df[col].swifter.apply(transformation_function) for col in columns_to_transform}),**{col: df[col].swifter.apply(transformation_function) for col in columns_to_transform},**{col: df[col].swifter.apply(transformation_function) for col in columns_to_transform},"def my_comprehension_func(df):
    tmp_DictComp0 = dict()
    for col in columns_to_transform:
        tmp_DictComp0[col] = df[col].swifter.apply(transformation_function)
    return tmp_DictComp0
return df.assign(**my_comprehension_func(df))",1,1,1,0
jax,https://github.com/google/jax/tree/master/jax/interpreters/masking.py,Poly,__neg__$164,"return Poly({mon: -coeff for (mon, coeff) in self.items()})","Poly({mon: -coeff for (mon, coeff) in self.items()})","Poly({mon: -coeff for (mon, coeff) in self.items()})","tmp_DictComp0 = dict()
for (mon, coeff) in self.items():
    tmp_DictComp0[mon] = -coeff
return Poly(tmp_DictComp0)",1,0,1,0
dask,https://github.com/dask/dask/tree/master/dask/dataframe/io/tests/test_parquet.py,,test_drill_scheme$1853,"df1 = pd.DataFrame({c: np.random.random(N) for (i, c) in enumerate(['a', 'b', 'c'])})","pd.DataFrame({c: np.random.random(N) for (i, c) in enumerate(['a', 'b', 'c'])})","pd.DataFrame({c: np.random.random(N) for (i, c) in enumerate(['a', 'b', 'c'])})","tmp_DictComp0 = dict()
for (i, c) in enumerate(['a', 'b', 'c']):
    tmp_DictComp0[c] = np.random.random(N)
df1 = pd.DataFrame(tmp_DictComp0)",1,0,1,0
MonkeyType,https://github.com/Instagram/MonkeyType/tree/master/monkeytype/encoding.py,,typed_dict_from_dict$95,"return TypedDict(d['qualname'], {k: type_from_dict(v) for (k, v) in d['elem_types'].items()})","TypedDict(d['qualname'], {k: type_from_dict(v) for (k, v) in d['elem_types'].items()})","TypedDict(d['qualname'], {k: type_from_dict(v) for (k, v) in d['elem_types'].items()})","def my_comprehension_func(d):
    tmp_DictComp0 = dict()
    for (k, v) in d['elem_types'].items():
        tmp_DictComp0[k] = type_from_dict(v)
    return tmp_DictComp0
return TypedDict(d['qualname'], my_comprehension_func(d))",1,1,1,0
holoviews,https://github.com/holoviz/holoviews/tree/master/holoviews/tests/plotting/bokeh/test_overlayplot.py,TestOverlayPlot,test_points_errorbars_text_ndoverlay_categorical_xaxis$174,"overlay = NdOverlay({i: Points(([chr(65 + i)] * 10, np.random.randn(10))) for i in range(5)})","NdOverlay({i: Points(([chr(65 + i)] * 10, np.random.randn(10))) for i in range(5)})","NdOverlay({i: Points(([chr(65 + i)] * 10, np.random.randn(10))) for i in range(5)})","tmp_DictComp0 = dict()
for i in range(5):
    tmp_DictComp0[i] = Points(([chr(65 + i)] * 10, np.random.randn(10)))
overlay = NdOverlay(tmp_DictComp0)",1,0,1,0
sacred,https://github.com/IDSIA/sacred/tree/master/sacred/config/utils.py,,dogmatize$120,"return DogmaticDict({key: dogmatize(val) for (key, val) in obj.items()})","DogmaticDict({key: dogmatize(val) for (key, val) in obj.items()})","DogmaticDict({key: dogmatize(val) for (key, val) in obj.items()})","tmp_DictComp0 = dict()
for (key, val) in obj.items():
    tmp_DictComp0[key] = dogmatize(val)
return DogmaticDict(tmp_DictComp0)",1,0,1,0
scipy,https://github.com/scipy/scipy/tree/master/scipy/cluster/tests/test_disjoint_set.py,,test_subsets$182,expected = {dis[element]: set() for element in dis},expected = {dis[element]: set() for element in dis},expected = {dis[element]: set() for element in dis},"expected = dict()
for element in dis:
    expected[dis[element]] = set()
",0,0,0,0
mimicry,https://github.com/kwotsin/mimicry/tree/master/torch_mimicry/datasets/imagenet/imagenet.py,ImageNet,__init__$43,"self.class_to_idx = {cls: idx for (idx, clss) in enumerate(self.classes) for cls in clss}","self.class_to_idx = {cls: idx for (idx, clss) in enumerate(self.classes) for cls in clss}","self.class_to_idx = {cls: idx for (idx, clss) in enumerate(self.classes) for cls in clss}","tmp_DictComp0 = dict()
for (idx, clss) in enumerate(self.classes):
    for cls in clss:
        tmp_DictComp0[cls] = idx
self.class_to_idx = tmp_DictComp0",1,0,1,0
multi-agent-emergence-environments,https://github.com/openai/multi-agent-emergence-environments/tree/master/ma_policy/ma_policy.py,MAPolicy,__init__$29,"self.pdtypes = {k: make_pdtype(s) for (k, s) in self.ac_space.spaces.items()}","self.pdtypes = {k: make_pdtype(s) for (k, s) in self.ac_space.spaces.items()}","self.pdtypes = {k: make_pdtype(s) for (k, s) in self.ac_space.spaces.items()}","tmp_DictComp0 = dict()
for (k, s) in self.ac_space.spaces.items():
    tmp_DictComp0[k] = make_pdtype(s)
self.pdtypes = tmp_DictComp0",1,0,1,0
altair,https://github.com/altair-viz/altair/tree/master/altair/tests/test_magics.py,,test_vega_magic_pandas_data_renamed$172,"spec = {key: val for (key, val) in VEGA_SPEC.items() if key != 'data'}","spec = {key: val for (key, val) in VEGA_SPEC.items() if key != 'data'}","spec = {key: val for (key, val) in VEGA_SPEC.items() if key != 'data'}","spec = dict()
for (key, val) in VEGA_SPEC.items():
    if key != 'data':
        spec[key] = val
",0,0,0,0
nextcord,https://github.com/nextcord/nextcord/tree/master/nextcord/http.py,HTTPClient,create_event$2392,"payload = {k: v for (k, v) in payload.items() if k in valid_keys}","payload = {k: v for (k, v) in payload.items() if k in valid_keys}","payload = {k: v for (k, v) in payload.items() if k in valid_keys}","tmp_DictComp0 = dict()
for (k, v) in payload.items():
    if k in valid_keys:
        tmp_DictComp0[k] = v
payload = tmp_DictComp0",1,0,1,0
holoviews,https://github.com/holoviz/holoviews/tree/master/holoviews/tests/plotting/bokeh/test_overlayplot.py,TestLegends,test_dynamicmap_legend_updates$334,"hmap = HoloMap({i: Curve([1, 2, 3], label=chr(65 + i + 2)) * Curve([1, 2, 3], label='B') for i in range(3)})","HoloMap({i: Curve([1, 2, 3], label=chr(65 + i + 2)) * Curve([1, 2, 3], label='B') for i in range(3)})","HoloMap({i: Curve([1, 2, 3], label=chr(65 + i + 2)) * Curve([1, 2, 3], label='B') for i in range(3)})","tmp_DictComp0 = dict()
for i in range(3):
    tmp_DictComp0[i] = Curve([1, 2, 3], label=chr(65 + i + 2)) * Curve([1, 2, 3], label='B')
hmap = HoloMap(tmp_DictComp0)",1,0,1,0
maro,https://github.com/microsoft/maro/tree/master/maro/communication/proxy.py,Proxy,_get_peers_list$250,self._onboard_peer_dict[peer_type] = {peer_name: None for peer_name in registered_peers},self._onboard_peer_dict[peer_type] = {peer_name: None for peer_name in registered_peers},self._onboard_peer_dict[peer_type] = {peer_name: None for peer_name in registered_peers},"self._onboard_peer_dict[peer_type] = dict()
for peer_name in registered_peers:
    self._onboard_peer_dict[peer_type][peer_name] = None
",0,0,0,0
ParlAI,https://github.com/facebookresearch/ParlAI/tree/master/parlai/crowdsourcing/projects/multisession_chat/human_eval/compile_results.py,ModelChatResultsCompiler,compile_results$84,"df = df.append({'folder': info_dict['read_folder_name'], 'file_name': info_dict['file_name'], 'worker_id': info_dict['worker'], 'hit_id': info_dict['hit_id'], 'is_incomplete': info_dict['is_incomplete'], 'context_info': info_dict['context_info'], 'initial_data_id': info_dict['initial_task_data'], 'acceptability_violations_0': info_dict['acceptability_violations_0'], 'model_nickname': model_nickname, 'conversation_idx': conversation_idx, 'turn_idx': -1, 'agent_idx': 0, 'text': info_dict['context_info']['observation_for_bot']['text'], **{bucket: '' for bucket in self.problem_buckets}}, ignore_index=True)","{'folder': info_dict['read_folder_name'], 'file_name': info_dict['file_name'], 'worker_id': info_dict['worker'], 'hit_id': info_dict['hit_id'], 'is_incomplete': info_dict['is_incomplete'], 'context_info': info_dict['context_info'], 'initial_data_id': info_dict['initial_task_data'], 'acceptability_violations_0': info_dict['acceptability_violations_0'], 'model_nickname': model_nickname, 'conversation_idx': conversation_idx, 'turn_idx': -1, 'agent_idx': 0, 'text': info_dict['context_info']['observation_for_bot']['text'], **{bucket: '' for bucket in self.problem_buckets}}","{'folder': info_dict['read_folder_name'], 'file_name': info_dict['file_name'], 'worker_id': info_dict['worker'], 'hit_id': info_dict['hit_id'], 'is_incomplete': info_dict['is_incomplete'], 'context_info': info_dict['context_info'], 'initial_data_id': info_dict['initial_task_data'], 'acceptability_violations_0': info_dict['acceptability_violations_0'], 'model_nickname': model_nickname, 'conversation_idx': conversation_idx, 'turn_idx': -1, 'agent_idx': 0, 'text': info_dict['context_info']['observation_for_bot']['text'], **{bucket: '' for bucket in self.problem_buckets}}","tmp_DictComp0 = dict()
for bucket in self.problem_buckets:
    tmp_DictComp0[bucket] = ''
df = df.append({'folder': info_dict['read_folder_name'], 'file_name': info_dict['file_name'], 'worker_id': info_dict['worker'], 'hit_id': info_dict['hit_id'], 'is_incomplete': info_dict['is_incomplete'], 'context_info': info_dict['context_info'], 'initial_data_id': info_dict['initial_task_data'], 'acceptability_violations_0': info_dict['acceptability_violations_0'], 'model_nickname': model_nickname, 'conversation_idx': conversation_idx, 'turn_idx': -1, 'agent_idx': 0, 'text': info_dict['context_info']['observation_for_bot']['text'], **tmp_DictComp0}, ignore_index=True)",1,0,1,0
pyro,https://github.com/pyro-ppl/pyro/tree/master/pyro/infer/mcmc/util.py,,select_samples$778,"samples = {k: v.reshape((-1,) + v.shape[2:]) for (k, v) in samples.items()}","samples = {k: v.reshape((-1,) + v.shape[2:]) for (k, v) in samples.items()}","samples = {k: v.reshape((-1,) + v.shape[2:]) for (k, v) in samples.items()}","tmp_DictComp0 = dict()
for (k, v) in samples.items():
    tmp_DictComp0[k] = v.reshape((-1,) + v.shape[2:])
samples = tmp_DictComp0",1,0,1,0
wagtail,https://github.com/wagtail/wagtail/tree/master/wagtail/admin/tests/test_workflows.py,TestWorkflowsCreateView,test_permissions$216,"full_context = {key: value for context in response.context for (key, value) in context.items()}","full_context = {key: value for context in response.context for (key, value) in context.items()}","full_context = {key: value for context in response.context for (key, value) in context.items()}","full_context = dict()
for context in response.context:
    for (key, value) in context.items():
        full_context[key] = value
",0,0,0,0
PPLM,https://github.com/uber-research/PPLM/tree/master/paper_code/pytorch_pretrained_bert/tokenization_gpt2.py,GPT2Tokenizer,set_special_tokens$167,"self.special_tokens_decoder = {v: k for (k, v) in self.special_tokens.items()}","self.special_tokens_decoder = {v: k for (k, v) in self.special_tokens.items()}","self.special_tokens_decoder = {v: k for (k, v) in self.special_tokens.items()}","tmp_DictComp0 = dict()
for (k, v) in self.special_tokens.items():
    tmp_DictComp0[v] = k
self.special_tokens_decoder = tmp_DictComp0",1,0,1,0
evalml,https://github.com/alteryx/evalml/tree/master/evalml/pipelines/components/transformers/preprocessing/featuretools.py,DFSTransformer,fit$45,X_ww = X_ww.ww.rename({col: str(col) for col in X_ww.columns}),X_ww.ww.rename({col: str(col) for col in X_ww.columns}),X_ww.ww.rename({col: str(col) for col in X_ww.columns}),"def my_comprehension_func(X_ww):
    tmp_DictComp0 = dict()
    for col in X_ww.columns:
        tmp_DictComp0[col] = str(col)
    return tmp_DictComp0
X_ww = X_ww.ww.rename(my_comprehension_func(X_ww))",1,1,1,0
mara-pipelines,https://github.com/mara/mara-pipelines/tree/master/mara_pipelines/incremental_processing/processed_files.py,,already_processed_files$45,return {row[0]: row[1] for row in cursor.fetchall()},return {row[0]: row[1] for row in cursor.fetchall()},return {row[0]: row[1] for row in cursor.fetchall()},"tmp_DictComp0 = dict()
for row in cursor.fetchall():
    tmp_DictComp0[row[0]] = row[1]
return tmp_DictComp0",1,0,1,0
mlrun,https://github.com/mlrun/mlrun/tree/master/mlrun/frameworks/_common/mlrun_interface.py,MLRunInterface,_get_function_argument$417,"func_parameters = {parameter_name: i for (i, parameter_name) in enumerate(inspect.signature(func).parameters.keys())}","func_parameters = {parameter_name: i for (i, parameter_name) in enumerate(inspect.signature(func).parameters.keys())}","func_parameters = {parameter_name: i for (i, parameter_name) in enumerate(inspect.signature(func).parameters.keys())}","func_parameters = dict()
for (i, parameter_name) in enumerate(inspect.signature(func).parameters.keys()):
    func_parameters[parameter_name] = i
",0,0,0,0
sparseml,https://github.com/neuralmagic/sparseml/tree/master/src/sparseml/pytorch/utils/helpers.py,,tensors_to_device$267,"return {key: tensors_to_device(tens, device) for (key, tens) in tensors.items()}","return {key: tensors_to_device(tens, device) for (key, tens) in tensors.items()}","return {key: tensors_to_device(tens, device) for (key, tens) in tensors.items()}","tmp_DictComp0 = dict()
for (key, tens) in tensors.items():
    tmp_DictComp0[key] = tensors_to_device(tens, device)
return tmp_DictComp0",1,0,1,0
elasticsearch-dsl-py,https://github.com/elastic/elasticsearch-dsl-py/tree/master/elasticsearch_dsl/response/aggs.py,BucketData,buckets$57,bs = AttrDict({k: self._wrap_bucket(bs[k]) for k in bs}),AttrDict({k: self._wrap_bucket(bs[k]) for k in bs}),AttrDict({k: self._wrap_bucket(bs[k]) for k in bs}),"tmp_DictComp0 = dict()
for k in bs:
    tmp_DictComp0[k] = self._wrap_bucket(bs[k])
bs = AttrDict(tmp_DictComp0)",1,0,1,0
python-slack-sdk,https://github.com/slackapi/python-slack-sdk/tree/master/slack/web/async_internal_utils.py,,_build_req_args$89,"files = {k: v for (k, v) in files.items() if v is not None}","files = {k: v for (k, v) in files.items() if v is not None}","files = {k: v for (k, v) in files.items() if v is not None}","tmp_DictComp0 = dict()
for (k, v) in files.items():
    if v is not None:
        tmp_DictComp0[k] = v
files = tmp_DictComp0",1,0,1,0
neural-doodle,https://github.com/alexjc/neural-doodle/tree/master//doodle.py,NeuralGenerator,prepare_optimization$364,self.matcher_inputs = {self.model.network['dup' + l]: self.matcher_tensors[l] for l in self.style_layers},self.matcher_inputs = {self.model.network['dup' + l]: self.matcher_tensors[l] for l in self.style_layers},self.matcher_inputs = {self.model.network['dup' + l]: self.matcher_tensors[l] for l in self.style_layers},"tmp_DictComp0 = dict()
for l in self.style_layers:
    tmp_DictComp0[self.model.network['dup' + l]] = self.matcher_tensors[l]
self.matcher_inputs = tmp_DictComp0",1,0,1,0
pycolab,https://github.com/deepmind/pycolab/tree/master/pycolab/ascii_art.py,,ascii_art_to_game$31,update_group_for.update({character: group_id for character in update_group}),update_group_for.update({character: group_id for character in update_group}),update_group_for.update({character: group_id for character in update_group}),"tmp_DictComp0 = dict()
for character in update_group:
    tmp_DictComp0[character] = group_id
update_group_for.update(tmp_DictComp0)",1,0,1,0
nlp-recipes,https://github.com/microsoft/nlp-recipes/tree/master/utils_nlp/eval/question_answering.py,,evaluate_qa$210,"qid_to_has_ans = {qa_id: bool(ans) for (qa_id, ans) in zip(qa_ids, actuals)}","qid_to_has_ans = {qa_id: bool(ans) for (qa_id, ans) in zip(qa_ids, actuals)}","qid_to_has_ans = {qa_id: bool(ans) for (qa_id, ans) in zip(qa_ids, actuals)}","qid_to_has_ans = dict()
for (qa_id, ans) in zip(qa_ids, actuals):
    qid_to_has_ans[qa_id] = bool(ans)
",0,0,0,0
deep-person-reid,https://github.com/KaiyangZhou/deep-person-reid/tree/master/torchreid/models/resnet_ibn_a.py,,init_pretrained_weights$267,"pretrain_dict = {k: v for (k, v) in pretrain_dict.items() if k in model_dict and model_dict[k].size() == v.size()}","pretrain_dict = {k: v for (k, v) in pretrain_dict.items() if k in model_dict and model_dict[k].size() == v.size()}","pretrain_dict = {k: v for (k, v) in pretrain_dict.items() if k in model_dict and model_dict[k].size() == v.size()}","tmp_DictComp0 = dict()
for (k, v) in pretrain_dict.items():
    if k in model_dict and model_dict[k].size() == v.size():
        tmp_DictComp0[k] = v
pretrain_dict = tmp_DictComp0",1,0,1,0
indico,https://github.com/indico/indico/tree/master/indico/modules/events/registration/clone.py,RegistrationFormCloner,run$38,"new_form = RegistrationForm(**{attr: getattr(old_form, attr) for attr in attrs})","**{attr: getattr(old_form, attr) for attr in attrs}","**{attr: getattr(old_form, attr) for attr in attrs}","tmp_DictComp0 = dict()
for attr in attrs:
    tmp_DictComp0[attr] = getattr(old_form, attr)
new_form = RegistrationForm(**tmp_DictComp0)",1,0,1,0
qlib,https://github.com/microsoft/qlib/tree/master/qlib/contrib/report/graph.py,BaseGraph,_init_parameters$58,self._name_dict = {_item: _item for _item in self._df.columns},self._name_dict = {_item: _item for _item in self._df.columns},self._name_dict = {_item: _item for _item in self._df.columns},"tmp_DictComp0 = dict()
for _item in self._df.columns:
    tmp_DictComp0[_item] = _item
self._name_dict = tmp_DictComp0",1,0,1,0
PhiFlow,https://github.com/tum-pbs/PhiFlow/tree/master/phi/vis/_user_namespace.py,ModuleNamespace,list_variables$76,"variables = {n: v for (n, v) in variables.items() if is_value(v) or inspect.getmodule(v) == self.module}","variables = {n: v for (n, v) in variables.items() if is_value(v) or inspect.getmodule(v) == self.module}","variables = {n: v for (n, v) in variables.items() if is_value(v) or inspect.getmodule(v) == self.module}","tmp_DictComp0 = dict()
for (n, v) in variables.items():
    if is_value(v) or inspect.getmodule(v) == self.module:
        tmp_DictComp0[n] = v
variables = tmp_DictComp0",1,0,1,0
monoid,https://github.com/larsenwork/monoid/tree/master/Scripts/fontbuilder.py,,Variation$169,sfnt_dict = {sfnt[1]: sfnt[2] for sfnt in fnt.sfnt_names if sfnt[0] == 'English (US)'},sfnt_dict = {sfnt[1]: sfnt[2] for sfnt in fnt.sfnt_names if sfnt[0] == 'English (US)'},sfnt_dict = {sfnt[1]: sfnt[2] for sfnt in fnt.sfnt_names if sfnt[0] == 'English (US)'},"sfnt_dict = dict()
for sfnt in fnt.sfnt_names:
    if sfnt[0] == 'English (US)':
        sfnt_dict[sfnt[1]] = sfnt[2]
",0,0,0,0
indico,https://github.com/indico/indico/tree/master/indico/util/suggestions.py,,get_category_scores$108,"return {categ: _get_category_score(user, categ, events, debug) for (categ, events) in categ_events.items()}","return {categ: _get_category_score(user, categ, events, debug) for (categ, events) in categ_events.items()}","return {categ: _get_category_score(user, categ, events, debug) for (categ, events) in categ_events.items()}","tmp_DictComp0 = dict()
for (categ, events) in categ_events.items():
    tmp_DictComp0[categ] = _get_category_score(user, categ, events, debug)
return tmp_DictComp0",1,0,1,0
sac,https://github.com/haarnoja/sac/tree/master/examples/mujoco_all_sac_lsp_hierarchy.py,,run_experiment$250,"env_args = {name.replace('env_', '', 1): value for (name, value) in variant.items() if name.startswith('env_') and name != 'env_name'}","env_args = {name.replace('env_', '', 1): value for (name, value) in variant.items() if name.startswith('env_') and name != 'env_name'}","env_args = {name.replace('env_', '', 1): value for (name, value) in variant.items() if name.startswith('env_') and name != 'env_name'}","env_args = dict()
for (name, value) in variant.items():
    if name.startswith('env_') and name != 'env_name':
        env_args[name.replace('env_', '', 1)] = value
",0,0,0,0
bindsnet,https://github.com/BindsNET/bindsnet/tree/master/bindsnet/network/monitors.py,NetworkMonitor,reset_state_variables$260,self.recording = {k: {} for k in self.layers + self.connections},self.recording = {k: {} for k in self.layers + self.connections},self.recording = {k: {} for k in self.layers + self.connections},"tmp_DictComp0 = dict()
for k in self.layers + self.connections:
    tmp_DictComp0[k] = {}
self.recording = tmp_DictComp0",1,0,1,0
PhiFlow,https://github.com/tum-pbs/PhiFlow/tree/master/tests/release/plasma/numpy_reference.py,Namespace,__add__$37,"return Namespace({key: other[key] + val for (key, val) in self.items()})","Namespace({key: other[key] + val for (key, val) in self.items()})","Namespace({key: other[key] + val for (key, val) in self.items()})","tmp_DictComp0 = dict()
for (key, val) in self.items():
    tmp_DictComp0[key] = other[key] + val
return Namespace(tmp_DictComp0)",1,0,1,0
zipline,https://github.com/quantopian/zipline/tree/master/tests/pipeline/test_events.py,EventsLoaderTestCase,test_load_with_trading_calendar$428,"results = self.engine.run_pipeline(Pipeline({c.name: c.latest for c in EventDataSet_US.columns}), start_date=self.trading_days[0], end_date=self.trading_days[-1])",Pipeline({c.name: c.latest for c in EventDataSet_US.columns}),Pipeline({c.name: c.latest for c in EventDataSet_US.columns}),"tmp_DictComp0 = dict()
for c in EventDataSet_US.columns:
    tmp_DictComp0[c.name] = c.latest
results = self.engine.run_pipeline(Pipeline(tmp_DictComp0), start_date=self.trading_days[0], end_date=self.trading_days[-1])",1,0,1,0
Dryvo,https://github.com/AdamGold/Dryvo/tree/master/server/api/database/models/appointment.py,Appointment,update_only_changed_fields$80,"args = {k: v for (k, v) in kwargs.items() if v or isinstance(v, bool)}","args = {k: v for (k, v) in kwargs.items() if v or isinstance(v, bool)}","args = {k: v for (k, v) in kwargs.items() if v or isinstance(v, bool)}","args = dict()
for (k, v) in kwargs.items():
    if v or isinstance(v, bool):
        args[k] = v
",0,0,0,0
audio,https://github.com/pytorch/audio/tree/master/examples/hubert/utils/common_utils.py,,_get_id2label$99,"return {i: char.lower() for (i, char) in enumerate(labels)}","return {i: char.lower() for (i, char) in enumerate(labels)}","return {i: char.lower() for (i, char) in enumerate(labels)}","tmp_DictComp0 = dict()
for (i, char) in enumerate(labels):
    tmp_DictComp0[i] = char.lower()
return tmp_DictComp0",1,0,1,0
horizon,https://github.com/openstack/horizon/tree/master/openstack_dashboard/api/glance.py,Image,to_dict$97,"image_dict['properties'] = {k: self._apiresource[k] for k in self._apiresource if self.property_visible(k, show_ext_attrs=show_ext_attrs)}","image_dict['properties'] = {k: self._apiresource[k] for k in self._apiresource if self.property_visible(k, show_ext_attrs=show_ext_attrs)}","image_dict['properties'] = {k: self._apiresource[k] for k in self._apiresource if self.property_visible(k, show_ext_attrs=show_ext_attrs)}","image_dict['properties'] = dict()
for k in self._apiresource:
    if self.property_visible(k, show_ext_attrs=show_ext_attrs):
        image_dict['properties'][k] = self._apiresource[k]
",0,0,0,0
Ultra-Light-Fast-Generic-Face-Detector-1MB,https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB/tree/master/caffe/onnx2caffe/_graph.py,Graph,from_onnx$168,input_tensors = {t.name: numpy_helper.to_array(t) for t in graph.initializer},input_tensors = {t.name: numpy_helper.to_array(t) for t in graph.initializer},input_tensors = {t.name: numpy_helper.to_array(t) for t in graph.initializer},"input_tensors = dict()
for t in graph.initializer:
    input_tensors[t.name] = numpy_helper.to_array(t)
",0,0,0,0
causalml,https://github.com/uber/causalml/tree/master/causalml/inference/meta/tlearner.py,BaseTLearner,fit$64,self.models_t = {group: deepcopy(self.model_t) for group in self.t_groups},self.models_t = {group: deepcopy(self.model_t) for group in self.t_groups},self.models_t = {group: deepcopy(self.model_t) for group in self.t_groups},"tmp_DictComp0 = dict()
for group in self.t_groups:
    tmp_DictComp0[group] = deepcopy(self.model_t)
self.models_t = tmp_DictComp0",1,0,1,0
great_expectations,https://github.com/great-expectations/great_expectations/tree/master/great_expectations/core/util.py,,spark_restart_required$776,"current_spark_config_dict: dict = {k: v for (k, v) in current_spark_config}","current_spark_config_dict: dict = {k: v for (k, v) in current_spark_config}","current_spark_config_dict: dict = {k: v for (k, v) in current_spark_config}","current_spark_config_dict = dict()
for (k, v) in current_spark_config:
    current_spark_config_dict[k] = v
",0,0,0,0
nameko,https://github.com/nameko/nameko/tree/master/nameko/messaging.py,HeaderEncoder,get_message_headers$41,"headers = {self._get_header_name(key): value for (key, value) in data.items() if value is not None}","headers = {self._get_header_name(key): value for (key, value) in data.items() if value is not None}","headers = {self._get_header_name(key): value for (key, value) in data.items() if value is not None}","headers = dict()
for (key, value) in data.items():
    if value is not None:
        headers[self._get_header_name(key)] = value
",0,0,0,0
mmgeneration,https://github.com/open-mmlab/mmgeneration/tree/master/mmgen/core/evaluation/metrics.py,Metric,feed$383,"batch_to_feed = {k: v[:end, ...] for (k, v) in batch.items()}","batch_to_feed = {k: v[:end, ...] for (k, v) in batch.items()}","batch_to_feed = {k: v[:end, ...] for (k, v) in batch.items()}","batch_to_feed = dict()
for (k, v) in batch.items():
    batch_to_feed[k] = v[:end, ...]
",0,0,0,0
indico,https://github.com/indico/indico/tree/master/indico/modules/events/abstracts/clone.py,AbstractSettingsCloner,run$39,"self._contrib_type_id_map = {old.id: new.id for (old, new) in shared_data['contribution_types']['contrib_type_map'].items()}","self._contrib_type_id_map = {old.id: new.id for (old, new) in shared_data['contribution_types']['contrib_type_map'].items()}","self._contrib_type_id_map = {old.id: new.id for (old, new) in shared_data['contribution_types']['contrib_type_map'].items()}","self._contrib_type_id_map = dict()
for (old, new) in shared_data['contribution_types']['contrib_type_map'].items():
    self._contrib_type_id_map[old.id] = new.id
",0,0,0,0
pupil,https://github.com/pupil-labs/pupil/tree/master/pupil_src/shared_modules/gl_utils/utils.py,GLFWErrorReporting,error_code_handling$350,new_reporting.update({err_code: 'warn' for err_code in warn or ()}),new_reporting.update({err_code: 'warn' for err_code in warn or ()}),new_reporting.update({err_code: 'warn' for err_code in warn or ()}),"tmp_DictComp0 = dict()
for err_code in warn or ():
    tmp_DictComp0[err_code] = 'warn'
new_reporting.update(tmp_DictComp0)",1,0,1,0
pint,https://github.com/hgrecco/pint/tree/master/pint/util.py,,solve_dependencies$242,"dependencies = {k: v - t for (k, v) in dependencies.items() if v}","dependencies = {k: v - t for (k, v) in dependencies.items() if v}","dependencies = {k: v - t for (k, v) in dependencies.items() if v}","tmp_DictComp0 = dict()
for (k, v) in dependencies.items():
    if v:
        tmp_DictComp0[k] = v - t
dependencies = tmp_DictComp0",1,0,1,0
tvm,https://github.com/apache/tvm/tree/master/tests/python/relay/test_pass_fold_scale_axis.py,,check$782,type_dict = {x.name_hint: x.checked_type for x in y1.params},type_dict = {x.name_hint: x.checked_type for x in y1.params},type_dict = {x.name_hint: x.checked_type for x in y1.params},"type_dict = dict()
for x in y1.params:
    type_dict[x.name_hint] = x.checked_type
",0,0,0,0
keops,https://github.com/getkeops/keops/tree/master/pykeops/torch/kernel_product/kernels.py,Kernel,__init__$236,"var_to_ind = {k: i for (i, k) in enumerate(variables)}","var_to_ind = {k: i for (i, k) in enumerate(variables)}","var_to_ind = {k: i for (i, k) in enumerate(variables)}","var_to_ind = dict()
for (i, k) in enumerate(variables):
    var_to_ind[k] = i
",0,0,0,0
PathPlanning,https://github.com/zhm-real/PathPlanning/tree/master/Sampling_based_Planning/rrt_3D/BIT_star3D.py,BIT_star,BestInQueue$238,V = {state: self.g_T(state) + self.h_hat(state) for state in self.QV},V = {state: self.g_T(state) + self.h_hat(state) for state in self.QV},V = {state: self.g_T(state) + self.h_hat(state) for state in self.QV},"V = dict()
for state in self.QV:
    V[state] = self.g_T(state) + self.h_hat(state)
",0,0,0,0
SMARTS,https://github.com/huawei-noah/SMARTS/tree/master/examples/rllib.py,,main$91,"tune_config = {'env': RLlibHiWayEnv, 'log_level': 'WARN', 'num_workers': num_workers, 'env_config': {'seed': tune.sample_from(lambda spec: random.randint(0, 300)), 'scenarios': [str(Path(scenario).expanduser().resolve().absolute())], 'headless': headless, 'agent_specs': {f'AGENT-{i}': rllib_agent['agent_spec'] for i in range(num_agents)}}, 'multiagent': {'policies': rllib_policies}, 'callbacks': Callbacks}","{'seed': tune.sample_from(lambda spec: random.randint(0, 300)), 'scenarios': [str(Path(scenario).expanduser().resolve().absolute())], 'headless': headless, 'agent_specs': {f'AGENT-{i}': rllib_agent['agent_spec'] for i in range(num_agents)}}","{'seed': tune.sample_from(lambda spec: random.randint(0, 300)), 'scenarios': [str(Path(scenario).expanduser().resolve().absolute())], 'headless': headless, 'agent_specs': {f'AGENT-{i}': rllib_agent['agent_spec'] for i in range(num_agents)}}","tmp_DictComp0 = dict()
for i in range(num_agents):
    tmp_DictComp0[f'AGENT-{i}'] = rllib_agent['agent_spec']
tune_config = {'env': RLlibHiWayEnv, 'log_level': 'WARN', 'num_workers': num_workers, 'env_config': {'seed': tune.sample_from(lambda spec: random.randint(0, 300)), 'scenarios': [str(Path(scenario).expanduser().resolve().absolute())], 'headless': headless, 'agent_specs': tmp_DictComp0}, 'multiagent': {'policies': rllib_policies}, 'callbacks': Callbacks}",1,0,1,0
transform,https://github.com/tensorflow/transform/tree/master/tensorflow_transform/saved/saved_transform_io_v2.py,SavedModelLoader,_apply_v2_transform_model$303,return {key: transformed_features[key] for key in fetches_keys},return {key: transformed_features[key] for key in fetches_keys},return {key: transformed_features[key] for key in fetches_keys},"tmp_DictComp0 = dict()
for key in fetches_keys:
    tmp_DictComp0[key] = transformed_features[key]
return tmp_DictComp0",1,0,1,0
localstack,https://github.com/localstack/localstack/tree/master/tests/unit/test_lambda.py,TestLambdaAPI,test_update_configuration$861,"subset = {k: v for (k, v) in response.items() if k in expected_response.keys()}","subset = {k: v for (k, v) in response.items() if k in expected_response.keys()}","subset = {k: v for (k, v) in response.items() if k in expected_response.keys()}","subset = dict()
for (k, v) in response.items():
    if k in expected_response.keys():
        subset[k] = v
",0,0,0,0
holoviews,https://github.com/holoviz/holoviews/tree/master/holoviews/core/data/spatialpandas.py,SpatialPandasInterface,split$387,d.update({dim.name: row[dim.name] for dim in value_dims}),d.update({dim.name: row[dim.name] for dim in value_dims}),d.update({dim.name: row[dim.name] for dim in value_dims}),"tmp_DictComp0 = dict()
for dim in value_dims:
    tmp_DictComp0[dim.name] = row[dim.name]
d.update(tmp_DictComp0)",1,0,1,0
sentry,https://github.com/getsentry/sentry/tree/master/src/sentry/api/serializers/models/apitoken.py,ApiTokenSerializer,get_attrs$7,"apps = {d['id']: d for d in serialize({i.application for i in item_list if i.application_id}, user)}","apps = {d['id']: d for d in serialize({i.application for i in item_list if i.application_id}, user)}","apps = {d['id']: d for d in serialize({i.application for i in item_list if i.application_id}, user)}","apps = dict()
for d in serialize({i.application for i in item_list if i.application_id}, user):
    apps[d['id']] = d
",0,0,0,0
keras,https://github.com/keras-team/keras/tree/master/keras/callbacks_v1.py,TensorBoard,set_model$224,embeddings_metadata = {layer_name: self.embeddings_metadata for layer_name in embeddings_vars.keys()},embeddings_metadata = {layer_name: self.embeddings_metadata for layer_name in embeddings_vars.keys()},embeddings_metadata = {layer_name: self.embeddings_metadata for layer_name in embeddings_vars.keys()},"embeddings_metadata = dict()
for layer_name in embeddings_vars.keys():
    embeddings_metadata[layer_name] = self.embeddings_metadata
",0,0,0,0
clif,https://github.com/google/clif/tree/master/clif/python/pyext.py,Module,__init__$121,self.typemap = {t.lang_type: t.postconversion for t in typemap if t.postconversion},self.typemap = {t.lang_type: t.postconversion for t in typemap if t.postconversion},self.typemap = {t.lang_type: t.postconversion for t in typemap if t.postconversion},"self.typemap = dict()
for t in typemap:
    if t.postconversion:
        self.typemap[t.lang_type] = t.postconversion
",0,0,0,0
PaddleHub,https://github.com/PaddlePaddle/PaddleHub/tree/master/modules/video/multiple_object_tracking/jde_darknet53/dataset.py,MOTVideoStreamReader,to_tensor$246,batch = {key: self.to_tensor(batch[key]) for key in batch},batch = {key: self.to_tensor(batch[key]) for key in batch},batch = {key: self.to_tensor(batch[key]) for key in batch},"tmp_DictComp0 = dict()
for key in batch:
    tmp_DictComp0[key] = self.to_tensor(batch[key])
batch = tmp_DictComp0",1,0,1,0
sentry,https://github.com/getsentry/sentry/tree/master/tests/sentry/integrations/gitlab/test_integration.py,GitlabIntegrationTest,assert_setup_flow$38,"authorize_params = {k: v[0] for (k, v) in params.items()}","authorize_params = {k: v[0] for (k, v) in params.items()}","authorize_params = {k: v[0] for (k, v) in params.items()}","authorize_params = dict()
for (k, v) in params.items():
    authorize_params[k] = v[0]
",0,0,0,0
pretix,https://github.com/pretix/pretix/tree/master/src/pretix/base/models/orders.py,AbstractPosition,cache_answers$1336,question_cache = {q.pk: q for q in questions},question_cache = {q.pk: q for q in questions},question_cache = {q.pk: q for q in questions},"question_cache = dict()
for q in questions:
    question_cache[q.pk] = q
",0,0,0,0
jiant,https://github.com/nyu-mll/jiant/tree/master/jiant/proj/main/modeling/primary.py,JiantAlbertModel,get_mlm_weights_dict$442,"mlm_weights_dict = {strings.remove_prefix(k, 'predictions.'): v for (k, v) in weights_dict.items()}","mlm_weights_dict = {strings.remove_prefix(k, 'predictions.'): v for (k, v) in weights_dict.items()}","mlm_weights_dict = {strings.remove_prefix(k, 'predictions.'): v for (k, v) in weights_dict.items()}","mlm_weights_dict = dict()
for (k, v) in weights_dict.items():
    mlm_weights_dict[strings.remove_prefix(k, 'predictions.')] = v
",0,0,0,0
djongo,https://github.com/nesdis/djongo/tree/master/tests/django_tests/tests/v21/tests/model_formsets_regress/tests.py,FormfieldShouldDeleteFormTests,test_all_delete$451,"data.update({'form-%d-id' % i: user.pk for (i, user) in enumerate(User.objects.all())})","data.update({'form-%d-id' % i: user.pk for (i, user) in enumerate(User.objects.all())})","data.update({'form-%d-id' % i: user.pk for (i, user) in enumerate(User.objects.all())})","tmp_DictComp0 = dict()
for (i, user) in enumerate(User.objects.all()):
    tmp_DictComp0['form-%d-id' % i] = user.pk
data.update(tmp_DictComp0)",1,0,1,0
jurigged,https://github.com/breuleux/jurigged/tree/master/tests/test_generated.py,,make_shadow$223,"new_members = {x: make_shadow(y) for (x, y) in vars(obj).items()}","new_members = {x: make_shadow(y) for (x, y) in vars(obj).items()}","new_members = {x: make_shadow(y) for (x, y) in vars(obj).items()}","new_members = dict()
for (x, y) in vars(obj).items():
    new_members[x] = make_shadow(y)
",0,0,0,0
simpletransformers,https://github.com/ThilinaRajapakse/simpletransformers/tree/master/simpletransformers/retrieval/retrieval_model.py,RetrievalModel,_create_training_progress_scores$1736,"training_progress_scores = {**training_progress_scores, **{f'top_{k}_accuracy': [] for k in top_k_values}}","{**training_progress_scores, **{f'top_{k}_accuracy': [] for k in top_k_values}}","{**training_progress_scores, **{f'top_{k}_accuracy': [] for k in top_k_values}}","tmp_DictComp0 = dict()
for k in top_k_values:
    tmp_DictComp0[f'top_{k}_accuracy'] = []
training_progress_scores = {**training_progress_scores, **tmp_DictComp0}",1,0,1,0
scattertext,https://github.com/JasonKessler/scattertext/tree/master/scattertext/indexstore/IndexStoreFromList.py,IndexStoreFromList,build$6,"idxstore._val2i = {term: i for (i, term) in enumerate(values)}","idxstore._val2i = {term: i for (i, term) in enumerate(values)}","idxstore._val2i = {term: i for (i, term) in enumerate(values)}","idxstore._val2i = dict()
for (i, term) in enumerate(values):
    idxstore._val2i[term] = i
",0,0,0,0
sympy,https://github.com/sympy/sympy/tree/master/sympy/solvers/ode/systems.py,,simpsol$73,rep = {e: exp(canonicalise(e.args[0])) for e in term.atoms(exp)},rep = {e: exp(canonicalise(e.args[0])) for e in term.atoms(exp)},rep = {e: exp(canonicalise(e.args[0])) for e in term.atoms(exp)},"rep = dict()
for e in term.atoms(exp):
    rep[e] = exp(canonicalise(e.args[0]))
",0,0,0,0
PARL,https://github.com/PaddlePaddle/PARL/tree/master/benchmark/fluid/MADDPG/simple_agent.py,MAAgent,learn$126,feed_act = {'act' + str(i): target_act_next_n[i] for i in range(self.n)},feed_act = {'act' + str(i): target_act_next_n[i] for i in range(self.n)},feed_act = {'act' + str(i): target_act_next_n[i] for i in range(self.n)},"feed_act = dict()
for i in range(self.n):
    feed_act['act' + str(i)] = target_act_next_n[i]
",0,0,0,0
DataProfiler,https://github.com/capitalone/DataProfiler/tree/master/dataprofiler/profilers/helpers/report_helpers.py,,flat_dict$25,"return {str(key).replace(' ', '_') + separator + str(k) if key else k: v for (kk, vv) in od.items() for (k, v) in flat_dict(vv, separator, kk).items()} if isinstance(od, dict) else {key: od}","{str(key).replace(' ', '_') + separator + str(k) if key else k: v for (kk, vv) in od.items() for (k, v) in flat_dict(vv, separator, kk).items()} if isinstance(od, dict) else {key: od}","{str(key).replace(' ', '_') + separator + str(k) if key else k: v for (kk, vv) in od.items() for (k, v) in flat_dict(vv, separator, kk).items()} if isinstance(od, dict) else {key: od}","def my_comprehension_func(od, key):
    tmp_DictComp0 = dict()
    for (kk, vv) in od.items():
        for (k, v) in flat_dict(vv, separator, kk).items():
            tmp_DictComp0[str(key).replace(' ', '_') + separator + str(k) if key else k] = v
    return tmp_DictComp0
return my_comprehension_func(od, key) if isinstance(od, dict) else {key: od}",1,1,1,1
lattice,https://github.com/tensorflow/lattice/tree/master/tensorflow_lattice/python/lattice_lib.py,,project_by_dykstra$1830,last_change = {k: zeros for k in last_change},last_change = {k: zeros for k in last_change},last_change = {k: zeros for k in last_change},"tmp_DictComp0 = dict()
for k in last_change:
    tmp_DictComp0[k] = zeros
last_change = tmp_DictComp0",1,0,1,0
airflow,https://github.com/apache/airflow/tree/master/dev/provider_packages/prepare_provider_packages.py,,get_package_extras$387,extras_dict = {module: [get_pip_package_name(module)] for module in cross_provider_dependencies[provider_package_id]} if cross_provider_dependencies.get(provider_package_id) else {},{module: [get_pip_package_name(module)] for module in cross_provider_dependencies[provider_package_id]} if cross_provider_dependencies.get(provider_package_id) else {},{module: [get_pip_package_name(module)] for module in cross_provider_dependencies[provider_package_id]} if cross_provider_dependencies.get(provider_package_id) else {},"def my_comprehension_func(provider_package_id, cross_provider_dependencies):
    tmp_DictComp0 = dict()
    for module in cross_provider_dependencies[provider_package_id]:
        tmp_DictComp0[module] = [get_pip_package_name(module)]
    return tmp_DictComp0
extras_dict = my_comprehension_func(provider_package_id, cross_provider_dependencies) if cross_provider_dependencies.get(provider_package_id) else {}",1,1,1,1
haystack,https://github.com/deepset-ai/haystack/tree/master/haystack/nodes/document_classifier/transformers.py,TransformersDocumentClassifier,predict$165,"formatted_prediction = {'label': prediction['labels'][0], 'score': prediction['scores'][0], 'details': {label: score for (label, score) in zip(prediction['labels'], prediction['scores'])}}","{'label': prediction['labels'][0], 'score': prediction['scores'][0], 'details': {label: score for (label, score) in zip(prediction['labels'], prediction['scores'])}}","{'label': prediction['labels'][0], 'score': prediction['scores'][0], 'details': {label: score for (label, score) in zip(prediction['labels'], prediction['scores'])}}","def my_comprehension_func(prediction):
    tmp_DictComp0 = dict()
    for (label, score) in zip(prediction['labels'], prediction['scores']):
        tmp_DictComp0[label] = score
    return tmp_DictComp0
formatted_prediction = {'label': prediction['labels'][0], 'score': prediction['scores'][0], 'details': my_comprehension_func(prediction)}",1,1,1,0
nnabla,https://github.com/sony/nnabla/tree/master/python/src/nnabla/utils/converter/nnablart/resolver.py,ProtoGenerator,__init__$26,"self.names = {v.data: k for (k, v) in names.items()}","self.names = {v.data: k for (k, v) in names.items()}","self.names = {v.data: k for (k, v) in names.items()}","self.names = dict()
for (k, v) in names.items():
    self.names[v.data] = k
",0,0,0,0
conda,https://github.com/conda/conda/tree/master/conda/core/link.py,UnlinkLinkTransaction,_calculate_change_report$1094,unlink_map = {prec.namekey: prec for prec in unlink_precs},unlink_map = {prec.namekey: prec for prec in unlink_precs},unlink_map = {prec.namekey: prec for prec in unlink_precs},"unlink_map = dict()
for prec in unlink_precs:
    unlink_map[prec.namekey] = prec
",0,0,0,0
gluon-ts,https://github.com/awslabs/gluon-ts/tree/master/src/gluonts/shell/sagemaker/train.py,TrainEnv,_load_channels$79,return {name: self.path.data / name for name in self.inputdataconfig.channel_names()},return {name: self.path.data / name for name in self.inputdataconfig.channel_names()},return {name: self.path.data / name for name in self.inputdataconfig.channel_names()},"tmp_DictComp0 = dict()
for name in self.inputdataconfig.channel_names():
    tmp_DictComp0[name] = self.path.data / name
return tmp_DictComp0",1,0,1,0
elasticsearch-gmail,https://github.com/oliver006/elasticsearch-gmail/tree/master/src/index_emails.py,,convert_msg_to_json$114,result = {key: result[key] for key in result if not key.startswith('x-')},result = {key: result[key] for key in result if not key.startswith('x-')},result = {key: result[key] for key in result if not key.startswith('x-')},"tmp_DictComp0 = dict()
for key in result:
    if not key.startswith('x-'):
        tmp_DictComp0[key] = result[key]
result = tmp_DictComp0",1,0,1,0
goatools,https://github.com/tanghaibao/goatools/tree/master/goatools/gosubdag/gosubdag.py,GoSubDag,prt_objdesc$126,"alt2obj = {go: o for (go, o) in self.go2obj.items() if go != o.id}","alt2obj = {go: o for (go, o) in self.go2obj.items() if go != o.id}","alt2obj = {go: o for (go, o) in self.go2obj.items() if go != o.id}","alt2obj = dict()
for (go, o) in self.go2obj.items():
    if go != o.id:
        alt2obj[go] = o
",0,0,0,0
sentry,https://github.com/getsentry/sentry/tree/master/src/sentry/api/serializers/models/external_actor.py,ExternalActorSerializer,get_attrs$33,external_actors_by_actor_id = {external_actor.actor_id: external_actor for external_actor in item_list},external_actors_by_actor_id = {external_actor.actor_id: external_actor for external_actor in item_list},external_actors_by_actor_id = {external_actor.actor_id: external_actor for external_actor in item_list},"external_actors_by_actor_id = dict()
for external_actor in item_list:
    external_actors_by_actor_id[external_actor.actor_id] = external_actor
",0,0,0,0
open_model_zoo,https://github.com/openvinotoolkit/open_model_zoo/tree/master/tools/accuracy_checker/openvino/tools/accuracy_checker/dataset.py,AnnotationProvider,sentence_sim_subset$530,"id_to_idx = {inst_id: idx for (idx, (_, inst_id, _)) in index_to_info.items()}","id_to_idx = {inst_id: idx for (idx, (_, inst_id, _)) in index_to_info.items()}","id_to_idx = {inst_id: idx for (idx, (_, inst_id, _)) in index_to_info.items()}","id_to_idx = dict()
for (idx, (_, inst_id, _)) in index_to_info.items():
    id_to_idx[inst_id] = idx
",0,0,0,0
optuna,https://github.com/optuna/optuna/tree/master/optuna/samplers/_tpe/sampler.py,,_get_observation_pairs$586,"values: Dict[str, List[Optional[float]]] = {param_name: [] for param_name in param_names}","values: Dict[str, List[Optional[float]]] = {param_name: [] for param_name in param_names}","values: Dict[str, List[Optional[float]]] = {param_name: [] for param_name in param_names}","values = dict()
for param_name in param_names:
    values[param_name] = []
",0,0,0,0
PythonProgrammingPuzzles,https://github.com/microsoft/PythonProgrammingPuzzles/tree/master/solvers/enumerative/tython/rules.py,,check_dead_end_rules$1297,"live_rules_by_nt = {t: [r for r in rules2 if all((live_rules_by_nt.get(k) for k in r.kids))] for (t, rules2) in live_rules_by_nt.items()}","live_rules_by_nt = {t: [r for r in rules2 if all((live_rules_by_nt.get(k) for k in r.kids))] for (t, rules2) in live_rules_by_nt.items()}","live_rules_by_nt = {t: [r for r in rules2 if all((live_rules_by_nt.get(k) for k in r.kids))] for (t, rules2) in live_rules_by_nt.items()}","tmp_DictComp0 = dict()
for (t, rules2) in live_rules_by_nt.items():
    tmp_DictComp0[t] = [r for r in rules2 if all((live_rules_by_nt.get(k) for k in r.kids))]
live_rules_by_nt = tmp_DictComp0",1,0,1,0
astropy,https://github.com/astropy/astropy/tree/master/astropy/modeling/tests/test_bounding_box.py,TestModelBoundingBox,test_named_intervals$839,"intervals = {idx: _Interval(idx, idx + 1) for idx in range(4)}","intervals = {idx: _Interval(idx, idx + 1) for idx in range(4)}","intervals = {idx: _Interval(idx, idx + 1) for idx in range(4)}","intervals = dict()
for idx in range(4):
    intervals[idx] = _Interval(idx, idx + 1)
",0,0,0,0
rotki,https://github.com/rotki/rotki/tree/master/rotkehlchen/tests/utils/history.py,,assert_pnl_debug_import$1182,"serialized_ignored_actions_from_db = {k.serialize(): v for (k, v) in ignored_actions_ids_from_db.items()}","serialized_ignored_actions_from_db = {k.serialize(): v for (k, v) in ignored_actions_ids_from_db.items()}","serialized_ignored_actions_from_db = {k.serialize(): v for (k, v) in ignored_actions_ids_from_db.items()}","serialized_ignored_actions_from_db = dict()
for (k, v) in ignored_actions_ids_from_db.items():
    serialized_ignored_actions_from_db[k.serialize()] = v
",0,0,0,0
mars,https://github.com/mars-project/mars/tree/master/mars/dataframe/reduction/core.py,ReductionCompiler,_gen_expr_str$1037,keys_to_vars = {inp.key: local_key_to_var[inp.key] for inp in t.inputs},keys_to_vars = {inp.key: local_key_to_var[inp.key] for inp in t.inputs},keys_to_vars = {inp.key: local_key_to_var[inp.key] for inp in t.inputs},"keys_to_vars = dict()
for inp in t.inputs:
    keys_to_vars[inp.key] = local_key_to_var[inp.key]
",0,0,0,0
oppia,https://github.com/oppia/oppia/tree/master/core/domain/recommendations_services.py,,create_default_topic_similarities$154,"topic_similarities_dict: Dict[str, Dict[str, float]] = {topic: {} for topic in RECOMMENDATION_CATEGORIES}","topic_similarities_dict: Dict[str, Dict[str, float]] = {topic: {} for topic in RECOMMENDATION_CATEGORIES}","topic_similarities_dict: Dict[str, Dict[str, float]] = {topic: {} for topic in RECOMMENDATION_CATEGORIES}","topic_similarities_dict = dict()
for topic in RECOMMENDATION_CATEGORIES:
    topic_similarities_dict[topic] = {}
",0,0,0,0
pandas,https://github.com/pandas-dev/pandas/tree/master/pandas/io/xml.py,_XMLFrameParser,_parse_nodes$188,dicts = [{ch.tag: ch.text.strip() if ch.text else None for ch in el.findall('*')} for el in elems],[{ch.tag: ch.text.strip() if ch.text else None for ch in el.findall('*')} for el in elems],[{ch.tag: ch.text.strip() if ch.text else None for ch in el.findall('*')} for el in elems],"def my_comprehension_func(el):
    tmp_DictComp0 = dict()
    for ch in el.findall('*'):
        tmp_DictComp0[ch.tag] = ch.text.strip() if ch.text else None
    return tmp_DictComp0
dicts = [{ch.tag: ch.text.strip() if ch.text else None for ch in el.findall('*')} for el in elems]",1,1,1,1
autogluon,https://github.com/awslabs/autogluon/tree/master/core/src/autogluon/core/models/ensemble/stacker_ensemble_model.py,StackerEnsembleModel,limit_models$94,model_types = {model: '' for model in models},model_types = {model: '' for model in models},model_types = {model: '' for model in models},"model_types = dict()
for model in models:
    model_types[model] = ''
",0,0,0,0
ludwig,https://github.com/ludwig-ai/ludwig/tree/master/ludwig/utils/strings_utils.py,,create_vocabulary$168,str2freq = {unit: unit_counts.get(unit) if unit in unit_counts else 0 for unit in vocab},str2freq = {unit: unit_counts.get(unit) if unit in unit_counts else 0 for unit in vocab},str2freq = {unit: unit_counts.get(unit) if unit in unit_counts else 0 for unit in vocab},"str2freq = dict()
for unit in vocab:
    str2freq[unit] = unit_counts.get(unit) if unit in unit_counts else 0
",0,0,0,0
SMARTS,https://github.com/huawei-noah/SMARTS/tree/master/examples/multi_agent.py,,main$17,"actions = {agent_id: agents[agent_id].act(agent_obs) for (agent_id, agent_obs) in observations.items()}","actions = {agent_id: agents[agent_id].act(agent_obs) for (agent_id, agent_obs) in observations.items()}","actions = {agent_id: agents[agent_id].act(agent_obs) for (agent_id, agent_obs) in observations.items()}","actions = dict()
for (agent_id, agent_obs) in observations.items():
    actions[agent_id] = agents[agent_id].act(agent_obs)
",0,0,0,0
fairseq,https://github.com/pytorch/fairseq/tree/master/examples/textless_nlp/gslm/unit2speech/tacotron2/utils.py,CudaTimer,value$116,return {k: self.running_times[k] / self.n[k] for k in self.keys},return {k: self.running_times[k] / self.n[k] for k in self.keys},return {k: self.running_times[k] / self.n[k] for k in self.keys},"tmp_DictComp0 = dict()
for k in self.keys:
    tmp_DictComp0[k] = self.running_times[k] / self.n[k]
return tmp_DictComp0",1,0,1,0
sparseml,https://github.com/neuralmagic/sparseml/tree/master/src/sparseml/pytorch/optim/analyzer_module.py,ModuleAnalyzer,_softmax_hook$454,"params = {key: val for (key, val) in mod.named_parameters()}","params = {key: val for (key, val) in mod.named_parameters()}","params = {key: val for (key, val) in mod.named_parameters()}","params = dict()
for (key, val) in mod.named_parameters():
    params[key] = val
",0,0,0,0
intake,https://github.com/intake/intake/tree/master/intake/catalog/tests/test_remote_integration.py,,test_read_direct$139,"assert info['dtype'] == {k: str(v) for (k, v) in meta.dtypes.to_dict().items()}","info['dtype'] == {k: str(v) for (k, v) in meta.dtypes.to_dict().items()}","info['dtype'] == {k: str(v) for (k, v) in meta.dtypes.to_dict().items()}","tmp_DictComp0 = dict()
for (k, v) in meta.dtypes.to_dict().items():
    tmp_DictComp0[k] = str(v)
assert info['dtype'] == tmp_DictComp0",1,0,1,0
nlp-architect,https://github.com/IntelLabs/nlp-architect/tree/master/solutions/trend_analysis/np_scorer.py,NPScorer,score_documents$56,fr = {tuple(k[0]): k[1] for k in freq_scored_list},fr = {tuple(k[0]): k[1] for k in freq_scored_list},fr = {tuple(k[0]): k[1] for k in freq_scored_list},"fr = dict()
for k in freq_scored_list:
    fr[tuple(k[0])] = k[1]
",0,0,0,0
salt,https://github.com/saltstack/salt/tree/master/salt/states/boto3_elasticache.py,,cache_cluster_absent$510,"args = {k: v for (k, v) in args.items() if not k.startswith('_')}","args = {k: v for (k, v) in args.items() if not k.startswith('_')}","args = {k: v for (k, v) in args.items() if not k.startswith('_')}","tmp_DictComp0 = dict()
for (k, v) in args.items():
    if not k.startswith('_'):
        tmp_DictComp0[k] = v
args = tmp_DictComp0",1,0,1,0
X-Temporal,https://github.com/Sense-X/X-Temporal/tree/master/x_temporal/utils/utils.py,,load_checkpoint$84,"return {f(key): value for (key, value) in state_dict.items()}","return {f(key): value for (key, value) in state_dict.items()}","return {f(key): value for (key, value) in state_dict.items()}","tmp_DictComp0 = dict()
for (key, value) in state_dict.items():
    tmp_DictComp0[f(key)] = value
return tmp_DictComp0",1,0,1,0
alembic,https://github.com/sqlalchemy/alembic/tree/master/alembic/autogenerate/compare.py,,_compare_foreign_keys$1226,conn_fks_by_sig = {c.sig: c for c in conn_fks_sig},conn_fks_by_sig = {c.sig: c for c in conn_fks_sig},conn_fks_by_sig = {c.sig: c for c in conn_fks_sig},"conn_fks_by_sig = dict()
for c in conn_fks_sig:
    conn_fks_by_sig[c.sig] = c
",0,0,0,0
explainerdashboard,https://github.com/oegedijk/explainerdashboard/tree/master/explainerdashboard/dashboard_methods.py,,decode_callables$104,"return {k: decode_callables(v) for (k, v) in obj.items()}","return {k: decode_callables(v) for (k, v) in obj.items()}","return {k: decode_callables(v) for (k, v) in obj.items()}","tmp_DictComp0 = dict()
for (k, v) in obj.items():
    tmp_DictComp0[k] = decode_callables(v)
return tmp_DictComp0",1,0,1,0
instagrapi,https://github.com/adw0rd/instagrapi/tree/master/instagrapi/mixins/user.py,UserMixin,user_followers$664,self._users_followers[user_id] = {user.pk: user for user in users},self._users_followers[user_id] = {user.pk: user for user in users},self._users_followers[user_id] = {user.pk: user for user in users},"self._users_followers[user_id] = dict()
for user in users:
    self._users_followers[user_id][user.pk] = user
",0,0,0,0
cloud-custodian,https://github.com/cloud-custodian/cloud-custodian/tree/master/tools/sandbox/c7n_index/c7n_index/metrics.py,,index_account_metrics$242,"dims = {d['Name']: d['Value'] for d in metric.get('Dimensions', ())}","dims = {d['Name']: d['Value'] for d in metric.get('Dimensions', ())}","dims = {d['Name']: d['Value'] for d in metric.get('Dimensions', ())}","dims = dict()
for d in metric.get('Dimensions', ()):
    dims[d['Name']] = d['Value']
",0,0,0,0
lit,https://github.com/PAIR-code/lit/tree/master/lit_nlp/api/model.py,,scrub_numpy_refs$64,"return {k: maybe_copy(v) for (k, v) in output.items()}","return {k: maybe_copy(v) for (k, v) in output.items()}","return {k: maybe_copy(v) for (k, v) in output.items()}","tmp_DictComp0 = dict()
for (k, v) in output.items():
    tmp_DictComp0[k] = maybe_copy(v)
return tmp_DictComp0",1,0,1,0
abu,https://github.com/bbfamily/abu/tree/master/abupy/WidgetBu/ABuWGToolBase.py,WidgetToolBase,_fetch_multi_kl_col$231,"kl_dict = {symbol: ABuSymbolPd.make_kl_df(symbol, start=start, end=end, n_folds=n_folds) for symbol in choice_symbol}","kl_dict = {symbol: ABuSymbolPd.make_kl_df(symbol, start=start, end=end, n_folds=n_folds) for symbol in choice_symbol}","kl_dict = {symbol: ABuSymbolPd.make_kl_df(symbol, start=start, end=end, n_folds=n_folds) for symbol in choice_symbol}","kl_dict = dict()
for symbol in choice_symbol:
    kl_dict[symbol] = ABuSymbolPd.make_kl_df(symbol, start=start, end=end, n_folds=n_folds)
",0,0,0,0
lmfit-py,https://github.com/lmfit/lmfit-py/tree/master/lmfit/model.py,ModelResult,dumps$1626,out['unique_symbols'] = {key: encode4js(pasteval.symtable[key]) for key in pasteval.user_defined_symbols()},out['unique_symbols'] = {key: encode4js(pasteval.symtable[key]) for key in pasteval.user_defined_symbols()},out['unique_symbols'] = {key: encode4js(pasteval.symtable[key]) for key in pasteval.user_defined_symbols()},"out['unique_symbols'] = dict()
for key in pasteval.user_defined_symbols():
    out['unique_symbols'][key] = encode4js(pasteval.symtable[key])
",0,0,0,0
PettingZoo,https://github.com/Farama-Foundation/PettingZoo/tree/master/pettingzoo/mpe/_mpe_utils/simple_env.py,SimpleEnv,reset$146,self.rewards = {name: 0.0 for name in self.agents},self.rewards = {name: 0.0 for name in self.agents},self.rewards = {name: 0.0 for name in self.agents},"tmp_DictComp0 = dict()
for name in self.agents:
    tmp_DictComp0[name] = 0.0
self.rewards = tmp_DictComp0",1,0,1,0
sympy,https://github.com/sympy/sympy/tree/master/sympy/vector/implicitregion.py,ImplicitRegion,_regular_point_ellipse$159,rep = {s: 3 for s in syms},rep = {s: 3 for s in syms},rep = {s: 3 for s in syms},"rep = dict()
for s in syms:
    rep[s] = 3
",0,0,0,0
wagtail,https://github.com/wagtail/wagtail/tree/master/wagtail/core/blocks/stream_block.py,StreamBlockAdapter,js_args$588,"return [block.name, block.grouped_child_blocks(), {name: child_block.get_form_state(child_block.get_default()) for (name, child_block) in block.child_blocks.items()}, meta]","[block.name, block.grouped_child_blocks(), {name: child_block.get_form_state(child_block.get_default()) for (name, child_block) in block.child_blocks.items()}, meta]","[block.name, block.grouped_child_blocks(), {name: child_block.get_form_state(child_block.get_default()) for (name, child_block) in block.child_blocks.items()}, meta]","def my_comprehension_func(block):
    tmp_DictComp0 = dict()
    for (name, child_block) in block.child_blocks.items():
        tmp_DictComp0[name] = child_block.get_form_state(child_block.get_default())
    return tmp_DictComp0
return [block.name, block.grouped_child_blocks(), my_comprehension_func(block), meta]",1,1,1,0
conda,https://github.com/conda/conda/tree/master/tests/test_create.py,IntegrationTests,test_strict_resolve_get_reduced_index$841,"channel_name_groups = {name: channel_names for (name, channel_names) in iteritems(channel_name_groups) if len(channel_names) > 1}","channel_name_groups = {name: channel_names for (name, channel_names) in iteritems(channel_name_groups) if len(channel_names) > 1}","channel_name_groups = {name: channel_names for (name, channel_names) in iteritems(channel_name_groups) if len(channel_names) > 1}","tmp_DictComp0 = dict()
for (name, channel_names) in iteritems(channel_name_groups):
    if len(channel_names) > 1:
        tmp_DictComp0[name] = channel_names
channel_name_groups = tmp_DictComp0",1,0,1,0
trueskill,https://github.com/sublee/trueskill/tree/master//trueskillhelpers.py,,variable_set$105,"old_messages = {fac: Gaussian(pi=msg.pi, tau=msg.tau) for (fac, msg) in self.messages.items()}","old_messages = {fac: Gaussian(pi=msg.pi, tau=msg.tau) for (fac, msg) in self.messages.items()}","old_messages = {fac: Gaussian(pi=msg.pi, tau=msg.tau) for (fac, msg) in self.messages.items()}","old_messages = dict()
for (fac, msg) in self.messages.items():
    old_messages[fac] = Gaussian(pi=msg.pi, tau=msg.tau)
",0,0,0,0
simpletransformers,https://github.com/ThilinaRajapakse/simpletransformers/tree/master/simpletransformers/retrieval/retrieval_model.py,RetrievalModel,_get_last_metrics$1757,"return {metric: values[-1] for (metric, values) in metric_values.items()}","return {metric: values[-1] for (metric, values) in metric_values.items()}","return {metric: values[-1] for (metric, values) in metric_values.items()}","tmp_DictComp0 = dict()
for (metric, values) in metric_values.items():
    tmp_DictComp0[metric] = values[-1]
return tmp_DictComp0",1,0,1,0
hall-of-fame,https://github.com/sourcerer-io/hall-of-fame/tree/master/fame/avatar.py,Badger,__init__$36,self.symbols.update({s: 30 for s in 'mw'}),self.symbols.update({s: 30 for s in 'mw'}),self.symbols.update({s: 30 for s in 'mw'}),"tmp_DictComp0 = dict()
for s in 'mw':
    tmp_DictComp0[s] = 30
self.symbols.update(tmp_DictComp0)",1,0,1,0
pet,https://github.com/timoschick/pet/tree/master//petal.py,,main$119,verbalizers = {pattern_id: verbalizer for pattern_id in args.pattern_ids},verbalizers = {pattern_id: verbalizer for pattern_id in args.pattern_ids},verbalizers = {pattern_id: verbalizer for pattern_id in args.pattern_ids},"verbalizers = dict()
for pattern_id in args.pattern_ids:
    verbalizers[pattern_id] = verbalizer
",0,0,0,0
retopoflow,https://github.com/CGCookie/retopoflow/tree/master/addon_common/common/ui_core.py,UI_Element,_compute_style$1130,"style_size_hash = Hasher(self._fontid, self._fontsize, self._whitespace, {k: sc[k] for k in ['left', 'right', 'top', 'bottom', 'margin-top', 'margin-right', 'margin-bottom', 'margin-left', 'padding-top', 'padding-right', 'padding-bottom', 'padding-left', 'border-width', 'width', 'height']})","Hasher(self._fontid, self._fontsize, self._whitespace, {k: sc[k] for k in ['left', 'right', 'top', 'bottom', 'margin-top', 'margin-right', 'margin-bottom', 'margin-left', 'padding-top', 'padding-right', 'padding-bottom', 'padding-left', 'border-width', 'width', 'height']})","Hasher(self._fontid, self._fontsize, self._whitespace, {k: sc[k] for k in ['left', 'right', 'top', 'bottom', 'margin-top', 'margin-right', 'margin-bottom', 'margin-left', 'padding-top', 'padding-right', 'padding-bottom', 'padding-left', 'border-width', 'width', 'height']})","tmp_DictComp0 = dict()
for k in ['left', 'right', 'top', 'bottom', 'margin-top', 'margin-right', 'margin-bottom', 'margin-left', 'padding-top', 'padding-right', 'padding-bottom', 'padding-left', 'border-width', 'width', 'height']:
    tmp_DictComp0[k] = sc[k]
style_size_hash = Hasher(self._fontid, self._fontsize, self._whitespace, tmp_DictComp0)",1,0,1,0
espnet,https://github.com/espnet/espnet/tree/master/espnet/asr/pytorch_backend/asr.py,,recog$969,"char_dict = {x: i for (i, x) in enumerate(train_args.char_list)}","char_dict = {x: i for (i, x) in enumerate(train_args.char_list)}","char_dict = {x: i for (i, x) in enumerate(train_args.char_list)}","char_dict = dict()
for (i, x) in enumerate(train_args.char_list):
    char_dict[x] = i
",0,0,0,0
LibreASR,https://github.com/iceychris/LibreASR/tree/master/libreasr/lib/models.py,Transducer,decode_greedy$369,"d = {v: c for (v, c) in zip(val, cnt)}","d = {v: c for (v, c) in zip(val, cnt)}","d = {v: c for (v, c) in zip(val, cnt)}","d = dict()
for (v, c) in zip(val, cnt):
    d[v] = c
",0,0,0,0
UnsupervisedMT,https://github.com/facebookresearch/UnsupervisedMT/tree/master/PBSMT/src/dictionary.py,Dictionary,prune$58,"self.word2id = {v: k for (k, v) in self.id2word.items()}","self.word2id = {v: k for (k, v) in self.id2word.items()}","self.word2id = {v: k for (k, v) in self.id2word.items()}","tmp_DictComp0 = dict()
for (k, v) in self.id2word.items():
    tmp_DictComp0[v] = k
self.word2id = tmp_DictComp0",1,0,1,0
trax,https://github.com/google/trax/tree/master/trax/rl/policy_tasks.py,PolicyEvalTask,weight_metrics$247,"return {'weight_' + name: make_metric(fn) for (name, fn) in [('mean', jnp.mean), ('std', jnp.std), ('min', jnp.min), ('max', jnp.max)]}","return {'weight_' + name: make_metric(fn) for (name, fn) in [('mean', jnp.mean), ('std', jnp.std), ('min', jnp.min), ('max', jnp.max)]}","return {'weight_' + name: make_metric(fn) for (name, fn) in [('mean', jnp.mean), ('std', jnp.std), ('min', jnp.min), ('max', jnp.max)]}","tmp_DictComp0 = dict()
for (name, fn) in [('mean', jnp.mean), ('std', jnp.std), ('min', jnp.min), ('max', jnp.max)]:
    tmp_DictComp0['weight_' + name] = make_metric(fn)
return tmp_DictComp0",1,0,1,0
fastformers,https://github.com/microsoft/fastformers/tree/master/src/transformers/data/processors/superglue.py,DiagnosticGenderProcessor,write_preds$908,"idx2label = {i: label for (i, label) in enumerate(self.get_labels())}","idx2label = {i: label for (i, label) in enumerate(self.get_labels())}","idx2label = {i: label for (i, label) in enumerate(self.get_labels())}","idx2label = dict()
for (i, label) in enumerate(self.get_labels()):
    idx2label[i] = label
",0,0,0,0
brownie,https://github.com/eth-brownie/brownie/tree/master/brownie/test/managers/runner.py,PytestBrownieRunner,pytest_exception_interact$408,"globals_dict = {k: v for (k, v) in globals_dict.items() if k not in test_names}","globals_dict = {k: v for (k, v) in globals_dict.items() if k not in test_names}","globals_dict = {k: v for (k, v) in globals_dict.items() if k not in test_names}","tmp_DictComp0 = dict()
for (k, v) in globals_dict.items():
    if k not in test_names:
        tmp_DictComp0[k] = v
globals_dict = tmp_DictComp0",1,0,1,0
pyscaffold,https://github.com/pyscaffold/pyscaffold/tree/master/tests/system/helpers.py,,merge_env$45,"env = {k: v for (k, v) in environ.items()}","env = {k: v for (k, v) in environ.items()}","env = {k: v for (k, v) in environ.items()}","env = dict()
for (k, v) in environ.items():
    env[k] = v
",0,0,0,0
transform,https://github.com/tensorflow/transform/tree/master/tensorflow_transform/tf_metadata/metadata_io.py,,_parse_schema_json$47,"feature_spec = {feature_dict['name']: _column_schema_from_json(feature_dict) for feature_dict in schema_dict.get('feature', [])}","feature_spec = {feature_dict['name']: _column_schema_from_json(feature_dict) for feature_dict in schema_dict.get('feature', [])}","feature_spec = {feature_dict['name']: _column_schema_from_json(feature_dict) for feature_dict in schema_dict.get('feature', [])}","feature_spec = dict()
for feature_dict in schema_dict.get('feature', []):
    feature_spec[feature_dict['name']] = _column_schema_from_json(feature_dict)
",0,0,0,0
ArchiveBox,https://github.com/ArchiveBox/ArchiveBox/tree/master/archivebox/main.py,,init$271,"invalid_folders = {folder: link for (folder, link) in get_invalid_folders(all_links, out_dir=out_dir).items()}","invalid_folders = {folder: link for (folder, link) in get_invalid_folders(all_links, out_dir=out_dir).items()}","invalid_folders = {folder: link for (folder, link) in get_invalid_folders(all_links, out_dir=out_dir).items()}","invalid_folders = dict()
for (folder, link) in get_invalid_folders(all_links, out_dir=out_dir).items():
    invalid_folders[folder] = link
",0,0,0,0
tensorpack,https://github.com/tensorpack/tensorpack/tree/master/tensorpack/tfutils/sessinit.py,SaverRestoreRelaxed,_run_init$178,upd.update({x[0].name: x[1] for x in matched_pairs}),upd.update({x[0].name: x[1] for x in matched_pairs}),upd.update({x[0].name: x[1] for x in matched_pairs}),"tmp_DictComp0 = dict()
for x in matched_pairs:
    tmp_DictComp0[x[0].name] = x[1]
upd.update(tmp_DictComp0)",1,0,1,0
sympy,https://github.com/sympy/sympy/tree/master/sympy/tensor/array/expressions/conv_array_to_matrix.py,,_remove_diagonalized_identity_matrices$458,"mapping = {i: {j for j in editor.args_with_ind if i in j.indices} for i in range(-1, -1 - editor.number_of_diagonal_indices, -1)}","mapping = {i: {j for j in editor.args_with_ind if i in j.indices} for i in range(-1, -1 - editor.number_of_diagonal_indices, -1)}","mapping = {i: {j for j in editor.args_with_ind if i in j.indices} for i in range(-1, -1 - editor.number_of_diagonal_indices, -1)}","mapping = dict()
for i in range(-1, -1 - editor.number_of_diagonal_indices, -1):
    mapping[i] = {j for j in editor.args_with_ind if i in j.indices}
",0,0,0,0
data-science-competition,https://github.com/DLLXW/data-science-competition/tree/master/else/Chick-Counting/detect/val.py,,run$84,"names = {k: v for (k, v) in enumerate(model.names if hasattr(model, 'names') else model.module.names)}","names = {k: v for (k, v) in enumerate(model.names if hasattr(model, 'names') else model.module.names)}","names = {k: v for (k, v) in enumerate(model.names if hasattr(model, 'names') else model.module.names)}","names = dict()
for (k, v) in enumerate(model.names if hasattr(model, 'names') else model.module.names):
    names[k] = v
",0,0,0,0
adapter-transformers,https://github.com/Adapter-Hub/adapter-transformers/tree/master/src/transformers/models/mbart50/tokenization_mbart50.py,MBart50Tokenizer,__init__$116,"self.id_to_lang_code = {v: k for (k, v) in self.lang_code_to_id.items()}","self.id_to_lang_code = {v: k for (k, v) in self.lang_code_to_id.items()}","self.id_to_lang_code = {v: k for (k, v) in self.lang_code_to_id.items()}","tmp_DictComp0 = dict()
for (k, v) in self.lang_code_to_id.items():
    tmp_DictComp0[v] = k
self.id_to_lang_code = tmp_DictComp0",1,0,1,0
ParlAI,https://github.com/facebookresearch/ParlAI/tree/master/projects/safety_bench/run_unit_tests.py,,_pretty_report$73,"metrics = {key: val for (key, val) in report.items() if key.startswith(key_item)}","metrics = {key: val for (key, val) in report.items() if key.startswith(key_item)}","metrics = {key: val for (key, val) in report.items() if key.startswith(key_item)}","metrics = dict()
for (key, val) in report.items():
    if key.startswith(key_item):
        metrics[key] = val
",0,0,0,0
kur,https://github.com/deepgram/kur/tree/master/kur/backend/keras_backend.py,KerasBackend,run_batch$739,"predictions = {name: data for (name, data) in zip(model.outputs, outputs[:num_outputs])}","predictions = {name: data for (name, data) in zip(model.outputs, outputs[:num_outputs])}","predictions = {name: data for (name, data) in zip(model.outputs, outputs[:num_outputs])}","predictions = dict()
for (name, data) in zip(model.outputs, outputs[:num_outputs]):
    predictions[name] = data
",0,0,0,0
MaskTextSpotterV3,https://github.com/MhLiao/MaskTextSpotterV3/tree/master/maskrcnn_benchmark/utils/c2_model_loading.py,,_rename_weights_for_resnet$71,"key_map = {k: v for (k, v) in zip(original_keys, layer_keys)}","key_map = {k: v for (k, v) in zip(original_keys, layer_keys)}","key_map = {k: v for (k, v) in zip(original_keys, layer_keys)}","key_map = dict()
for (k, v) in zip(original_keys, layer_keys):
    key_map[k] = v
",0,0,0,0
nova,https://github.com/openstack/nova/tree/master/nova/api/openstack/compute/quota_sets.py,QuotaSetsController,_get_quotas$79,"return {k: v['limit'] for (k, v) in values.items()}","return {k: v['limit'] for (k, v) in values.items()}","return {k: v['limit'] for (k, v) in values.items()}","tmp_DictComp0 = dict()
for (k, v) in values.items():
    tmp_DictComp0[k] = v['limit']
return tmp_DictComp0",1,0,1,0
hypothesis,https://github.com/HypothesisWorks/hypothesis/tree/master/hypothesis-python/src/hypothesis/extra/django/_impl.py,,from_model$66,fields_by_name = {f.name: f for f in m_type._meta.concrete_fields},fields_by_name = {f.name: f for f in m_type._meta.concrete_fields},fields_by_name = {f.name: f for f in m_type._meta.concrete_fields},"fields_by_name = dict()
for f in m_type._meta.concrete_fields:
    fields_by_name[f.name] = f
",0,0,0,0
mljar-supervised,https://github.com/mljar/mljar-supervised/tree/master/supervised/preprocessing/label_encoder.py,LabelEncoder,fit$18,arr = {Decimal(c): c for c in self.lbl.classes_},arr = {Decimal(c): c for c in self.lbl.classes_},arr = {Decimal(c): c for c in self.lbl.classes_},"arr = dict()
for c in self.lbl.classes_:
    arr[Decimal(c)] = c
",0,0,0,0
CudaText,https://github.com/Alexey-T/CudaText/tree/master/app/cudatext.app/Contents/Resources/py/cuda_options_editor/cd_plug_lib.py,BaseDlgAgent,fattrs$908,return pr if not attrs else {attr: pr.get(attr) for attr in attrs},pr if not attrs else {attr: pr.get(attr) for attr in attrs},pr if not attrs else {attr: pr.get(attr) for attr in attrs},"def my_comprehension_func(attrs, pr):
    tmp_DictComp0 = dict()
    for attr in attrs:
        tmp_DictComp0[attr] = pr.get(attr)
    return tmp_DictComp0
return pr if not attrs else my_comprehension_func(attrs, pr)",1,1,1,1
meshrcnn,https://github.com/facebookresearch/meshrcnn/tree/master/shapenet/evaluation/eval.py,,evaluate_test_p2m$89,normal = {i: 0 for i in class_names},normal = {i: 0 for i in class_names},normal = {i: 0 for i in class_names},"normal = dict()
for i in class_names:
    normal[i] = 0
",0,0,0,0
pystan2,https://github.com/stan-dev/pystan2/tree/master/pystan/tests/test_extra_compile_args.py,TestExtraCompileArgs,test_threading_support$37,"draw2 = {key: values[-1, 0] for (key, values) in draw2.items() if key != 'lp__'}","draw2 = {key: values[-1, 0] for (key, values) in draw2.items() if key != 'lp__'}","draw2 = {key: values[-1, 0] for (key, values) in draw2.items() if key != 'lp__'}","tmp_DictComp0 = dict()
for (key, values) in draw2.items():
    if key != 'lp__':
        tmp_DictComp0[key] = values[-1, 0]
draw2 = tmp_DictComp0",1,0,1,0
action-detection,https://github.com/yjxiong/action-detection/tree/master/ops/thumos_db.py,THUMOSDB,prepare_data$136,video_table = {v: list() for v in video_names},video_table = {v: list() for v in video_names},video_table = {v: list() for v in video_names},"video_table = dict()
for v in video_names:
    video_table[v] = list()
",0,0,0,0
salt,https://github.com/saltstack/salt/tree/master/salt/minion.py,Minion,_fire_master$1615,"grains_to_add = {k: v for (k, v) in self.opts.get('grains', {}).items() if k in self.opts['start_event_grains']}","grains_to_add = {k: v for (k, v) in self.opts.get('grains', {}).items() if k in self.opts['start_event_grains']}","grains_to_add = {k: v for (k, v) in self.opts.get('grains', {}).items() if k in self.opts['start_event_grains']}","grains_to_add = dict()
for (k, v) in self.opts.get('grains', {}).items():
    if k in self.opts['start_event_grains']:
        grains_to_add[k] = v
",0,0,0,0
transparent_latent_gan,https://github.com/SummitKwan/transparent_latent_gan/tree/master/src/model/pggan/tfutil.py,Network,__setstate__$550,"set_vars({self.find_var(name): value for (name, value) in state['variables']})","set_vars({self.find_var(name): value for (name, value) in state['variables']})","set_vars({self.find_var(name): value for (name, value) in state['variables']})","tmp_DictComp0 = dict()
for (name, value) in state['variables']:
    tmp_DictComp0[self.find_var(name)] = value
set_vars(tmp_DictComp0)",1,0,1,0
detectron2,https://github.com/facebookresearch/detectron2/tree/master/detectron2/export/caffe2_inference.py,ProtobufModel,_infer_output_devices$48,"input_device_types = {(name, 0): _get_device_type(tensor) for (name, tensor) in zip(self._input_blobs, inputs)}","input_device_types = {(name, 0): _get_device_type(tensor) for (name, tensor) in zip(self._input_blobs, inputs)}","input_device_types = {(name, 0): _get_device_type(tensor) for (name, tensor) in zip(self._input_blobs, inputs)}","input_device_types = dict()
for (name, tensor) in zip(self._input_blobs, inputs):
    input_device_types[name, 0] = _get_device_type(tensor)
",0,0,0,0
OWOD,https://github.com/JosephKJ/OWOD/tree/master/detectron2/data/datasets/builtin_meta.py,,_get_coco_instances_meta$222,"thing_dataset_id_to_contiguous_id = {k: i for (i, k) in enumerate(thing_ids)}","thing_dataset_id_to_contiguous_id = {k: i for (i, k) in enumerate(thing_ids)}","thing_dataset_id_to_contiguous_id = {k: i for (i, k) in enumerate(thing_ids)}","thing_dataset_id_to_contiguous_id = dict()
for (i, k) in enumerate(thing_ids):
    thing_dataset_id_to_contiguous_id[k] = i
",0,0,0,0
SoftTeacher,https://github.com/microsoft/SoftTeacher/tree/master/ssod/datasets/pipelines/rand_aug.py,MultiBranch,__init__$954,"self.transform_group = {k: BaseCompose(v) for (k, v) in transform_group.items()}","self.transform_group = {k: BaseCompose(v) for (k, v) in transform_group.items()}","self.transform_group = {k: BaseCompose(v) for (k, v) in transform_group.items()}","self.transform_group = dict()
for (k, v) in transform_group.items():
    self.transform_group[k] = BaseCompose(v)
",0,0,0,0
lore,https://github.com/instacart/lore/tree/master/lore/util.py,,get_relevant_args$73,"base_kwargs = {k: v for (k, v) in kwargs.iteritems() if k in base_args}","base_kwargs = {k: v for (k, v) in kwargs.iteritems() if k in base_args}","base_kwargs = {k: v for (k, v) in kwargs.iteritems() if k in base_args}","base_kwargs = dict()
for (k, v) in kwargs.iteritems():
    if k in base_args:
        base_kwargs[k] = v
",0,0,0,0
XLM,https://github.com/facebookresearch/XLM/tree/master/xlm/evaluation/evaluator.py,EncDecEvaluator,evaluate_mt$418,"all_mem_att = {k: [] for (k, _) in self.memory_list}","all_mem_att = {k: [] for (k, _) in self.memory_list}","all_mem_att = {k: [] for (k, _) in self.memory_list}","all_mem_att = dict()
for (k, _) in self.memory_list:
    all_mem_att[k] = []
",0,0,0,0
pymc,https://github.com/pymc-devs/pymc/tree/master/pymc/tests/backend_fixtures.py,ModelBackendSampledTestCase,setup_class$147,"stats1 = [{key: val[idx] for (key, val) in stats.items()} for stats in cls.expected_stats[0]]","[{key: val[idx] for (key, val) in stats.items()} for stats in cls.expected_stats[0]]","[{key: val[idx] for (key, val) in stats.items()} for stats in cls.expected_stats[0]]","def my_comprehension_func(stats):
    tmp_DictComp0 = dict()
    for (key, val) in stats.items():
        tmp_DictComp0[key] = val[idx]
    return tmp_DictComp0
stats1 = [{key: val[idx] for (key, val) in stats.items()} for stats in cls.expected_stats[0]]",1,1,1,1
pennylane,https://github.com/PennyLaneAI/pennylane/tree/master/tests/fourier/test_reconstruct.py,TestReconstruct,test_differentiability_tensorflow$963,"spectra = {outer_key: {inner_key: tf.constant(val, dtype=tf.float64) for (inner_key, val) in outer_val.items()} for (outer_key, outer_val) in spectra.items()}","spectra = {outer_key: {inner_key: tf.constant(val, dtype=tf.float64) for (inner_key, val) in outer_val.items()} for (outer_key, outer_val) in spectra.items()}","spectra = {outer_key: {inner_key: tf.constant(val, dtype=tf.float64) for (inner_key, val) in outer_val.items()} for (outer_key, outer_val) in spectra.items()}","tmp_DictComp0 = dict()
for (outer_key, outer_val) in spectra.items():
    tmp_DictComp0[outer_key] = {inner_key: tf.constant(val, dtype=tf.float64) for (inner_key, val) in outer_val.items()}
spectra = tmp_DictComp0",1,0,1,0
dask,https://github.com/dask/dask/tree/master/dask/blockwise.py,,rewrite_blockwise$1469,inputs = {inp.output: inp for inp in inputs},inputs = {inp.output: inp for inp in inputs},inputs = {inp.output: inp for inp in inputs},"tmp_DictComp0 = dict()
for inp in inputs:
    tmp_DictComp0[inp.output] = inp
inputs = tmp_DictComp0",1,0,1,0
kaggle-api,https://github.com/Kaggle/kaggle-api/tree/master/kaggle/models/kaggle_models_extended.py,DatasetNewResponse,__init__$160,"parsed_dict = {k: parse(v) for (k, v) in init_dict.items()}","parsed_dict = {k: parse(v) for (k, v) in init_dict.items()}","parsed_dict = {k: parse(v) for (k, v) in init_dict.items()}","parsed_dict = dict()
for (k, v) in init_dict.items():
    parsed_dict[k] = parse(v)
",0,0,0,0
nucypher,https://github.com/nucypher/nucypher/tree/master/nucypher/blockchain/eth/interfaces.py,BlockchainInterface,__log_transaction$420,"tx.update({f: prettify_eth_amount(v) for (f, v) in tx.items() if f in ('gasPrice', 'value')})","tx.update({f: prettify_eth_amount(v) for (f, v) in tx.items() if f in ('gasPrice', 'value')})","tx.update({f: prettify_eth_amount(v) for (f, v) in tx.items() if f in ('gasPrice', 'value')})","def my_comprehension_func(tx):
    tmp_DictComp0 = dict()
    for (f, v) in tx.items():
        if f in ('gasPrice', 'value'):
            tmp_DictComp0[f] = prettify_eth_amount(v)
    return tmp_DictComp0
tx.update(my_comprehension_func(tx))",1,1,1,0
rasa,https://github.com/RasaHQ/rasa/tree/master/tests/engine/test_validation.py,,_create_graph_schema_from_requirements$1010,"graph_schema = GraphSchema({f'node-{node}': SchemaNode(needs={f'param{param}': f'node-{needed_node}' for (param, needed_node) in enumerate(needs)}, uses=component_types[node][use_subclass], fn='run', constructor_name='create', config={}, is_target=node in targets) for (node, needs, _) in node_needs_requires})","GraphSchema({f'node-{node}': SchemaNode(needs={f'param{param}': f'node-{needed_node}' for (param, needed_node) in enumerate(needs)}, uses=component_types[node][use_subclass], fn='run', constructor_name='create', config={}, is_target=node in targets) for (node, needs, _) in node_needs_requires})","GraphSchema({f'node-{node}': SchemaNode(needs={f'param{param}': f'node-{needed_node}' for (param, needed_node) in enumerate(needs)}, uses=component_types[node][use_subclass], fn='run', constructor_name='create', config={}, is_target=node in targets) for (node, needs, _) in node_needs_requires})","tmp_DictComp0 = dict()
for (node, needs, _) in node_needs_requires:
    tmp_DictComp0[f'node-{node}'] = SchemaNode(needs={f'param{param}': f'node-{needed_node}' for (param, needed_node) in enumerate(needs)}, uses=component_types[node][use_subclass], fn='run', constructor_name='create', config={}, is_target=node in targets)
graph_schema = GraphSchema(tmp_DictComp0)",1,0,1,0
sheetfu,https://github.com/socialpoint-labs/sheetfu/tree/master/sheetfu/modules/table.py,Item,to_dict$353,return {k: self.get_field_value(k) for k in self.header},return {k: self.get_field_value(k) for k in self.header},return {k: self.get_field_value(k) for k in self.header},"tmp_DictComp0 = dict()
for k in self.header:
    tmp_DictComp0[k] = self.get_field_value(k)
return tmp_DictComp0",1,0,1,0
pgmpy,https://github.com/pgmpy/pgmpy/tree/master/pgmpy/inference/EliminationOrder.py,BaseEliminationOrder,get_elimination_order$44,scores = {node: self.cost(node) for node in nodes},scores = {node: self.cost(node) for node in nodes},scores = {node: self.cost(node) for node in nodes},"scores = dict()
for node in nodes:
    scores[node] = self.cost(node)
",0,0,0,0
electricitymap-contrib,https://github.com/tmrowco/electricitymap-contrib/tree/master/parsers/IN.py,,fetch_production$43,"processed_data = {k: float(v.replace(',', '')) for (k, v) in raw_data.items()}","processed_data = {k: float(v.replace(',', '')) for (k, v) in raw_data.items()}","processed_data = {k: float(v.replace(',', '')) for (k, v) in raw_data.items()}","processed_data = dict()
for (k, v) in raw_data.items():
    processed_data[k] = float(v.replace(',', ''))
",0,0,0,0
Auto-PyTorch,https://github.com/automl/Auto-PyTorch/tree/master/test/test_ensemble/test_ensemble.py,,testPerformanceRangeThreshold$263,"ensbuilder.read_preds = {key: {key_2: True for key_2 in (Y_ENSEMBLE, Y_TEST)} for key in ensbuilder.read_losses}","ensbuilder.read_preds = {key: {key_2: True for key_2 in (Y_ENSEMBLE, Y_TEST)} for key in ensbuilder.read_losses}","ensbuilder.read_preds = {key: {key_2: True for key_2 in (Y_ENSEMBLE, Y_TEST)} for key in ensbuilder.read_losses}","tmp_DictComp0 = dict()
for key in ensbuilder.read_losses:
    tmp_DictComp0[key] = {key_2: True for key_2 in (Y_ENSEMBLE, Y_TEST)}
ensbuilder.read_preds = tmp_DictComp0",1,0,1,0
fairseq,https://github.com/pytorch/fairseq/tree/master/examples/textless_nlp/gslm/unit2speech/tacotron2/utils.py,,load_code_dict$66,"code_dict = {c: i for (i, c) in enumerate(codes)}","code_dict = {c: i for (i, c) in enumerate(codes)}","code_dict = {c: i for (i, c) in enumerate(codes)}","code_dict = dict()
for (i, c) in enumerate(codes):
    code_dict[c] = i
",0,0,0,0
checkmk,https://github.com/tribe29/checkmk/tree/master/cmk/base/plugins/agent_based/infoblox_services.py,,parse_infoblox_services$97,"return {SERVICE_ID[service_id]: (status, description) for (service_id, status_id, description) in string_table[0] for status in (STATUS_ID.get(status_id, 'unexpected'),) if status not in {'inactive', 'unknown'}}","return {SERVICE_ID[service_id]: (status, description) for (service_id, status_id, description) in string_table[0] for status in (STATUS_ID.get(status_id, 'unexpected'),) if status not in {'inactive', 'unknown'}}","return {SERVICE_ID[service_id]: (status, description) for (service_id, status_id, description) in string_table[0] for status in (STATUS_ID.get(status_id, 'unexpected'),) if status not in {'inactive', 'unknown'}}","tmp_DictComp0 = dict()
for (service_id, status_id, description) in string_table[0]:
    for status in (STATUS_ID.get(status_id, 'unexpected'),):
        if status not in {'inactive', 'unknown'}:
            tmp_DictComp0[SERVICE_ID[service_id]] = (status, description)
return tmp_DictComp0",1,0,1,0
aiomysql,https://github.com/aio-libs/aiomysql/tree/master/aiomysql/sa/connection.py,SAConnection,_base_params$71,"dp = {c.key: pval for (c, pval) in zip(query.table.c, dp)}","dp = {c.key: pval for (c, pval) in zip(query.table.c, dp)}","dp = {c.key: pval for (c, pval) in zip(query.table.c, dp)}","tmp_DictComp0 = dict()
for (c, pval) in zip(query.table.c, dp):
    tmp_DictComp0[c.key] = pval
dp = tmp_DictComp0",1,0,1,0
programmingbitcoin,https://github.com/jimmysong/programmingbitcoin/tree/master/code-ch07/tx.py,TxFetcher,dump_cache$65,"to_dump = {k: tx.serialize().hex() for (k, tx) in cls.cache.items()}","to_dump = {k: tx.serialize().hex() for (k, tx) in cls.cache.items()}","to_dump = {k: tx.serialize().hex() for (k, tx) in cls.cache.items()}","to_dump = dict()
for (k, tx) in cls.cache.items():
    to_dump[k] = tx.serialize().hex()
",0,0,0,0
python-sdk,https://github.com/qiniu/python-sdk/tree/master/test/unit/test_discovery_v2.py,TestGetTrainingQuery,test_get_training_query_value_error$1853,"req_copy = {key: val if key is not param else None for (key, val) in req_param_dict.items()}","req_copy = {key: val if key is not param else None for (key, val) in req_param_dict.items()}","req_copy = {key: val if key is not param else None for (key, val) in req_param_dict.items()}","req_copy = dict()
for (key, val) in req_param_dict.items():
    req_copy[key] = val if key is not param else None
",0,0,0,0
gluon-ts,https://github.com/awslabs/gluon-ts/tree/master/src/gluonts/core/component.py,,validated$227,"nmargs = {name: arg for ((name, param), arg) in zip(list(init_params.items()), [self] + args) if name != 'self'}","nmargs = {name: arg for ((name, param), arg) in zip(list(init_params.items()), [self] + args) if name != 'self'}","nmargs = {name: arg for ((name, param), arg) in zip(list(init_params.items()), [self] + args) if name != 'self'}","nmargs = dict()
for ((name, param), arg) in zip(list(init_params.items()), [self] + args):
    if name != 'self':
        nmargs[name] = arg
",0,0,0,0
goatools,https://github.com/tanghaibao/goatools/tree/master/tests/test_grpr_get_sections_2d.py,,chk_results$34,"h2i = {h: i for (i, h) in enumerate(hdrgos_act)}","h2i = {h: i for (i, h) in enumerate(hdrgos_act)}","h2i = {h: i for (i, h) in enumerate(hdrgos_act)}","h2i = dict()
for (i, h) in enumerate(hdrgos_act):
    h2i[h] = i
",0,0,0,0
pandapower,https://github.com/e2nIEE/pandapower/tree/master/pandapower/io_utils.py,FromSerializableRegistry,networkx$498,"attr = {k: v for (k, v) in mg.get_edge_data(n1, n2, key=e).items() if k not in ('json_id', 'json_key')}","attr = {k: v for (k, v) in mg.get_edge_data(n1, n2, key=e).items() if k not in ('json_id', 'json_key')}","attr = {k: v for (k, v) in mg.get_edge_data(n1, n2, key=e).items() if k not in ('json_id', 'json_key')}","attr = dict()
for (k, v) in mg.get_edge_data(n1, n2, key=e).items():
    if k not in ('json_id', 'json_key'):
        attr[k] = v
",0,0,0,0
metrics,https://github.com/PyTorchLightning/metrics/tree/master/tests/text/helpers.py,,_class_test$44,"kwargs_update = {k: v.to(device) if isinstance(v, Tensor) else v for (k, v) in kwargs_update.items()}","kwargs_update = {k: v.to(device) if isinstance(v, Tensor) else v for (k, v) in kwargs_update.items()}","kwargs_update = {k: v.to(device) if isinstance(v, Tensor) else v for (k, v) in kwargs_update.items()}","tmp_DictComp0 = dict()
for (k, v) in kwargs_update.items():
    tmp_DictComp0[k] = v.to(device) if isinstance(v, Tensor) else v
kwargs_update = tmp_DictComp0",1,0,1,0
online-judge,https://github.com/DMOJ/online-judge/tree/master/judge/views/api/api_v1.py,,api_v1_problem_list$91,"return JsonResponse({code: {'points': points, 'partial': partial, 'name': name, 'group': group} for (code, points, partial, name, group) in queryset})","JsonResponse({code: {'points': points, 'partial': partial, 'name': name, 'group': group} for (code, points, partial, name, group) in queryset})","JsonResponse({code: {'points': points, 'partial': partial, 'name': name, 'group': group} for (code, points, partial, name, group) in queryset})","tmp_DictComp0 = dict()
for (code, points, partial, name, group) in queryset:
    tmp_DictComp0[code] = {'points': points, 'partial': partial, 'name': name, 'group': group}
return JsonResponse(tmp_DictComp0)",1,0,1,0
featuretools,https://github.com/alteryx/featuretools/tree/master/featuretools/tests/entityset_tests/test_timedelta.py,,test_check_timedelta$65,"exp_to_standard_unit = {e: t for (e, t) in zip(expanded_units, time_units)}","exp_to_standard_unit = {e: t for (e, t) in zip(expanded_units, time_units)}","exp_to_standard_unit = {e: t for (e, t) in zip(expanded_units, time_units)}","exp_to_standard_unit = dict()
for (e, t) in zip(expanded_units, time_units):
    exp_to_standard_unit[e] = t
",0,0,0,0
featuretools,https://github.com/alteryx/featuretools/tree/master/featuretools/computational_backends/feature_set.py,FeatureSet,__init__$23,self.features_by_name = {f.unique_name(): f for f in features},self.features_by_name = {f.unique_name(): f for f in features},self.features_by_name = {f.unique_name(): f for f in features},"self.features_by_name = dict()
for f in features:
    self.features_by_name[f.unique_name()] = f
",0,0,0,0
qlib,https://github.com/microsoft/qlib/tree/master/qlib/data/data.py,LocalInstrumentProvider,list_instruments$666,"_instruments_filtered = {inst: list(filter(lambda x: x[0] <= x[1], [(max(start_time, pd.Timestamp(x[0])), min(end_time, pd.Timestamp(x[1]))) for x in spans])) for (inst, spans) in _instruments.items()}","_instruments_filtered = {inst: list(filter(lambda x: x[0] <= x[1], [(max(start_time, pd.Timestamp(x[0])), min(end_time, pd.Timestamp(x[1]))) for x in spans])) for (inst, spans) in _instruments.items()}","_instruments_filtered = {inst: list(filter(lambda x: x[0] <= x[1], [(max(start_time, pd.Timestamp(x[0])), min(end_time, pd.Timestamp(x[1]))) for x in spans])) for (inst, spans) in _instruments.items()}","_instruments_filtered = dict()
for (inst, spans) in _instruments.items():
    _instruments_filtered[inst] = list(filter(lambda x: x[0] <= x[1], [(max(start_time, pd.Timestamp(x[0])), min(end_time, pd.Timestamp(x[1]))) for x in spans]))
",0,0,0,0
picard,https://github.com/metabrainz/picard/tree/master/picard/config.py,ConfigSection,as_dict$91,"return {key: self[key] for (section, key) in Option.registry if section == self.__name}","return {key: self[key] for (section, key) in Option.registry if section == self.__name}","return {key: self[key] for (section, key) in Option.registry if section == self.__name}","tmp_DictComp0 = dict()
for (section, key) in Option.registry:
    if section == self.__name:
        tmp_DictComp0[key] = self[key]
return tmp_DictComp0",1,0,1,0
conan,https://github.com/conan-io/conan/tree/master/conans/client/rest/rest_client_v2.py,RestV2Methods,get_recipe_sources$102,"urls = {fn: self.router.recipe_file(ref, fn) for fn in files}","urls = {fn: self.router.recipe_file(ref, fn) for fn in files}","urls = {fn: self.router.recipe_file(ref, fn) for fn in files}","urls = dict()
for fn in files:
    urls[fn] = self.router.recipe_file(ref, fn)
",0,0,0,0
deepnl,https://github.com/attardi/deepnl/tree/master/bin/dl-ner.py,,main$84,"tag_index = {t: i for (i, t) in enumerate(tagset)}","tag_index = {t: i for (i, t) in enumerate(tagset)}","tag_index = {t: i for (i, t) in enumerate(tagset)}","tag_index = dict()
for (i, t) in enumerate(tagset):
    tag_index[t] = i
",0,0,0,0
splash,https://github.com/scrapinghub/splash/tree/master/splash/qtrender_lua.py,,decodes_lua_arguments$169,"kwargs = {self.lua.lua2python(k): self.lua.lua2python(v, **l2p_kw) for (k, v) in kwargs.items()}","kwargs = {self.lua.lua2python(k): self.lua.lua2python(v, **l2p_kw) for (k, v) in kwargs.items()}","kwargs = {self.lua.lua2python(k): self.lua.lua2python(v, **l2p_kw) for (k, v) in kwargs.items()}","tmp_DictComp0 = dict()
for (k, v) in kwargs.items():
    tmp_DictComp0[self.lua.lua2python(k)] = self.lua.lua2python(v, **l2p_kw)
kwargs = tmp_DictComp0",1,0,1,0
GFocal,https://github.com/implus/GFocal/tree/master/mmdet/datasets/xml_style.py,XMLDataset,__init__$14,"self.cat2label = {cat: i + 1 for (i, cat) in enumerate(self.CLASSES)}","self.cat2label = {cat: i + 1 for (i, cat) in enumerate(self.CLASSES)}","self.cat2label = {cat: i + 1 for (i, cat) in enumerate(self.CLASSES)}","tmp_DictComp0 = dict()
for (i, cat) in enumerate(self.CLASSES):
    tmp_DictComp0[cat] = i + 1
self.cat2label = tmp_DictComp0",1,0,1,0
ReAgent,https://github.com/facebookresearch/ReAgent/tree/master/reagent/core/tracker.py,,observable$98,self._observers = {v: [] for v in observable_value_types},self._observers = {v: [] for v in observable_value_types},self._observers = {v: [] for v in observable_value_types},"self._observers = dict()
for v in observable_value_types:
    self._observers[v] = []
",0,0,0,0
allennlp,https://github.com/allenai/allennlp/tree/master/allennlp/nn/util.py,,load_state_dict_distributed$2279,"submodule_state_dict = {key.replace(name + '.', '', 1): value for (key, value) in state_dict.items() if key.startswith(name + '.')}","submodule_state_dict = {key.replace(name + '.', '', 1): value for (key, value) in state_dict.items() if key.startswith(name + '.')}","submodule_state_dict = {key.replace(name + '.', '', 1): value for (key, value) in state_dict.items() if key.startswith(name + '.')}","submodule_state_dict = dict()
for (key, value) in state_dict.items():
    if key.startswith(name + '.'):
        submodule_state_dict[key.replace(name + '.', '', 1)] = value
",0,0,0,0
SMAC3,https://github.com/automl/SMAC3/tree/master/test/test_runhistory/test_rfr_imputor.py,ImputorTest,testRealImputation$174,instance_features = {run_key.instance_id: numpy.random.rand(10) for run_key in rh.data},instance_features = {run_key.instance_id: numpy.random.rand(10) for run_key in rh.data},instance_features = {run_key.instance_id: numpy.random.rand(10) for run_key in rh.data},"instance_features = dict()
for run_key in rh.data:
    instance_features[run_key.instance_id] = numpy.random.rand(10)
",0,0,0,0
shuup,https://github.com/shuup/shuup/tree/master/shuup/core/pricing/_discounts.py,DiscountModule,discount_prices$60,"return {pk: self.discount_price(context, product_map[pk], price_info) for (pk, price_info) in six.iteritems(price_infos)}","return {pk: self.discount_price(context, product_map[pk], price_info) for (pk, price_info) in six.iteritems(price_infos)}","return {pk: self.discount_price(context, product_map[pk], price_info) for (pk, price_info) in six.iteritems(price_infos)}","tmp_DictComp0 = dict()
for (pk, price_info) in six.iteritems(price_infos):
    tmp_DictComp0[pk] = self.discount_price(context, product_map[pk], price_info)
return tmp_DictComp0",1,0,1,0
nni,https://github.com/microsoft/nni/tree/master/examples/trials/ga_squad/trial.py,,load_data$375,"word_vcb = {key: value + 1 for (key, value) in word_vcb.items()}","word_vcb = {key: value + 1 for (key, value) in word_vcb.items()}","word_vcb = {key: value + 1 for (key, value) in word_vcb.items()}","tmp_DictComp0 = dict()
for (key, value) in word_vcb.items():
    tmp_DictComp0[key] = value + 1
word_vcb = tmp_DictComp0",1,0,1,0
bi-att-flow,https://github.com/allenai/bi-att-flow/tree/master/basic/read_data.py,,read_data$158,"idx2vec_dict = {idx: word2vec_dict[word] for (word, idx) in new_word2idx_dict.items()}","idx2vec_dict = {idx: word2vec_dict[word] for (word, idx) in new_word2idx_dict.items()}","idx2vec_dict = {idx: word2vec_dict[word] for (word, idx) in new_word2idx_dict.items()}","idx2vec_dict = dict()
for (word, idx) in new_word2idx_dict.items():
    idx2vec_dict[idx] = word2vec_dict[word]
",0,0,0,0
psycopg,https://github.com/psycopg/psycopg/tree/master/psycopg/psycopg/pq/pq_ctypes.py,Conninfo,_options_from_array$938,"d = {kw: getattr(opt, kw) for kw in skws}","d = {kw: getattr(opt, kw) for kw in skws}","d = {kw: getattr(opt, kw) for kw in skws}","d = dict()
for kw in skws:
    d[kw] = getattr(opt, kw)
",0,0,0,0
qiskit-terra,https://github.com/Qiskit/qiskit-terra/tree/master/test/python/transpiler/test_stochastic_swap.py,TestStochasticSwapRandomCircuitValidOutput,assert_valid_circuit$1224,"_visit_block(transpiled, qubit_mapping={qubit: index for (index, qubit) in enumerate(transpiled.qubits)})","qubit_mapping={qubit: index for (index, qubit) in enumerate(transpiled.qubits)}","qubit_mapping={qubit: index for (index, qubit) in enumerate(transpiled.qubits)}","def my_comprehension_func(transpiled):
    tmp_DictComp0 = dict()
    for (index, qubit) in enumerate(transpiled.qubits):
        tmp_DictComp0[qubit] = index
    return tmp_DictComp0
_visit_block(transpiled, qubit_mapping=my_comprehension_func(transpiled))",1,1,1,0
Bert-Multi-Label-Text-Classification,https://github.com/lonePatient/Bert-Multi-Label-Text-Classification/tree/master//run_albert.py,,run_train$25,"id2label = {i: label for (i, label) in enumerate(label_list)}","id2label = {i: label for (i, label) in enumerate(label_list)}","id2label = {i: label for (i, label) in enumerate(label_list)}","id2label = dict()
for (i, label) in enumerate(label_list):
    id2label[i] = label
",0,0,0,0
splash,https://github.com/scrapinghub/splash/tree/master/splash/engines/webkit/render_scripts.py,JsonRender,start$155,self.include = {inc: kwargs.pop(inc) for inc in include_options},self.include = {inc: kwargs.pop(inc) for inc in include_options},self.include = {inc: kwargs.pop(inc) for inc in include_options},"self.include = dict()
for inc in include_options:
    self.include[inc] = kwargs.pop(inc)
",0,0,0,0
mesh,https://github.com/tensorflow/mesh/tree/master/mesh_tensorflow/transformer/dataset.py,,pack_dataset$479,"return {k: tf.reshape(v, [length[k]]) for (k, v) in x.items()}","return {k: tf.reshape(v, [length[k]]) for (k, v) in x.items()}","return {k: tf.reshape(v, [length[k]]) for (k, v) in x.items()}","tmp_DictComp0 = dict()
for (k, v) in x.items():
    tmp_DictComp0[k] = tf.reshape(v, [length[k]])
return tmp_DictComp0",1,0,1,0
mopidy,https://github.com/mopidy/mopidy/tree/master/mopidy/core/playlists.py,PlaylistsController,as_list$47,futures = {backend: backend.playlists.as_list() for backend in set(self.backends.with_playlists.values())},futures = {backend: backend.playlists.as_list() for backend in set(self.backends.with_playlists.values())},futures = {backend: backend.playlists.as_list() for backend in set(self.backends.with_playlists.values())},"futures = dict()
for backend in set(self.backends.with_playlists.values()):
    futures[backend] = backend.playlists.as_list()
",0,0,0,0
hent-AI,https://github.com/natethegreate/hent-AI/tree/master/mrcnn/utils.py,Dataset,prepare$294,"self.image_from_source_map = {'{}.{}'.format(info['source'], info['id']): id for (info, id) in zip(self.image_info, self.image_ids)}","self.image_from_source_map = {'{}.{}'.format(info['source'], info['id']): id for (info, id) in zip(self.image_info, self.image_ids)}","self.image_from_source_map = {'{}.{}'.format(info['source'], info['id']): id for (info, id) in zip(self.image_info, self.image_ids)}","tmp_DictComp0 = dict()
for (info, id) in zip(self.image_info, self.image_ids):
    tmp_DictComp0['{}.{}'.format(info['source'], info['id'])] = id
self.image_from_source_map = tmp_DictComp0",1,0,1,0
autonomous-learning-library,https://github.com/cpnota/autonomous-learning-library/tree/master/all/core/state.py,StateArray,__getitem__$360,"return StateArray({k: v[key] for (k, v) in self.items()}, shape, device=self.device)","StateArray({k: v[key] for (k, v) in self.items()}, shape, device=self.device)","StateArray({k: v[key] for (k, v) in self.items()}, shape, device=self.device)","tmp_DictComp0 = dict()
for (k, v) in self.items():
    tmp_DictComp0[k] = v[key]
return StateArray(tmp_DictComp0, shape, device=self.device)",1,0,1,0
airflow,https://github.com/apache/airflow/tree/master/tests/operators/test_python.py,TestPythonBase,_assert_calls_equal$109,"first.kwargs = {key: value for (key, value) in first.kwargs.items() if key in test_args}","first.kwargs = {key: value for (key, value) in first.kwargs.items() if key in test_args}","first.kwargs = {key: value for (key, value) in first.kwargs.items() if key in test_args}","tmp_DictComp0 = dict()
for (key, value) in first.kwargs.items():
    if key in test_args:
        tmp_DictComp0[key] = value
first.kwargs = tmp_DictComp0",1,0,1,0
qiskit-terra,https://github.com/Qiskit/qiskit-terra/tree/master/qiskit/opflow/state_fns/sparse_vector_state_fn.py,SparseVectorStateFn,to_dict_fn$109,"new_dict = {format(i[1], 'b').zfill(num_qubits): v for (i, v) in dok.items()}","new_dict = {format(i[1], 'b').zfill(num_qubits): v for (i, v) in dok.items()}","new_dict = {format(i[1], 'b').zfill(num_qubits): v for (i, v) in dok.items()}","new_dict = dict()
for (i, v) in dok.items():
    new_dict[format(i[1], 'b').zfill(num_qubits)] = v
",0,0,0,0
joinmarket-clientserver,https://github.com/JoinMarket-Org/joinmarket-clientserver/tree/master/jmclient/jmclient/wallet.py,UTXOManager,select_utxos$231,"return {s['utxo']: {'path': utxos[s['utxo']][0], 'value': utxos[s['utxo']][1]} for s in selected}","return {s['utxo']: {'path': utxos[s['utxo']][0], 'value': utxos[s['utxo']][1]} for s in selected}","return {s['utxo']: {'path': utxos[s['utxo']][0], 'value': utxos[s['utxo']][1]} for s in selected}","tmp_DictComp0 = dict()
for s in selected:
    tmp_DictComp0[s['utxo']] = {'path': utxos[s['utxo']][0], 'value': utxos[s['utxo']][1]}
return tmp_DictComp0",1,0,1,0
hydrus,https://github.com/hydrusnetwork/hydrus/tree/master/hydrus/server/ServerDB.py,DB,_ModifyAccountTypes$1370,future_account_type_keys_to_account_types = {account_type.GetAccountTypeKey(): account_type for account_type in account_types},future_account_type_keys_to_account_types = {account_type.GetAccountTypeKey(): account_type for account_type in account_types},future_account_type_keys_to_account_types = {account_type.GetAccountTypeKey(): account_type for account_type in account_types},"future_account_type_keys_to_account_types = dict()
for account_type in account_types:
    future_account_type_keys_to_account_types[account_type.GetAccountTypeKey()] = account_type
",0,0,0,0
fiber,https://github.com/uber/fiber/tree/master/fiber/config.py,,get_dict$210,return {k: global_vars[k] for k in vars(_current_config)},return {k: global_vars[k] for k in vars(_current_config)},return {k: global_vars[k] for k in vars(_current_config)},"tmp_DictComp0 = dict()
for k in vars(_current_config):
    tmp_DictComp0[k] = global_vars[k]
return tmp_DictComp0",1,0,1,0
DLTK,https://github.com/DLTK/DLTK/tree/master/dltk/utils.py,,sliding_window_segmentation_inference$83,"padded_dict = {k: np.pad(v, padding, mode='constant') for (k, v) in sample_dict.items()}","padded_dict = {k: np.pad(v, padding, mode='constant') for (k, v) in sample_dict.items()}","padded_dict = {k: np.pad(v, padding, mode='constant') for (k, v) in sample_dict.items()}","padded_dict = dict()
for (k, v) in sample_dict.items():
    padded_dict[k] = np.pad(v, padding, mode='constant')
",0,0,0,0
Just-Code,https://github.com/YaxeZhang/Just-Code/tree/master/src/1255.maximum-score-words-formed-by-letters/maximum-score-words-formed-by-letters.py,Solution,maxScoreWords$2,"dfs(j + 1, curr_score + words_score[j], {c: n - wcnt.get(c, 0) for (c, n) in counter.items()})","dfs(j + 1, curr_score + words_score[j], {c: n - wcnt.get(c, 0) for (c, n) in counter.items()})","dfs(j + 1, curr_score + words_score[j], {c: n - wcnt.get(c, 0) for (c, n) in counter.items()})","tmp_DictComp0 = dict()
for (c, n) in counter.items():
    tmp_DictComp0[c] = n - wcnt.get(c, 0)
dfs(j + 1, curr_score + words_score[j], tmp_DictComp0)",1,0,1,0
transformers,https://github.com/huggingface/transformers/tree/master/examples/research_projects/jax-projects/big_bird/prepare_natural_questions.py,,_get_single_answer$16,a = {k: [a[k]] for k in a},a = {k: [a[k]] for k in a},a = {k: [a[k]] for k in a},"tmp_DictComp0 = dict()
for k in a:
    tmp_DictComp0[k] = [a[k]]
a = tmp_DictComp0",1,0,1,0
buffalo,https://github.com/kakao/buffalo/tree/master/buffalo/algo/base.py,Algo,build_userid_map$178,self._idmanager.userid_map = {str(i): i for i in range(header['num_users'])},self._idmanager.userid_map = {str(i): i for i in range(header['num_users'])},self._idmanager.userid_map = {str(i): i for i in range(header['num_users'])},"self._idmanager.userid_map = dict()
for i in range(header['num_users']):
    self._idmanager.userid_map[str(i)] = i
",0,0,0,0
programmingbitcoin,https://github.com/jimmysong/programmingbitcoin/tree/master/code-ch08/tx.py,TxFetcher,dump_cache$65,"to_dump = {k: tx.serialize().hex() for (k, tx) in cls.cache.items()}","to_dump = {k: tx.serialize().hex() for (k, tx) in cls.cache.items()}","to_dump = {k: tx.serialize().hex() for (k, tx) in cls.cache.items()}","to_dump = dict()
for (k, tx) in cls.cache.items():
    to_dump[k] = tx.serialize().hex()
",0,0,0,0
dbt-core,https://github.com/dbt-labs/dbt-core/tree/master/test/unit/test_linker.py,,_mock_manifest$16,"manifest = mock.MagicMock(nodes={n: mock.MagicMock(unique_id=n, package_name='pkg', name=n, empty=False, config=config, fqn=['pkg', n]) for n in nodes})","nodes={n: mock.MagicMock(unique_id=n, package_name='pkg', name=n, empty=False, config=config, fqn=['pkg', n]) for n in nodes}","nodes={n: mock.MagicMock(unique_id=n, package_name='pkg', name=n, empty=False, config=config, fqn=['pkg', n]) for n in nodes}","def my_comprehension_func(mock):
    tmp_DictComp0 = dict()
    for n in nodes:
        tmp_DictComp0[n] = mock.MagicMock(unique_id=n, package_name='pkg', name=n, empty=False, config=config, fqn=['pkg', n])
    return tmp_DictComp0
manifest = mock.MagicMock(nodes=my_comprehension_func(mock))",1,1,1,0
ByteTrack,https://github.com/ifzhang/ByteTrack/tree/master/tutorials/motr/motr.py,ClipMatcher,match_for_single_frame$175,"self.losses_dict.update({'frame_{}_aux{}_{}'.format(self._current_frame_idx, i, key): value for (key, value) in l_dict.items()})","self.losses_dict.update({'frame_{}_aux{}_{}'.format(self._current_frame_idx, i, key): value for (key, value) in l_dict.items()})","self.losses_dict.update({'frame_{}_aux{}_{}'.format(self._current_frame_idx, i, key): value for (key, value) in l_dict.items()})","def my_comprehension_func(self):
    tmp_DictComp0 = dict()
    for (key, value) in l_dict.items():
        tmp_DictComp0['frame_{}_aux{}_{}'.format(self._current_frame_idx, i, key)] = value
    return tmp_DictComp0
self.losses_dict.update(my_comprehension_func(self))",1,1,1,0
holoviews,https://github.com/holoviz/holoviews/tree/master/holoviews/tests/plotting/matplotlib/test_overlayplot.py,TestOverlayPlot,cb$63,"return NdOverlay({i: Curve(np.arange(10) + i) for i in range(X - 2, X)})","NdOverlay({i: Curve(np.arange(10) + i) for i in range(X - 2, X)})","NdOverlay({i: Curve(np.arange(10) + i) for i in range(X - 2, X)})","tmp_DictComp0 = dict()
for i in range(X - 2, X):
    tmp_DictComp0[i] = Curve(np.arange(10) + i)
return NdOverlay(tmp_DictComp0)",1,0,1,0
matplotlib,https://github.com/matplotlib/matplotlib/tree/master/lib/matplotlib/axes/_base.py,_AxesBase,ticklabel_format$3282,"axis_map = {**{k: [v] for (k, v) in self._axis_map.items()}, 'both': list(self._axis_map.values())}","{**{k: [v] for (k, v) in self._axis_map.items()}, 'both': list(self._axis_map.values())}","{**{k: [v] for (k, v) in self._axis_map.items()}, 'both': list(self._axis_map.values())}","def my_comprehension_func(self):
    tmp_DictComp0 = dict()
    for (k, v) in self._axis_map.items():
        tmp_DictComp0[k] = [v]
    return tmp_DictComp0
axis_map = {**my_comprehension_func(self), 'both': list(self._axis_map.values())}",1,1,1,0
Rasa_NLU_Chi,https://github.com/crownpku/Rasa_NLU_Chi/tree/master/rasa_nlu/evaluate.py,,combine_intent_result$546,"return {k: v + results[k] for (k, v) in current_result.items()}","return {k: v + results[k] for (k, v) in current_result.items()}","return {k: v + results[k] for (k, v) in current_result.items()}","tmp_DictComp0 = dict()
for (k, v) in current_result.items():
    tmp_DictComp0[k] = v + results[k]
return tmp_DictComp0",1,0,1,0
big_transfer,https://github.com/google-research/big_transfer/tree/master/bit_pytorch/lbtoolbox.py,Chrono,__str__$106,"avgtimes = {k: self.avgtime(k, dropfirst) for k in self.timings}","avgtimes = {k: self.avgtime(k, dropfirst) for k in self.timings}","avgtimes = {k: self.avgtime(k, dropfirst) for k in self.timings}","avgtimes = dict()
for k in self.timings:
    avgtimes[k] = self.avgtime(k, dropfirst)
",0,0,0,0
gql,https://github.com/graphql-python/gql/tree/master/gql/utilities/serialize_variable_values.py,,serialize_value$53,"return {field_name: serialize_value(field.type, value[field_name]) for (field_name, field) in type_.fields.items()}","return {field_name: serialize_value(field.type, value[field_name]) for (field_name, field) in type_.fields.items()}","return {field_name: serialize_value(field.type, value[field_name]) for (field_name, field) in type_.fields.items()}","tmp_DictComp0 = dict()
for (field_name, field) in type_.fields.items():
    tmp_DictComp0[field_name] = serialize_value(field.type, value[field_name])
return tmp_DictComp0",1,0,1,0
dagster,https://github.com/dagster-io/dagster/tree/master/python_modules/dagster-graphql/dagster_graphql/schema/pipelines/pipeline.py,GrapheneGraph,resolve_solid_handles$869,"handles = {key: handle for (key, handle) in handles.items() if handle.parent and handle.parent.handleID.to_string() == parentHandleID}","handles = {key: handle for (key, handle) in handles.items() if handle.parent and handle.parent.handleID.to_string() == parentHandleID}","handles = {key: handle for (key, handle) in handles.items() if handle.parent and handle.parent.handleID.to_string() == parentHandleID}","tmp_DictComp0 = dict()
for (key, handle) in handles.items():
    if handle.parent and handle.parent.handleID.to_string() == parentHandleID:
        tmp_DictComp0[key] = handle
handles = tmp_DictComp0",1,0,1,0
torchdistill,https://github.com/yoshitomo-matsubara/torchdistill/tree/master/torchdistill/eval/coco.py,,evaluate$381,"self.ious = {(imgId, catId): computeIoU(imgId, catId) for imgId in p.imgIds for catId in catIds}","self.ious = {(imgId, catId): computeIoU(imgId, catId) for imgId in p.imgIds for catId in catIds}","self.ious = {(imgId, catId): computeIoU(imgId, catId) for imgId in p.imgIds for catId in catIds}","self.ious = dict()
for imgId in p.imgIds:
    for catId in catIds:
        self.ious[imgId, catId] = computeIoU(imgId, catId)
",0,0,0,0
sentry,https://github.com/getsentry/sentry/tree/master/src/sentry/api/serializers/models/alert_rule.py,DetailedAlertRuleSerializer,get_attrs$172,query_to_alert_rule = {ar.snuba_query_id: ar for ar in item_list},query_to_alert_rule = {ar.snuba_query_id: ar for ar in item_list},query_to_alert_rule = {ar.snuba_query_id: ar for ar in item_list},"query_to_alert_rule = dict()
for ar in item_list:
    query_to_alert_rule[ar.snuba_query_id] = ar
",0,0,0,0
sanic,https://github.com/sanic-org/sanic/tree/master/tests/test_url_building.py,,test_fails_url_build_if_param_not_passed$114,fail_kwargs = {l: l for l in fail_args},fail_kwargs = {l: l for l in fail_args},fail_kwargs = {l: l for l in fail_args},"fail_kwargs = dict()
for l in fail_args:
    fail_kwargs[l] = l
",0,0,0,0
sentry,https://github.com/getsentry/sentry/tree/master/src/sentry/api/serializers/models/activity.py,ActivitySerializer,get_attrs$15,"return {item: {'user': users[str(item.user_id)] if item.user_id else None, 'source': groups.get(item.data['source_id']) if item.type == ActivityType.UNMERGE_DESTINATION.value else None, 'destination': groups.get(item.data['destination_id']) if item.type == ActivityType.UNMERGE_SOURCE.value else None, 'commit': commits.get(item), 'pull_request': pull_requests.get(item)} for item in item_list}","return {item: {'user': users[str(item.user_id)] if item.user_id else None, 'source': groups.get(item.data['source_id']) if item.type == ActivityType.UNMERGE_DESTINATION.value else None, 'destination': groups.get(item.data['destination_id']) if item.type == ActivityType.UNMERGE_SOURCE.value else None, 'commit': commits.get(item), 'pull_request': pull_requests.get(item)} for item in item_list}","return {item: {'user': users[str(item.user_id)] if item.user_id else None, 'source': groups.get(item.data['source_id']) if item.type == ActivityType.UNMERGE_DESTINATION.value else None, 'destination': groups.get(item.data['destination_id']) if item.type == ActivityType.UNMERGE_SOURCE.value else None, 'commit': commits.get(item), 'pull_request': pull_requests.get(item)} for item in item_list}","tmp_DictComp0 = dict()
for item in item_list:
    tmp_DictComp0[item] = {'user': users[str(item.user_id)] if item.user_id else None, 'source': groups.get(item.data['source_id']) if item.type == ActivityType.UNMERGE_DESTINATION.value else None, 'destination': groups.get(item.data['destination_id']) if item.type == ActivityType.UNMERGE_SOURCE.value else None, 'commit': commits.get(item), 'pull_request': pull_requests.get(item)}
return tmp_DictComp0",1,0,1,0
conda,https://github.com/conda/conda/tree/master/conda/resolve.py,Resolve,generate_feature_count$929,"result = {self.push_MatchSpec(C, MatchSpec(track_features=name)): 1 for name in iterkeys(self.trackers)}","result = {self.push_MatchSpec(C, MatchSpec(track_features=name)): 1 for name in iterkeys(self.trackers)}","result = {self.push_MatchSpec(C, MatchSpec(track_features=name)): 1 for name in iterkeys(self.trackers)}","result = dict()
for name in iterkeys(self.trackers):
    result[self.push_MatchSpec(C, MatchSpec(track_features=name))] = 1
",0,0,0,0
qiskit-terra,https://github.com/Qiskit/qiskit-terra/tree/master/qiskit/algorithms/amplitude_estimators/ae.py,AmplitudeEstimation,estimate$336,"result.circuit_results = {np.binary_repr(k, circuit.num_qubits): round(v * shots) for (k, v) in ret.quasi_dists[0].items()}","result.circuit_results = {np.binary_repr(k, circuit.num_qubits): round(v * shots) for (k, v) in ret.quasi_dists[0].items()}","result.circuit_results = {np.binary_repr(k, circuit.num_qubits): round(v * shots) for (k, v) in ret.quasi_dists[0].items()}","result.circuit_results = dict()
for (k, v) in ret.quasi_dists[0].items():
    result.circuit_results[np.binary_repr(k, circuit.num_qubits)] = round(v * shots)
",0,0,0,0
feincms,https://github.com/feincms/feincms/tree/master/feincms/templatetags/feincms_page_tags.py,,_translate_page_into$240,translations = {t.language: t for t in page.available_translations()},translations = {t.language: t for t in page.available_translations()},translations = {t.language: t for t in page.available_translations()},"translations = dict()
for t in page.available_translations():
    translations[t.language] = t
",0,0,0,0
django,https://github.com/django/django/tree/master/django/utils/datastructures.py,MultiValueDict,dict$216,return {key: self[key] for key in self},return {key: self[key] for key in self},return {key: self[key] for key in self},"tmp_DictComp0 = dict()
for key in self:
    tmp_DictComp0[key] = self[key]
return tmp_DictComp0",1,0,1,0
holoviews,https://github.com/holoviz/holoviews/tree/master/holoviews/plotting/bokeh/chart.py,ErrorPlot,_init_glyph$567,"properties = {k: v for (k, v) in properties.items() if 'legend' not in k}","properties = {k: v for (k, v) in properties.items() if 'legend' not in k}","properties = {k: v for (k, v) in properties.items() if 'legend' not in k}","tmp_DictComp0 = dict()
for (k, v) in properties.items():
    if 'legend' not in k:
        tmp_DictComp0[k] = v
properties = tmp_DictComp0",1,0,1,0
DeepLearning,https://github.com/MingchaoZhu/DeepLearning/tree/master/code/chapter6.py,FullyConnected,hyperparams$162,"return {'layer': 'FullyConnected', 'init_w': self.init_w, 'n_in': self.n_in, 'n_out': self.n_out, 'acti_fn': str(self.acti_fn), 'optimizer': {'hyperparams': self.optimizer.hyperparams}, 'components': {k: v for (k, v) in self.params.items()}}","{'layer': 'FullyConnected', 'init_w': self.init_w, 'n_in': self.n_in, 'n_out': self.n_out, 'acti_fn': str(self.acti_fn), 'optimizer': {'hyperparams': self.optimizer.hyperparams}, 'components': {k: v for (k, v) in self.params.items()}}","{'layer': 'FullyConnected', 'init_w': self.init_w, 'n_in': self.n_in, 'n_out': self.n_out, 'acti_fn': str(self.acti_fn), 'optimizer': {'hyperparams': self.optimizer.hyperparams}, 'components': {k: v for (k, v) in self.params.items()}}","def my_comprehension_func(self):
    tmp_DictComp0 = dict()
    for (k, v) in self.params.items():
        tmp_DictComp0[k] = v
    return tmp_DictComp0
return {'layer': 'FullyConnected', 'init_w': self.init_w, 'n_in': self.n_in, 'n_out': self.n_out, 'acti_fn': str(self.acti_fn), 'optimizer': {'hyperparams': self.optimizer.hyperparams}, 'components': my_comprehension_func(self)}",1,1,1,0
distributed,https://github.com/dask/distributed/tree/master/distributed/dashboard/components/scheduler.py,WorkerTable,update$3729,data = {name: [] for name in self.names + self.extra_names},data = {name: [] for name in self.names + self.extra_names},data = {name: [] for name in self.names + self.extra_names},"data = dict()
for name in self.names + self.extra_names:
    data[name] = []
",0,0,0,0
astropy,https://github.com/astropy/astropy/tree/master/astropy/coordinates/representation.py,BaseRepresentation,transform$929,difs_cls = {k: CartesianDifferential for k in self.differentials.keys()},difs_cls = {k: CartesianDifferential for k in self.differentials.keys()},difs_cls = {k: CartesianDifferential for k in self.differentials.keys()},"difs_cls = dict()
for k in self.differentials.keys():
    difs_cls[k] = CartesianDifferential
",0,0,0,0
cloud-custodian,https://github.com/cloud-custodian/cloud-custodian/tree/master/c7n/resources/appelb.py,SetS3Logging,process$329,"info = {t['Key']: t['Value'] for t in elb.get('Tags', ())}","info = {t['Key']: t['Value'] for t in elb.get('Tags', ())}","info = {t['Key']: t['Value'] for t in elb.get('Tags', ())}","info = dict()
for t in elb.get('Tags', ()):
    info[t['Key']] = t['Value']
",0,0,0,0
batch-ppo,https://github.com/google-research/batch-ppo/tree/master/agents/tools/nested.py,,impl$150,"filtered = {k: v for (k, v) in filtered.items() if not isinstance(v, (tuple, list, dict)) or v}","filtered = {k: v for (k, v) in filtered.items() if not isinstance(v, (tuple, list, dict)) or v}","filtered = {k: v for (k, v) in filtered.items() if not isinstance(v, (tuple, list, dict)) or v}","tmp_DictComp0 = dict()
for (k, v) in filtered.items():
    if not isinstance(v, (tuple, list, dict)) or v:
        tmp_DictComp0[k] = v
filtered = tmp_DictComp0",1,0,1,0
nlp-architect,https://github.com/IntelLabs/nlp-architect/tree/master/solutions/InterpreT/application/tasks.py,Task,get_dropdown_to_df_col_map$157,"return {displayed_option: internal_option for (displayed_option, internal_option) in zip(self.tsne_displayed_options, self.tsne_internal_options)}","return {displayed_option: internal_option for (displayed_option, internal_option) in zip(self.tsne_displayed_options, self.tsne_internal_options)}","return {displayed_option: internal_option for (displayed_option, internal_option) in zip(self.tsne_displayed_options, self.tsne_internal_options)}","tmp_DictComp0 = dict()
for (displayed_option, internal_option) in zip(self.tsne_displayed_options, self.tsne_internal_options):
    tmp_DictComp0[displayed_option] = internal_option
return tmp_DictComp0",1,0,1,0
pingouin,https://github.com/raphaelvallat/pingouin/tree/master/pingouin/plotting.py,,plot_paired$409,"data['wthn'] = data[within].replace({_ordr: i for (i, _ordr) in enumerate(order)})","data[within].replace({_ordr: i for (i, _ordr) in enumerate(order)})","data[within].replace({_ordr: i for (i, _ordr) in enumerate(order)})","tmp_DictComp0 = dict()
for (i, _ordr) in enumerate(order):
    tmp_DictComp0[_ordr] = i
data['wthn'] = data[within].replace(tmp_DictComp0)",1,0,1,0
python-sdk,https://github.com/qiniu/python-sdk/tree/master/test/unit/test_assistant_v2.py,TestCreateSession,test_create_session_value_error$90,"req_copy = {key: val if key is not param else None for (key, val) in req_param_dict.items()}","req_copy = {key: val if key is not param else None for (key, val) in req_param_dict.items()}","req_copy = {key: val if key is not param else None for (key, val) in req_param_dict.items()}","req_copy = dict()
for (key, val) in req_param_dict.items():
    req_copy[key] = val if key is not param else None
",0,0,0,0
ReAgent,https://github.com/facebookresearch/ReAgent/tree/master/reagent/replay_memory/circular_replay_buffer.py,IDScoreListMetadata,zero_example$218,"return {k: ([], []) for k in self.keys}","return {k: ([], []) for k in self.keys}","return {k: ([], []) for k in self.keys}","tmp_DictComp0 = dict()
for k in self.keys:
    tmp_DictComp0[k] = ([], [])
return tmp_DictComp0",1,0,1,0
OneNet,https://github.com/PeizeSun/OneNet/tree/master/detectron2/evaluation/panoptic_evaluation.py,COCOPanopticEvaluator,__init__$31,"self._stuff_contiguous_id_to_dataset_id = {v: k for (k, v) in self._metadata.stuff_dataset_id_to_contiguous_id.items()}","self._stuff_contiguous_id_to_dataset_id = {v: k for (k, v) in self._metadata.stuff_dataset_id_to_contiguous_id.items()}","self._stuff_contiguous_id_to_dataset_id = {v: k for (k, v) in self._metadata.stuff_dataset_id_to_contiguous_id.items()}","tmp_DictComp0 = dict()
for (k, v) in self._metadata.stuff_dataset_id_to_contiguous_id.items():
    tmp_DictComp0[v] = k
self._stuff_contiguous_id_to_dataset_id = tmp_DictComp0",1,0,1,0
PyWebIO,https://github.com/pywebio/PyWebIO/tree/master/pywebio/io_ctrl.py,,input_event_handle$312,"data = {name: preprocess_funcs[name](val) for (name, val) in event_data.items()}","data = {name: preprocess_funcs[name](val) for (name, val) in event_data.items()}","data = {name: preprocess_funcs[name](val) for (name, val) in event_data.items()}","data = dict()
for (name, val) in event_data.items():
    data[name] = preprocess_funcs[name](val)
",0,0,0,0
cloud-inquisitor,https://github.com/RiotGames/cloud-inquisitor/tree/master/plugins/public/cinq-collector-aws/cinq_collector_aws/account.py,AWSAccountCollector,update_route53$319,"record = DNSRecord.create(data['id'], account_id=self.account.account_id, properties={k: v for (k, v) in data.items() if k != 'id'}, tags={})","properties={k: v for (k, v) in data.items() if k != 'id'}","properties={k: v for (k, v) in data.items() if k != 'id'}","def my_comprehension_func(data):
    tmp_DictComp0 = dict()
    for (k, v) in data.items():
        if k != 'id':
            tmp_DictComp0[k] = v
    return tmp_DictComp0
record = DNSRecord.create(data['id'], account_id=self.account.account_id, properties=my_comprehension_func(data), tags={})",1,1,1,0
hydrus,https://github.com/hydrusnetwork/hydrus/tree/master/hydrus/client/gui/pages/ClientGUIResults.py,MediaPanelThumbnails,do_it$4255,hashes_to_media_results = {media_result.GetHash(): media_result for media_result in media_results},hashes_to_media_results = {media_result.GetHash(): media_result for media_result in media_results},hashes_to_media_results = {media_result.GetHash(): media_result for media_result in media_results},"hashes_to_media_results = dict()
for media_result in media_results:
    hashes_to_media_results[media_result.GetHash()] = media_result
",0,0,0,0
pretix,https://github.com/pretix/pretix/tree/master/src/pretix/control/views/organizer.py,OrganizerUpdate,form_valid$420,"self.request.organizer.log_action('pretix.organizer.changed', user=self.request.user, data={k: form.cleaned_data.get(k) for k in form.changed_data})",data={k: form.cleaned_data.get(k) for k in form.changed_data},data={k: form.cleaned_data.get(k) for k in form.changed_data},"tmp_DictComp0 = dict()
for k in form.changed_data:
    tmp_DictComp0[k] = form.cleaned_data.get(k)
self.request.organizer.log_action('pretix.organizer.changed', user=self.request.user, data=tmp_DictComp0)",1,0,1,0
transform,https://github.com/tensorflow/transform/tree/master/tensorflow_transform/saved/saved_transform_io_v2_test.py,SavedTransformIOV2Test,test_restore_from_v1_saved_model_with_pyfuncs$583,"input_kwargs = {k: v for (k, v) in zip(input_keys, inputs)}","input_kwargs = {k: v for (k, v) in zip(input_keys, inputs)}","input_kwargs = {k: v for (k, v) in zip(input_keys, inputs)}","input_kwargs = dict()
for (k, v) in zip(input_keys, inputs):
    input_kwargs[k] = v
",0,0,0,0
rewriting,https://github.com/davidbau/rewriting/tree/master/rewrite/ganrewrite.py,SeqStyleGanRewriter,detach$708,"return type(v)({k: d.detach() for (k, d) in v.items()})","type(v)({k: d.detach() for (k, d) in v.items()})","type(v)({k: d.detach() for (k, d) in v.items()})","tmp_DictComp0 = dict()
for (k, d) in v.items():
    tmp_DictComp0[k] = d.detach()
return type(v)(tmp_DictComp0)",1,0,1,0
graphql-compiler,https://github.com/kensho-technologies/graphql-compiler/tree/master/graphql_compiler/tests/snapshot_tests/test_query_pagination.py,QueryPaginationTests,test_pagination_datetime_existing_filter$689,uuid4_field_info = {vertex_name: {'uuid': UUIDOrdering.LeftToRight} for vertex_name in schema_graph.vertex_class_names},uuid4_field_info = {vertex_name: {'uuid': UUIDOrdering.LeftToRight} for vertex_name in schema_graph.vertex_class_names},uuid4_field_info = {vertex_name: {'uuid': UUIDOrdering.LeftToRight} for vertex_name in schema_graph.vertex_class_names},"uuid4_field_info = dict()
for vertex_name in schema_graph.vertex_class_names:
    uuid4_field_info[vertex_name] = {'uuid': UUIDOrdering.LeftToRight}
",0,0,0,0
sunpy,https://github.com/sunpy/sunpy/tree/master/sunpy/util/metadata.py,MetaDict,removed_items$89,return {k: self.original_meta[k] for k in set(self.original_meta) - set(self)},return {k: self.original_meta[k] for k in set(self.original_meta) - set(self)},return {k: self.original_meta[k] for k in set(self.original_meta) - set(self)},"tmp_DictComp0 = dict()
for k in set(self.original_meta) - set(self):
    tmp_DictComp0[k] = self.original_meta[k]
return tmp_DictComp0",1,0,1,0
PaddleDetection,https://github.com/PaddlePaddle/PaddleDetection/tree/master/ppdet/engine/trainer.py,Trainer,_init_metrics$162,"clsid2catid = {v: k for (k, v) in self.dataset.catid2clsid.items()} if self.mode == 'eval' else None","{v: k for (k, v) in self.dataset.catid2clsid.items()} if self.mode == 'eval' else None","{v: k for (k, v) in self.dataset.catid2clsid.items()} if self.mode == 'eval' else None","def my_comprehension_func(self):
    tmp_DictComp0 = dict()
    for (k, v) in self.dataset.catid2clsid.items():
        tmp_DictComp0[v] = k
    return tmp_DictComp0
clsid2catid = my_comprehension_func(self) if self.mode == 'eval' else None",1,1,1,0
dreamerv2,https://github.com/danijar/dreamerv2/tree/master/dreamerv2/agent.py,ActorCritic,train$229,"mets1 = {f'reward_{k}': v for (k, v) in mets1.items()}","mets1 = {f'reward_{k}': v for (k, v) in mets1.items()}","mets1 = {f'reward_{k}': v for (k, v) in mets1.items()}","tmp_DictComp0 = dict()
for (k, v) in mets1.items():
    tmp_DictComp0[f'reward_{k}'] = v
mets1 = tmp_DictComp0",1,0,1,0
karateclub,https://github.com/benedekrozemberczki/karateclub/tree/master/karateclub/community_detection/overlapping/nnsed.py,NNSED,get_memberships$113,memberships = {int(i): int(index[i]) for i in range(len(index))},memberships = {int(i): int(index[i]) for i in range(len(index))},memberships = {int(i): int(index[i]) for i in range(len(index))},"memberships = dict()
for i in range(len(index)):
    memberships[int(i)] = int(index[i])
",0,0,0,0
keystone,https://github.com/openstack/keystone/tree/master/keystone/api/role_inferences.py,RoleInferencesResource,get$28,role_dict = {role_ref['id']: role_ref for role_ref in PROVIDERS.role_api.list_roles()},role_dict = {role_ref['id']: role_ref for role_ref in PROVIDERS.role_api.list_roles()},role_dict = {role_ref['id']: role_ref for role_ref in PROVIDERS.role_api.list_roles()},"role_dict = dict()
for role_ref in PROVIDERS.role_api.list_roles():
    role_dict[role_ref['id']] = role_ref
",0,0,0,0
torba,https://github.com/lbryio/torba/tree/master/torba/server/mempool.py,MemPool,_accept_transactions$158,"return (deferred, {prevout: utxo_map[prevout] for prevout in unspent})","(deferred, {prevout: utxo_map[prevout] for prevout in unspent})","(deferred, {prevout: utxo_map[prevout] for prevout in unspent})","tmp_DictComp0 = dict()
for prevout in unspent:
    tmp_DictComp0[prevout] = utxo_map[prevout]
return (deferred, tmp_DictComp0)",1,0,1,0
mlrun,https://github.com/mlrun/mlrun/tree/master/mlrun/api/api/endpoints/feature_store.py,,list_feature_vectors_tags$605,feature_vector_name_to_tag = {tag_tuple[1]: tag_tuple[2] for tag_tuple in tag_tuples},feature_vector_name_to_tag = {tag_tuple[1]: tag_tuple[2] for tag_tuple in tag_tuples},feature_vector_name_to_tag = {tag_tuple[1]: tag_tuple[2] for tag_tuple in tag_tuples},"feature_vector_name_to_tag = dict()
for tag_tuple in tag_tuples:
    feature_vector_name_to_tag[tag_tuple[1]] = tag_tuple[2]
",0,0,0,0
video_analyst,https://github.com/MegviiDetection/video_analyst/tree/master/videoanalyst/evaluation/got_benchmark/experiments/lasot.py,ExperimentLaSOT,plot_curves$183,"performance = {k: v for (k, v) in performance.items() if k in tracker_names}","performance = {k: v for (k, v) in performance.items() if k in tracker_names}","performance = {k: v for (k, v) in performance.items() if k in tracker_names}","tmp_DictComp0 = dict()
for (k, v) in performance.items():
    if k in tracker_names:
        tmp_DictComp0[k] = v
performance = tmp_DictComp0",1,0,1,0
core,https://github.com/home-assistant/core/tree/master/homeassistant/components/zha/api.py,,async_load_api$943,cluster_channels = {ch.name: ch for pool in zha_device.channels.pools for ch in pool.claimed_channels.values()},cluster_channels = {ch.name: ch for pool in zha_device.channels.pools for ch in pool.claimed_channels.values()},cluster_channels = {ch.name: ch for pool in zha_device.channels.pools for ch in pool.claimed_channels.values()},"cluster_channels = dict()
for pool in zha_device.channels.pools:
    for ch in pool.claimed_channels.values():
        cluster_channels[ch.name] = ch
",0,0,0,0
jurigged,https://github.com/breuleux/jurigged/tree/master/jurigged/loop/richloop.py,RichDeveloopRunner,register_updates$364,"self._gvn.update({k: v for (k, v) in d.items() if not k.startswith('#') and (not k.startswith('$'))})","self._gvn.update({k: v for (k, v) in d.items() if not k.startswith('#') and (not k.startswith('$'))})","self._gvn.update({k: v for (k, v) in d.items() if not k.startswith('#') and (not k.startswith('$'))})","tmp_DictComp0 = dict()
for (k, v) in d.items():
    if not k.startswith('#') and (not k.startswith('$')):
        tmp_DictComp0[k] = v
self._gvn.update(tmp_DictComp0)",1,0,1,0
jiant,https://github.com/nyu-mll/jiant/tree/master/jiant/utils/torch_utils.py,,get_only_requires_grad$86,"return {n: p for (n, p) in parameters if p.requires_grad == requires_grad}","return {n: p for (n, p) in parameters if p.requires_grad == requires_grad}","return {n: p for (n, p) in parameters if p.requires_grad == requires_grad}","tmp_DictComp0 = dict()
for (n, p) in parameters:
    if p.requires_grad == requires_grad:
        tmp_DictComp0[n] = p
return tmp_DictComp0",1,0,1,0
data-validation,https://github.com/tensorflow/data-validation/tree/master/tensorflow_data_validation/statistics/stats_options.py,StatsOptions,from_json$289,"options_dict['_per_feature_weight_override'] = {types.FeaturePath.from_json(k): v for (k, v) in per_feature_weight_override_json.items()}","options_dict['_per_feature_weight_override'] = {types.FeaturePath.from_json(k): v for (k, v) in per_feature_weight_override_json.items()}","options_dict['_per_feature_weight_override'] = {types.FeaturePath.from_json(k): v for (k, v) in per_feature_weight_override_json.items()}","options_dict['_per_feature_weight_override'] = dict()
for (k, v) in per_feature_weight_override_json.items():
    options_dict['_per_feature_weight_override'][types.FeaturePath.from_json(k)] = v
",0,0,0,0
stellargraph,https://github.com/stellargraph/stellargraph/tree/master/stellargraph/data/epgm.py,EPGM,to_nx_OLD$319,self.G_nx[graph_id] = {v: [e[1] for e in edges if e[0] == v] for v in nodes},self.G_nx[graph_id] = {v: [e[1] for e in edges if e[0] == v] for v in nodes},self.G_nx[graph_id] = {v: [e[1] for e in edges if e[0] == v] for v in nodes},"self.G_nx[graph_id] = dict()
for v in nodes:
    self.G_nx[graph_id][v] = [e[1] for e in edges if e[0] == v]
",0,0,0,0
xarray,https://github.com/pydata/xarray/tree/master/xarray/tests/test_dataarray.py,TestReduce3D,test_argmin_dim$5226,"minindices_xy = {key: xr.where(nanindices_xy[key] == None, minindices_xy[key], nanindices_xy[key]) for key in minindices_xy}","minindices_xy = {key: xr.where(nanindices_xy[key] == None, minindices_xy[key], nanindices_xy[key]) for key in minindices_xy}","minindices_xy = {key: xr.where(nanindices_xy[key] == None, minindices_xy[key], nanindices_xy[key]) for key in minindices_xy}","tmp_DictComp0 = dict()
for key in minindices_xy:
    tmp_DictComp0[key] = xr.where(nanindices_xy[key] == None, minindices_xy[key], nanindices_xy[key])
minindices_xy = tmp_DictComp0",1,0,1,0
model_search,https://github.com/google/model_search/tree/master/model_search/data/csv_data_for_binary.py,Provider,input_fn$69,"features = {str(i): tensor for (i, tensor) in enumerate(args)}","features = {str(i): tensor for (i, tensor) in enumerate(args)}","features = {str(i): tensor for (i, tensor) in enumerate(args)}","features = dict()
for (i, tensor) in enumerate(args):
    features[str(i)] = tensor
",0,0,0,0
NVTabular,https://github.com/NVIDIA-Merlin/NVTabular/tree/master/nvtabular/ops/groupby.py,,_ensure_agg_dict$269,"return {k: v for (k, v) in _aggs.items() if k in _allowed_cols}","return {k: v for (k, v) in _aggs.items() if k in _allowed_cols}","return {k: v for (k, v) in _aggs.items() if k in _allowed_cols}","tmp_DictComp0 = dict()
for (k, v) in _aggs.items():
    if k in _allowed_cols:
        tmp_DictComp0[k] = v
return tmp_DictComp0",1,0,1,0
pyjanitor,https://github.com/pyjanitor-devs/pyjanitor/tree/master/janitor/functions/pivot.py,,_final_frame_longer$1098,"index = {name: arr[indexer] for (name, arr) in index.items()}","index = {name: arr[indexer] for (name, arr) in index.items()}","index = {name: arr[indexer] for (name, arr) in index.items()}","tmp_DictComp0 = dict()
for (name, arr) in index.items():
    tmp_DictComp0[name] = arr[indexer]
index = tmp_DictComp0",1,0,1,0
checkmk,https://github.com/tribe29/checkmk/tree/master/cmk/base/plugins/agent_based/docker_node_info.py,,inventory_docker_node_info$54,"yield TableRow(path=swarm_manager_path, key_columns={'NodeID': swarm_manager['NodeID']}, inventory_columns={k: v for (k, v) in swarm_manager.items() if k != 'NodeID'}, status_columns={})","inventory_columns={k: v for (k, v) in swarm_manager.items() if k != 'NodeID'}","inventory_columns={k: v for (k, v) in swarm_manager.items() if k != 'NodeID'}","tmp_DictComp0 = dict()
for (k, v) in swarm_manager.items():
    if k != 'NodeID':
        tmp_DictComp0[k] = v
yield TableRow(path=swarm_manager_path, key_columns={'NodeID': swarm_manager['NodeID']}, inventory_columns=tmp_DictComp0, status_columns={})",1,0,1,0
rasa,https://github.com/RasaHQ/rasa/tree/master/rasa/shared/core/events.py,UserUttered,_entity_string$632,"return json.dumps({entity[ENTITY_ATTRIBUTE_TYPE]: entity[ENTITY_ATTRIBUTE_VALUE] for entity in self.entities}, ensure_ascii=False)","json.dumps({entity[ENTITY_ATTRIBUTE_TYPE]: entity[ENTITY_ATTRIBUTE_VALUE] for entity in self.entities}, ensure_ascii=False)","json.dumps({entity[ENTITY_ATTRIBUTE_TYPE]: entity[ENTITY_ATTRIBUTE_VALUE] for entity in self.entities}, ensure_ascii=False)","tmp_DictComp0 = dict()
for entity in self.entities:
    tmp_DictComp0[entity[ENTITY_ATTRIBUTE_TYPE]] = entity[ENTITY_ATTRIBUTE_VALUE]
return json.dumps(tmp_DictComp0, ensure_ascii=False)",1,0,1,0
networkx,https://github.com/networkx/networkx/tree/master/networkx/generators/geometric.py,,random_geometric_graph$111,pos = {v: [seed.random() for i in range(dim)] for v in nodes},pos = {v: [seed.random() for i in range(dim)] for v in nodes},pos = {v: [seed.random() for i in range(dim)] for v in nodes},"pos = dict()
for v in nodes:
    pos[v] = [seed.random() for i in range(dim)]
",0,0,0,0
SMARTS,https://github.com/huawei-noah/SMARTS/tree/master/ultra/ultra/baselines/dqn/dqn/policy.py,DQNPolicy,learn$354,"out.update({'loss/td{}'.format(j): {'type': 'scalar', 'data': td_loss[j].data.cpu().numpy(), 'freq': 10} for j in range(len(td_loss))})","out.update({'loss/td{}'.format(j): {'type': 'scalar', 'data': td_loss[j].data.cpu().numpy(), 'freq': 10} for j in range(len(td_loss))})","out.update({'loss/td{}'.format(j): {'type': 'scalar', 'data': td_loss[j].data.cpu().numpy(), 'freq': 10} for j in range(len(td_loss))})","tmp_DictComp0 = dict()
for j in range(len(td_loss)):
    tmp_DictComp0['loss/td{}'.format(j)] = {'type': 'scalar', 'data': td_loss[j].data.cpu().numpy(), 'freq': 10}
out.update(tmp_DictComp0)",1,0,1,0
airflow,https://github.com/apache/airflow/tree/master/airflow/providers/odbc/hooks/odbc.py,OdbcHook,connection_extra_lower$92,"return {k.lower(): v for (k, v) in self.connection.extra_dejson.items()}","return {k.lower(): v for (k, v) in self.connection.extra_dejson.items()}","return {k.lower(): v for (k, v) in self.connection.extra_dejson.items()}","tmp_DictComp0 = dict()
for (k, v) in self.connection.extra_dejson.items():
    tmp_DictComp0[k.lower()] = v
return tmp_DictComp0",1,0,1,0
neutron,https://github.com/openstack/neutron/tree/master/neutron/tests/unit/agent/ovn/metadata/test_agent.py,TestMetadataAgent,test_get_networks$128,"expected_networks = {str(i): str(i) for i in range(0, 4)}","expected_networks = {str(i): str(i) for i in range(0, 4)}","expected_networks = {str(i): str(i) for i in range(0, 4)}","expected_networks = dict()
for i in range(0, 4):
    expected_networks[str(i)] = str(i)
",0,0,0,0
pororo,https://github.com/kakaobrain/pororo/tree/master/pororo/models/tts/utils/text.py,,to_sequence$57,"transform_dict = {s: i for (i, s) in enumerate(_other_symbols() + list(hp.phonemes if use_phonemes else hp.characters))}","transform_dict = {s: i for (i, s) in enumerate(_other_symbols() + list(hp.phonemes if use_phonemes else hp.characters))}","transform_dict = {s: i for (i, s) in enumerate(_other_symbols() + list(hp.phonemes if use_phonemes else hp.characters))}","transform_dict = dict()
for (i, s) in enumerate(_other_symbols() + list(hp.phonemes if use_phonemes else hp.characters)):
    transform_dict[s] = i
",0,0,0,0
deep-learning-for-image-processing,https://github.com/WZMIAOMIAO/deep-learning-for-image-processing/tree/master/pytorch_object_detection/train_coco_dataset/train_utils/distributed_utils.py,,reduce_dict$92,"reduced_dict = {k: v for (k, v) in zip(names, values)}","reduced_dict = {k: v for (k, v) in zip(names, values)}","reduced_dict = {k: v for (k, v) in zip(names, values)}","reduced_dict = dict()
for (k, v) in zip(names, values):
    reduced_dict[k] = v
",0,0,0,0
adapter-transformers,https://github.com/Adapter-Hub/adapter-transformers/tree/master/src/transformers/trainer_utils.py,RemoveColumnsCollator,_remove_columns$678,"return {k: v for (k, v) in feature.items() if k in self.signature_columns}","return {k: v for (k, v) in feature.items() if k in self.signature_columns}","return {k: v for (k, v) in feature.items() if k in self.signature_columns}","tmp_DictComp0 = dict()
for (k, v) in feature.items():
    if k in self.signature_columns:
        tmp_DictComp0[k] = v
return tmp_DictComp0",1,0,1,0
trackerjacker,https://github.com/calebmadrigal/trackerjacker/tree/master/trackerjacker/config_management.py,,build_config$212,"config_from_args = {k: v for (k, v) in config_from_args.items() if v is not None and k not in non_config_args}","config_from_args = {k: v for (k, v) in config_from_args.items() if v is not None and k not in non_config_args}","config_from_args = {k: v for (k, v) in config_from_args.items() if v is not None and k not in non_config_args}","tmp_DictComp0 = dict()
for (k, v) in config_from_args.items():
    if v is not None and k not in non_config_args:
        tmp_DictComp0[k] = v
config_from_args = tmp_DictComp0",1,0,1,0
Mailu,https://github.com/Mailu/Mailu/tree/master/core/admin/mailu/models.py,MailuCollection,_items$798,return {inspect(item).identity: item for item in self.model.query.all()},return {inspect(item).identity: item for item in self.model.query.all()},return {inspect(item).identity: item for item in self.model.query.all()},"tmp_DictComp0 = dict()
for item in self.model.query.all():
    tmp_DictComp0[inspect(item).identity] = item
return tmp_DictComp0",1,0,1,0
numpyro,https://github.com/pyro-ppl/numpyro/tree/master/numpyro/infer/hmc_util.py,,warmup_adapter$512,"size = {k: v.shape for (k, v) in inverse_mass_matrix.items()}","size = {k: v.shape for (k, v) in inverse_mass_matrix.items()}","size = {k: v.shape for (k, v) in inverse_mass_matrix.items()}","size = dict()
for (k, v) in inverse_mass_matrix.items():
    size[k] = v.shape
",0,0,0,0
awspx,https://github.com/FSecureLABS/awspx/tree/master/lib/aws/ingestor.py,Ingestor,load_associatives$805,"ambiguous = {k: v for (k, v) in refs[0].items() if len(v) > 1}","ambiguous = {k: v for (k, v) in refs[0].items() if len(v) > 1}","ambiguous = {k: v for (k, v) in refs[0].items() if len(v) > 1}","ambiguous = dict()
for (k, v) in refs[0].items():
    if len(v) > 1:
        ambiguous[k] = v
",0,0,0,0
torchdrug,https://github.com/DeepGraphLearning/torchdrug/tree/master/torchdrug/utils/torch.py,,cuda$82,"return type(obj)({k: cuda(v, *args, **kwargs) for (k, v) in obj.items()})","type(obj)({k: cuda(v, *args, **kwargs) for (k, v) in obj.items()})","type(obj)({k: cuda(v, *args, **kwargs) for (k, v) in obj.items()})","tmp_DictComp0 = dict()
for (k, v) in obj.items():
    tmp_DictComp0[k] = cuda(v, *args, **kwargs)
return type(obj)(tmp_DictComp0)",1,0,1,0
kepler-mapper,https://github.com/scikit-tda/kepler-mapper/tree/master/test/test_visuals.py,TestVisualHelpers,test_node_averages_multiple_color_value_vectors$130,nodes = {node['name']: node for node in graph_data['nodes']},nodes = {node['name']: node for node in graph_data['nodes']},nodes = {node['name']: node for node in graph_data['nodes']},"nodes = dict()
for node in graph_data['nodes']:
    nodes[node['name']] = node
",0,0,0,0
pyquil,https://github.com/rigetti/pyquil/tree/master/test/unit/test_compatibility_v2_quantum_computer.py,,test_orthogonal_array$561,occurences = {entry: 0 for entry in range(2 ** num_cols)},occurences = {entry: 0 for entry in range(2 ** num_cols)},occurences = {entry: 0 for entry in range(2 ** num_cols)},"occurences = dict()
for entry in range(2 ** num_cols):
    occurences[entry] = 0
",0,0,0,0
pydantic,https://github.com/samuelcolvin/pydantic/tree/master/pydantic/main.py,BaseModel,__getstate__$392,"return {'__dict__': self.__dict__, '__fields_set__': self.__fields_set__, '__private_attribute_values__': {k: v for (k, v) in private_attrs if v is not Undefined}}","{'__dict__': self.__dict__, '__fields_set__': self.__fields_set__, '__private_attribute_values__': {k: v for (k, v) in private_attrs if v is not Undefined}}","{'__dict__': self.__dict__, '__fields_set__': self.__fields_set__, '__private_attribute_values__': {k: v for (k, v) in private_attrs if v is not Undefined}}","tmp_DictComp0 = dict()
for (k, v) in private_attrs:
    if v is not Undefined:
        tmp_DictComp0[k] = v
return {'__dict__': self.__dict__, '__fields_set__': self.__fields_set__, '__private_attribute_values__': tmp_DictComp0}",1,0,1,0
upvote_py2,https://github.com/google/upvote_py2/tree/master/upvote/gae/bigquery/tables.py,BigQueryTable,_DoInsertRow$307,"memcache_key = self.CreateUniqueId(**{k: v for (k, v) in kwargs.iteritems() if k != 'timestamp'})","**{k: v for (k, v) in kwargs.iteritems() if k != 'timestamp'}","**{k: v for (k, v) in kwargs.iteritems() if k != 'timestamp'}","tmp_DictComp0 = dict()
for (k, v) in kwargs.iteritems():
    if k != 'timestamp':
        tmp_DictComp0[k] = v
memcache_key = self.CreateUniqueId(**tmp_DictComp0)",1,0,1,0
cloud-custodian,https://github.com/cloud-custodian/cloud-custodian/tree/master/c7n/resources/redshift.py,RedshiftSetPublicAccess,process$558,"futures = {w.submit(self.set_access, c): c for c in clusters}","futures = {w.submit(self.set_access, c): c for c in clusters}","futures = {w.submit(self.set_access, c): c for c in clusters}","futures = dict()
for c in clusters:
    futures[w.submit(self.set_access, c)] = c
",0,0,0,0
SOLO,https://github.com/WXinlong/SOLO/tree/master/mmdet/datasets/pipelines/transforms.py,Albu,__init__$725,"self.keymap_back = {v: k for (k, v) in self.keymap_to_albu.items()}","self.keymap_back = {v: k for (k, v) in self.keymap_to_albu.items()}","self.keymap_back = {v: k for (k, v) in self.keymap_to_albu.items()}","tmp_DictComp0 = dict()
for (k, v) in self.keymap_to_albu.items():
    tmp_DictComp0[v] = k
self.keymap_back = tmp_DictComp0",1,0,1,0
adapter-transformers,https://github.com/Adapter-Hub/adapter-transformers/tree/master/src/transformers/models/t5/tokenization_t5.py,T5Tokenizer,get_vocab$183,vocab = {self.convert_ids_to_tokens(i): i for i in range(self.vocab_size)},vocab = {self.convert_ids_to_tokens(i): i for i in range(self.vocab_size)},vocab = {self.convert_ids_to_tokens(i): i for i in range(self.vocab_size)},"vocab = dict()
for i in range(self.vocab_size):
    vocab[self.convert_ids_to_tokens(i)] = i
",0,0,0,0
mypy,https://github.com/python/mypy/tree/master/mypy/checker.py,,group_comparison_operands$6101,"groups: Dict[str, DisjointDict[Key, int]] = {op: DisjointDict() for op in operators_to_group}","groups: Dict[str, DisjointDict[Key, int]] = {op: DisjointDict() for op in operators_to_group}","groups: Dict[str, DisjointDict[Key, int]] = {op: DisjointDict() for op in operators_to_group}","groups = dict()
for op in operators_to_group:
    groups[op] = DisjointDict()
",0,0,0,0
thriftpy2,https://github.com/Thriftpy/thriftpy2/tree/master/thriftpy2/protocol/apache_json.py,TApacheJSONProtocol,_dict_to_thrift$228,"return {self._dict_to_thrift(k, item_type[0]): self._dict_to_thrift(v, item_type[1]) for (k, v) in data[3].items()}","return {self._dict_to_thrift(k, item_type[0]): self._dict_to_thrift(v, item_type[1]) for (k, v) in data[3].items()}","return {self._dict_to_thrift(k, item_type[0]): self._dict_to_thrift(v, item_type[1]) for (k, v) in data[3].items()}","tmp_DictComp0 = dict()
for (k, v) in data[3].items():
    tmp_DictComp0[self._dict_to_thrift(k, item_type[0])] = self._dict_to_thrift(v, item_type[1])
return tmp_DictComp0",1,0,1,0
rlpyt,https://github.com/astooke/rlpyt/tree/master/rlpyt/utils/collections.py,AttrDict,copy$219,"return type(self)(**{k: v.copy() if isinstance(v, AttrDict) else v for (k, v) in self.items()})","**{k: v.copy() if isinstance(v, AttrDict) else v for (k, v) in self.items()}","**{k: v.copy() if isinstance(v, AttrDict) else v for (k, v) in self.items()}","tmp_DictComp0 = dict()
for (k, v) in self.items():
    tmp_DictComp0[k] = v.copy() if isinstance(v, AttrDict) else v
return type(self)(**tmp_DictComp0)",1,0,1,0
flax,https://github.com/google/flax/tree/master/examples/linen_design_test/attention_simple.py,,concise_vmap$138,"variable_axes = {k: v[0] for (k, v) in var_specs.items() if isinstance(v, Sequence)}","variable_axes = {k: v[0] for (k, v) in var_specs.items() if isinstance(v, Sequence)}","variable_axes = {k: v[0] for (k, v) in var_specs.items() if isinstance(v, Sequence)}","variable_axes = dict()
for (k, v) in var_specs.items():
    if isinstance(v, Sequence):
        variable_axes[k] = v[0]
",0,0,0,0
pandas,https://github.com/pandas-dev/pandas/tree/master/asv_bench/benchmarks/multiindex_object.py,Difference,setup$287,"data = {k: {'left': mi, 'right': mi[:5]} for (k, mi) in data.items()}","data = {k: {'left': mi, 'right': mi[:5]} for (k, mi) in data.items()}","data = {k: {'left': mi, 'right': mi[:5]} for (k, mi) in data.items()}","tmp_DictComp0 = dict()
for (k, mi) in data.items():
    tmp_DictComp0[k] = {'left': mi, 'right': mi[:5]}
data = tmp_DictComp0",1,0,1,0
Open3D-ML,https://github.com/isl-org/Open3D-ML/tree/master/examples/tensorboard_tf.py,,object_detection$126,"name_to_labels = {name: label for (label, name) in dset.get_label_to_names().items()}","name_to_labels = {name: label for (label, name) in dset.get_label_to_names().items()}","name_to_labels = {name: label for (label, name) in dset.get_label_to_names().items()}","name_to_labels = dict()
for (label, name) in dset.get_label_to_names().items():
    name_to_labels[name] = label
",0,0,0,0
open_model_zoo,https://github.com/openvinotoolkit/open_model_zoo/tree/master/tools/accuracy_checker/openvino/tools/accuracy_checker/evaluators/custom_evaluators/lpcnet_evaluator.py,DecoderONNXModel,inputs$277,return {input_layer.name: input_layer.shape for input_layer in inputs_info},return {input_layer.name: input_layer.shape for input_layer in inputs_info},return {input_layer.name: input_layer.shape for input_layer in inputs_info},"tmp_DictComp0 = dict()
for input_layer in inputs_info:
    tmp_DictComp0[input_layer.name] = input_layer.shape
return tmp_DictComp0",1,0,1,0
motor,https://github.com/mongodb/motor/tree/master/motor/metaprogramming.py,,asynchronize$23,"unwrapped_kwargs = {key: obj.delegate if obj.__class__.__name__.endswith((unwrap_class, 'MotorClientSession')) else obj for (key, obj) in kwargs.items()}","unwrapped_kwargs = {key: obj.delegate if obj.__class__.__name__.endswith((unwrap_class, 'MotorClientSession')) else obj for (key, obj) in kwargs.items()}","unwrapped_kwargs = {key: obj.delegate if obj.__class__.__name__.endswith((unwrap_class, 'MotorClientSession')) else obj for (key, obj) in kwargs.items()}","unwrapped_kwargs = dict()
for (key, obj) in kwargs.items():
    unwrapped_kwargs[key] = obj.delegate if obj.__class__.__name__.endswith((unwrap_class, 'MotorClientSession')) else obj
",0,0,0,0
python-miio,https://github.com/rytilahti/python-miio/tree/master/miio/integrations/vacuum/viomi/viomivacuum.py,ViomiVacuum,start_with_room$614,"reverse_rooms = {v: k for (k, v) in self._cache['rooms'].items()}","reverse_rooms = {v: k for (k, v) in self._cache['rooms'].items()}","reverse_rooms = {v: k for (k, v) in self._cache['rooms'].items()}","reverse_rooms = dict()
for (k, v) in self._cache['rooms'].items():
    reverse_rooms[v] = k
",0,0,0,0
pyperf,https://github.com/psf/pyperf/tree/master/pyperf/_metadata.py,,_exclude_common_metadata$159,"metadata = {key: value for (key, value) in metadata.items() if key not in common_metadata}","metadata = {key: value for (key, value) in metadata.items() if key not in common_metadata}","metadata = {key: value for (key, value) in metadata.items() if key not in common_metadata}","tmp_DictComp0 = dict()
for (key, value) in metadata.items():
    if key not in common_metadata:
        tmp_DictComp0[key] = value
metadata = tmp_DictComp0",1,0,1,0
zipline,https://github.com/quantopian/zipline/tree/master/zipline/data/dispatch_bar_reader.py,AssetDispatchBarReader,load_raw_arrays$103,out_pos = {t: [] for t in asset_types},out_pos = {t: [] for t in asset_types},out_pos = {t: [] for t in asset_types},"out_pos = dict()
for t in asset_types:
    out_pos[t] = []
",0,0,0,0
garage,https://github.com/rlworkgroup/garage/tree/master/src/garage/torch/policies/deterministic_mlp_policy.py,DeterministicMLPPolicy,get_action$51,"return (action[0], {k: v[0] for (k, v) in agent_infos.items()})","(action[0], {k: v[0] for (k, v) in agent_infos.items()})","(action[0], {k: v[0] for (k, v) in agent_infos.items()})","tmp_DictComp0 = dict()
for (k, v) in agent_infos.items():
    tmp_DictComp0[k] = v[0]
return (action[0], tmp_DictComp0)",1,0,1,0
ranking,https://github.com/tensorflow/ranking/tree/master/tensorflow_ranking/examples/tf_ranking_libsvm.py,,get_eval_metric_fns$353,"metric_fns.update({'metric/%s' % name: tfr.metrics.make_ranking_metric_fn(name) for name in [tfr.metrics.RankingMetricKey.ARP, tfr.metrics.RankingMetricKey.ORDERED_PAIR_ACCURACY]})","metric_fns.update({'metric/%s' % name: tfr.metrics.make_ranking_metric_fn(name) for name in [tfr.metrics.RankingMetricKey.ARP, tfr.metrics.RankingMetricKey.ORDERED_PAIR_ACCURACY]})","metric_fns.update({'metric/%s' % name: tfr.metrics.make_ranking_metric_fn(name) for name in [tfr.metrics.RankingMetricKey.ARP, tfr.metrics.RankingMetricKey.ORDERED_PAIR_ACCURACY]})","tmp_DictComp0 = dict()
for name in [tfr.metrics.RankingMetricKey.ARP, tfr.metrics.RankingMetricKey.ORDERED_PAIR_ACCURACY]:
    tmp_DictComp0['metric/%s' % name] = tfr.metrics.make_ranking_metric_fn(name)
metric_fns.update(tmp_DictComp0)",1,0,1,0
fastformers,https://github.com/microsoft/fastformers/tree/master/src/transformers/modeling_encoder_decoder.py,EncoderDecoderModel,from_encoder_decoder_pretrained$91,"kwargs_decoder = {argument[len('decoder_'):]: value for (argument, value) in kwargs.items() if argument.startswith('decoder_')}","kwargs_decoder = {argument[len('decoder_'):]: value for (argument, value) in kwargs.items() if argument.startswith('decoder_')}","kwargs_decoder = {argument[len('decoder_'):]: value for (argument, value) in kwargs.items() if argument.startswith('decoder_')}","kwargs_decoder = dict()
for (argument, value) in kwargs.items():
    if argument.startswith('decoder_'):
        kwargs_decoder[argument[len('decoder_'):]] = value
",0,0,0,0
Skater,https://github.com/oracle/Skater/tree/master/skater/core/global_interpretation/partial_dependence.py,,_compute_pd$26,"pd_dict = {column: new_row[idx] for (idx, column) in enumerate(feature_columns)}","pd_dict = {column: new_row[idx] for (idx, column) in enumerate(feature_columns)}","pd_dict = {column: new_row[idx] for (idx, column) in enumerate(feature_columns)}","pd_dict = dict()
for (idx, column) in enumerate(feature_columns):
    pd_dict[column] = new_row[idx]
",0,0,0,0
review_object_detection_metrics,https://github.com/rafaelpadilla/review_object_detection_metrics/tree/master/src/evaluators/coco_evaluator.py,,get_coco_summary$28,"max_det1 = {i: _evaluate(iou_threshold=i, max_dets=1, area_range=(0, np.inf)) for i in iou_thresholds}","max_det1 = {i: _evaluate(iou_threshold=i, max_dets=1, area_range=(0, np.inf)) for i in iou_thresholds}","max_det1 = {i: _evaluate(iou_threshold=i, max_dets=1, area_range=(0, np.inf)) for i in iou_thresholds}","max_det1 = dict()
for i in iou_thresholds:
    max_det1[i] = _evaluate(iou_threshold=i, max_dets=1, area_range=(0, np.inf))
",0,0,0,0
scanpy,https://github.com/theislab/scanpy/tree/master/scanpy/tests/test_paga.py,,test_paga_pie$53,colors['Dendritic'] = {cm.Set2(_): 0.25 for _ in range(4)},colors['Dendritic'] = {cm.Set2(_): 0.25 for _ in range(4)},colors['Dendritic'] = {cm.Set2(_): 0.25 for _ in range(4)},"colors['Dendritic'] = dict()
for _ in range(4):
    colors['Dendritic'][cm.Set2(_)] = 0.25
",0,0,0,0
MillionHeroAssistant,https://github.com/smileboywtu/MillionHeroAssistant/tree/master/core/crawler/pmi.py,,sougou_count$57,return {ans: resp.text.count(ans) for ans in answers},return {ans: resp.text.count(ans) for ans in answers},return {ans: resp.text.count(ans) for ans in answers},"tmp_DictComp0 = dict()
for ans in answers:
    tmp_DictComp0[ans] = resp.text.count(ans)
return tmp_DictComp0",1,0,1,0
transformers,https://github.com/huggingface/transformers/tree/master/examples/tensorflow/text-classification/run_text_classification.py,,main$187,"label_to_id = {v: i for (i, v) in enumerate(label_list)}","label_to_id = {v: i for (i, v) in enumerate(label_list)}","label_to_id = {v: i for (i, v) in enumerate(label_list)}","label_to_id = dict()
for (i, v) in enumerate(label_list):
    label_to_id[v] = i
",0,0,0,0
LightAutoML,https://github.com/sberbank-ai-lab/LightAutoML/tree/master/lightautoml/tasks/losses/lgb.py,LGBLoss,__init__$139,loss_params = {param_mapping[x]: loss_params[x] for x in loss_params},loss_params = {param_mapping[x]: loss_params[x] for x in loss_params},loss_params = {param_mapping[x]: loss_params[x] for x in loss_params},"tmp_DictComp0 = dict()
for x in loss_params:
    tmp_DictComp0[param_mapping[x]] = loss_params[x]
loss_params = tmp_DictComp0",1,0,1,0
FastSpeech,https://github.com/xcmyz/FastSpeech/tree/master/text/cmudict.py,CMUDict,__init__$22,"entries = {word: pron for (word, pron) in entries.items() if len(pron) == 1}","entries = {word: pron for (word, pron) in entries.items() if len(pron) == 1}","entries = {word: pron for (word, pron) in entries.items() if len(pron) == 1}","tmp_DictComp0 = dict()
for (word, pron) in entries.items():
    if len(pron) == 1:
        tmp_DictComp0[word] = pron
entries = tmp_DictComp0",1,0,1,0
imbalanced-learn,https://github.com/scikit-learn-contrib/imbalanced-learn/tree/master/imblearn/utils/_validation.py,,_sampling_strategy_all$159,"sampling_strategy = {key: n_sample_majority - value for (key, value) in target_stats.items()}","sampling_strategy = {key: n_sample_majority - value for (key, value) in target_stats.items()}","sampling_strategy = {key: n_sample_majority - value for (key, value) in target_stats.items()}","sampling_strategy = dict()
for (key, value) in target_stats.items():
    sampling_strategy[key] = n_sample_majority - value
",0,0,0,0
GitHubPoster,https://github.com/yihong0618/GitHubPoster/tree/master/github_poster/cli.py,,run$20,"number_by_date_dict = {k: v for (k, v) in tracks.items() if k[:4] == str(year)}","number_by_date_dict = {k: v for (k, v) in tracks.items() if k[:4] == str(year)}","number_by_date_dict = {k: v for (k, v) in tracks.items() if k[:4] == str(year)}","number_by_date_dict = dict()
for (k, v) in tracks.items():
    if k[:4] == str(year):
        number_by_date_dict[k] = v
",0,0,0,0
gnes,https://github.com/gnes-ai/gnes/tree/master/gnes/client/base.py,ResponseHandler,__init__$31,"self.routes = {k: v for (k, v) in h.routes.items()} if h else {}","{k: v for (k, v) in h.routes.items()} if h else {}","{k: v for (k, v) in h.routes.items()} if h else {}","def my_comprehension_func(h):
    tmp_DictComp0 = dict()
    for (k, v) in h.routes.items():
        tmp_DictComp0[k] = v
    return tmp_DictComp0
self.routes = my_comprehension_func(h) if h else {}",1,1,1,1
kale,https://github.com/kubeflow-kale/kale/tree/master/backend/kale/common/kfputils.py,,get_kfp_run_metrics$346,return {metric.name: metric.number_value for metric in run_metrics},return {metric.name: metric.number_value for metric in run_metrics},return {metric.name: metric.number_value for metric in run_metrics},"tmp_DictComp0 = dict()
for metric in run_metrics:
    tmp_DictComp0[metric.name] = metric.number_value
return tmp_DictComp0",1,0,1,0
GANTheftAuto,https://github.com/Sentdex/GANTheftAuto/tree/master/simulator_model/rendering_engine.py,,G_arch$52,"arch['84x84'] = {'in_channels': [ch * item for item in [16, 8, 4]], 'out_channels': [ch * item for item in [8, 4, 2]], 'upsample': [True] * 3, 'resolution': [16, 32, 64], 'attention': {2 ** i: 2 ** i in [int(item) for item in attention.split('_')] for i in range(4, 7)}}","{'in_channels': [ch * item for item in [16, 8, 4]], 'out_channels': [ch * item for item in [8, 4, 2]], 'upsample': [True] * 3, 'resolution': [16, 32, 64], 'attention': {2 ** i: 2 ** i in [int(item) for item in attention.split('_')] for i in range(4, 7)}}","{'in_channels': [ch * item for item in [16, 8, 4]], 'out_channels': [ch * item for item in [8, 4, 2]], 'upsample': [True] * 3, 'resolution': [16, 32, 64], 'attention': {2 ** i: 2 ** i in [int(item) for item in attention.split('_')] for i in range(4, 7)}}","tmp_DictComp0 = dict()
for i in range(4, 7):
    tmp_DictComp0[2 ** i] = 2 ** i in [int(item) for item in attention.split('_')]
arch['84x84'] = {'in_channels': [ch * item for item in [16, 8, 4]], 'out_channels': [ch * item for item in [8, 4, 2]], 'upsample': [True] * 3, 'resolution': [16, 32, 64], 'attention': tmp_DictComp0}",1,0,1,0
ActionAI,https://github.com/smellslikeml/ActionAI/tree/master/examples/yogai/demo.py,motionClassifier,__init__$16,"self.move_dict = {idx: val for (idx, val) in enumerate(self.move_lst)}","self.move_dict = {idx: val for (idx, val) in enumerate(self.move_lst)}","self.move_dict = {idx: val for (idx, val) in enumerate(self.move_lst)}","tmp_DictComp0 = dict()
for (idx, val) in enumerate(self.move_lst):
    tmp_DictComp0[idx] = val
self.move_dict = tmp_DictComp0",1,0,1,0
mlrun,https://github.com/mlrun/mlrun/tree/master/mlrun/api/db/sqldb/db.py,SQLDB,list_artifacts$603,indexed_artifacts = {f'{artifact.key}-{artifact.uid}': artifact for artifact in artifact_records},indexed_artifacts = {f'{artifact.key}-{artifact.uid}': artifact for artifact in artifact_records},indexed_artifacts = {f'{artifact.key}-{artifact.uid}': artifact for artifact in artifact_records},"indexed_artifacts = dict()
for artifact in artifact_records:
    indexed_artifacts[f'{artifact.key}-{artifact.uid}'] = artifact
",0,0,0,0
nextcord,https://github.com/nextcord/nextcord/tree/master/nextcord/http.py,Route,__init__$134,"url = url.format_map({k: _uriquote(v) if isinstance(v, str) else v for (k, v) in parameters.items()})","url.format_map({k: _uriquote(v) if isinstance(v, str) else v for (k, v) in parameters.items()})","url.format_map({k: _uriquote(v) if isinstance(v, str) else v for (k, v) in parameters.items()})","tmp_DictComp0 = dict()
for (k, v) in parameters.items():
    tmp_DictComp0[k] = _uriquote(v) if isinstance(v, str) else v
url = url.format_map(tmp_DictComp0)",1,0,1,0
PVT,https://github.com/whai362/PVT/tree/master/classification/engine.py,,train_one_epoch$19,"return {k: meter.global_avg for (k, meter) in metric_logger.meters.items()}","return {k: meter.global_avg for (k, meter) in metric_logger.meters.items()}","return {k: meter.global_avg for (k, meter) in metric_logger.meters.items()}","tmp_DictComp0 = dict()
for (k, meter) in metric_logger.meters.items():
    tmp_DictComp0[k] = meter.global_avg
return tmp_DictComp0",1,0,1,0
raster-vision,https://github.com/azavea/raster-vision/tree/master/rastervision_core/rastervision/core/evaluation/classification_evaluation.py,ClassificationEvaluation,to_json$45,"per_scene_evals = {scene_id: eval.to_json() for (scene_id, eval) in self.scene_to_eval.items()}","per_scene_evals = {scene_id: eval.to_json() for (scene_id, eval) in self.scene_to_eval.items()}","per_scene_evals = {scene_id: eval.to_json() for (scene_id, eval) in self.scene_to_eval.items()}","per_scene_evals = dict()
for (scene_id, eval) in self.scene_to_eval.items():
    per_scene_evals[scene_id] = eval.to_json()
",0,0,0,0
motor,https://github.com/mongodb/motor/tree/master/motor/metaprogramming.py,,method$42,"unwrapped_kwargs = {key: obj.delegate if obj.__class__.__name__.endswith('MotorClientSession') else obj for (key, obj) in kwargs.items()}","unwrapped_kwargs = {key: obj.delegate if obj.__class__.__name__.endswith('MotorClientSession') else obj for (key, obj) in kwargs.items()}","unwrapped_kwargs = {key: obj.delegate if obj.__class__.__name__.endswith('MotorClientSession') else obj for (key, obj) in kwargs.items()}","unwrapped_kwargs = dict()
for (key, obj) in kwargs.items():
    unwrapped_kwargs[key] = obj.delegate if obj.__class__.__name__.endswith('MotorClientSession') else obj
",0,0,0,0
icevision,https://github.com/airctic/icevision/tree/master/icevision/models/interpretation.py,,_get_info$269,"d = {k: v for (k, v) in sample.losses.items() if 'loss' in k}","d = {k: v for (k, v) in sample.losses.items() if 'loss' in k}","d = {k: v for (k, v) in sample.losses.items() if 'loss' in k}","d = dict()
for (k, v) in sample.losses.items():
    if 'loss' in k:
        d[k] = v
",0,0,0,0
differentiable_volumetric_rendering,https://github.com/autonomousvision/differentiable_volumetric_rendering/tree/master/im2mesh/training.py,BaseTrainer,evaluate$11,"eval_dict = {k: np.mean(v) for (k, v) in eval_list.items()}","eval_dict = {k: np.mean(v) for (k, v) in eval_list.items()}","eval_dict = {k: np.mean(v) for (k, v) in eval_list.items()}","eval_dict = dict()
for (k, v) in eval_list.items():
    eval_dict[k] = np.mean(v)
",0,0,0,0
horovod,https://github.com/horovod/horovod/tree/master/test/parallel/test_tensorflow2_keras.py,Tf2KerasTests,test_partial_distributed_optimizer$527,"var_grad_tape = {var.ref(): grad for (var, grad) in zip(model.trainable_weights, gradients_tape)}","var_grad_tape = {var.ref(): grad for (var, grad) in zip(model.trainable_weights, gradients_tape)}","var_grad_tape = {var.ref(): grad for (var, grad) in zip(model.trainable_weights, gradients_tape)}","var_grad_tape = dict()
for (var, grad) in zip(model.trainable_weights, gradients_tape):
    var_grad_tape[var.ref()] = grad
",0,0,0,0
salt,https://github.com/saltstack/salt/tree/master/tests/unit/modules/test_virt.py,VirtTestCase,test_gen_xml_cpu_numa$676,"self.assertEqual({d.get('id'): d.get('value') for d in cell1.findall('distances/sibling')}, {'0': '20', '1': '10'})","self.assertEqual({d.get('id'): d.get('value') for d in cell1.findall('distances/sibling')}, {'0': '20', '1': '10'})","self.assertEqual({d.get('id'): d.get('value') for d in cell1.findall('distances/sibling')}, {'0': '20', '1': '10'})","tmp_DictComp0 = dict()
for d in cell1.findall('distances/sibling'):
    tmp_DictComp0[d.get('id')] = d.get('value')
self.assertEqual(tmp_DictComp0, {'0': '20', '1': '10'})",1,0,1,0
accelerate,https://github.com/huggingface/accelerate/tree/master/src/accelerate/test_utils/training.py,,mocked_dataloaders$51,"label_to_id = {v: i for (i, v) in enumerate(label_list)}","label_to_id = {v: i for (i, v) in enumerate(label_list)}","label_to_id = {v: i for (i, v) in enumerate(label_list)}","label_to_id = dict()
for (i, v) in enumerate(label_list):
    label_to_id[v] = i
",0,0,0,0
edx-platform,https://github.com/edx/edx-platform/tree/master/lms/djangoapps/courseware/tests/test_module_render.py,TestFilteredChildren,setUp$2474,self.users = {number: UserFactory() for number in USER_NUMBERS},self.users = {number: UserFactory() for number in USER_NUMBERS},self.users = {number: UserFactory() for number in USER_NUMBERS},"self.users = dict()
for number in USER_NUMBERS:
    self.users[number] = UserFactory()
",0,0,0,0
causalml,https://github.com/uber/causalml/tree/master/causalml/inference/meta/drlearner.py,BaseDRLearner,fit$89,self.propensity = {group: np.zeros(y.shape[0]) for group in self.t_groups},self.propensity = {group: np.zeros(y.shape[0]) for group in self.t_groups},self.propensity = {group: np.zeros(y.shape[0]) for group in self.t_groups},"tmp_DictComp0 = dict()
for group in self.t_groups:
    tmp_DictComp0[group] = np.zeros(y.shape[0])
self.propensity = tmp_DictComp0",1,0,1,0
KILT,https://github.com/facebookresearch/KILT/tree/master/kilt/readers/fid/postprocess.py,,convert_to_kilt$12,datadict = {ex['id']: ex for ex in data},datadict = {ex['id']: ex for ex in data},datadict = {ex['id']: ex for ex in data},"datadict = dict()
for ex in data:
    datadict[ex['id']] = ex
",0,0,0,0
exbert,https://github.com/bhoov/exbert/tree/master/server/utils/token_processing.py,TokenAligner,meta_hdf5_to_obj$121,out = {k: [] for k in keys},out = {k: [] for k in keys},out = {k: [] for k in keys},"out = dict()
for k in keys:
    out[k] = []
",0,0,0,0
qtile,https://github.com/qtile/qtile/tree/master/libqtile/widget/prompt.py,Prompt,_configure$392,printables = {x: self._write_char for x in range(127) if chr(x) in string.printable},printables = {x: self._write_char for x in range(127) if chr(x) in string.printable},printables = {x: self._write_char for x in range(127) if chr(x) in string.printable},"printables = dict()
for x in range(127):
    if chr(x) in string.printable:
        printables[x] = self._write_char
",0,0,0,0
requests,https://github.com/psf/requests/tree/master/requests/models.py,Response,__getstate__$659,"return {attr: getattr(self, attr, None) for attr in self.__attrs__}","return {attr: getattr(self, attr, None) for attr in self.__attrs__}","return {attr: getattr(self, attr, None) for attr in self.__attrs__}","tmp_DictComp0 = dict()
for attr in self.__attrs__:
    tmp_DictComp0[attr] = getattr(self, attr, None)
return tmp_DictComp0",1,0,1,0
qiskit-terra,https://github.com/Qiskit/qiskit-terra/tree/master/qiskit/quantum_info/operators/symplectic/base_pauli.py,BasePauli,_append_circuit$498,"bit_indices = {bit: index for bits in [flat_instr.qubits, flat_instr.clbits] for (index, bit) in enumerate(bits)}","bit_indices = {bit: index for bits in [flat_instr.qubits, flat_instr.clbits] for (index, bit) in enumerate(bits)}","bit_indices = {bit: index for bits in [flat_instr.qubits, flat_instr.clbits] for (index, bit) in enumerate(bits)}","bit_indices = dict()
for bits in [flat_instr.qubits, flat_instr.clbits]:
    for (index, bit) in enumerate(bits):
        bit_indices[bit] = index
",0,0,0,0
pretix,https://github.com/pretix/pretix/tree/master/src/pretix/api/views/event.py,EventViewSet,perform_update$187,disabled = {m: 'disabled' for m in current_plugins_value if m not in updated_plugins_value},disabled = {m: 'disabled' for m in current_plugins_value if m not in updated_plugins_value},disabled = {m: 'disabled' for m in current_plugins_value if m not in updated_plugins_value},"disabled = dict()
for m in current_plugins_value:
    if m not in updated_plugins_value:
        disabled[m] = 'disabled'
",0,0,0,0
checkmk,https://github.com/tribe29/checkmk/tree/master/tests/unit/cmk/base/plugins/agent_based/test_juniper_trpz_aps_sessions.py,,test_discovery_juniper_trpz_aps_sessions$217,"services = {node_name: [service.item for service in discovery_juniper_trpz_aps_sessions(section)] for (node_name, section) in node_sections.items()}","services = {node_name: [service.item for service in discovery_juniper_trpz_aps_sessions(section)] for (node_name, section) in node_sections.items()}","services = {node_name: [service.item for service in discovery_juniper_trpz_aps_sessions(section)] for (node_name, section) in node_sections.items()}","services = dict()
for (node_name, section) in node_sections.items():
    services[node_name] = [service.item for service in discovery_juniper_trpz_aps_sessions(section)]
",0,0,0,0
YOLOF,https://github.com/megvii-model/YOLOF/tree/master/cvpods/analyser/tide/quantify.py,TIDE,get_main_errors$714,"errors[run_name] = {error.short_name: value for (error, value) in run.fix_main_errors().items()}","errors[run_name] = {error.short_name: value for (error, value) in run.fix_main_errors().items()}","errors[run_name] = {error.short_name: value for (error, value) in run.fix_main_errors().items()}","errors[run_name] = dict()
for (error, value) in run.fix_main_errors().items():
    errors[run_name][error.short_name] = value
",0,0,0,0
udiskie,https://github.com/coldfix/udiskie/tree/master/udiskie/notify.py,Notify,__init__$35,"self.events = {event: getattr(self, event) for event in self.EVENTS if self._enabled(event)}","self.events = {event: getattr(self, event) for event in self.EVENTS if self._enabled(event)}","self.events = {event: getattr(self, event) for event in self.EVENTS if self._enabled(event)}","tmp_DictComp0 = dict()
for event in self.EVENTS:
    if self._enabled(event):
        tmp_DictComp0[event] = getattr(self, event)
self.events = tmp_DictComp0",1,0,1,0
accelerate,https://github.com/huggingface/accelerate/tree/master/src/accelerate/accelerator.py,Accelerator,prepare$801,"mapping = {p: new_named_params[n] for (n, p) in old_named_params.items()}","mapping = {p: new_named_params[n] for (n, p) in old_named_params.items()}","mapping = {p: new_named_params[n] for (n, p) in old_named_params.items()}","mapping = dict()
for (n, p) in old_named_params.items():
    mapping[p] = new_named_params[n]
",0,0,0,0
nuscenes-devkit,https://github.com/nutonomy/nuscenes-devkit/tree/master/python-sdk/nuscenes/tests/test_predict_helper.py,MockNuScenes,__init__$16,self._sample = {r['token']: r for r in samples},self._sample = {r['token']: r for r in samples},self._sample = {r['token']: r for r in samples},"self._sample = dict()
for r in samples:
    self._sample[r['token']] = r
",0,0,0,0
airflow,https://github.com/apache/airflow/tree/master/airflow/task/task_runner/cgroup_task_runner.py,CgroupTaskRunner,_create_cgroup$75,name_to_node = {x.name.decode(): x for x in node.children},name_to_node = {x.name.decode(): x for x in node.children},name_to_node = {x.name.decode(): x for x in node.children},"name_to_node = dict()
for x in node.children:
    name_to_node[x.name.decode()] = x
",0,0,0,0
river,https://github.com/online-ml/river/tree/master/river/naive_bayes/complement.py,ComplementNB,learn_many$188,"self.feature_totals.update({c: count.item() for (c, count) in zip(columns, np.array(fc.sum(axis=0)).flatten())})","self.feature_totals.update({c: count.item() for (c, count) in zip(columns, np.array(fc.sum(axis=0)).flatten())})","self.feature_totals.update({c: count.item() for (c, count) in zip(columns, np.array(fc.sum(axis=0)).flatten())})","tmp_DictComp0 = dict()
for (c, count) in zip(columns, np.array(fc.sum(axis=0)).flatten()):
    tmp_DictComp0[c] = count.item()
self.feature_totals.update(tmp_DictComp0)",1,0,1,0
DeepPavlov,https://github.com/deepmipt/DeepPavlov/tree/master/deeppavlov/core/commands/train.py,,train_evaluate_model_from_config$69,"res = {k: v['metrics'] for (k, v) in res.items()}","res = {k: v['metrics'] for (k, v) in res.items()}","res = {k: v['metrics'] for (k, v) in res.items()}","tmp_DictComp0 = dict()
for (k, v) in res.items():
    tmp_DictComp0[k] = v['metrics']
res = tmp_DictComp0",1,0,1,0
PettingZoo,https://github.com/Farama-Foundation/PettingZoo/tree/master/pettingzoo/classic/connect_four/connect_four.py,raw_env,reset$206,self.terminations = {i: False for i in self.agents},self.terminations = {i: False for i in self.agents},self.terminations = {i: False for i in self.agents},"tmp_DictComp0 = dict()
for i in self.agents:
    tmp_DictComp0[i] = False
self.terminations = tmp_DictComp0",1,0,1,0
haystack,https://github.com/deepset-ai/haystack/tree/master/haystack/modeling/data_handler/data_silo.py,DistillationDataSilo,_pass_batches$767,"batch_dict = {key: tensor.to(self.device) for (key, tensor) in zip(tensor_names, batch_transposed_list)}","batch_dict = {key: tensor.to(self.device) for (key, tensor) in zip(tensor_names, batch_transposed_list)}","batch_dict = {key: tensor.to(self.device) for (key, tensor) in zip(tensor_names, batch_transposed_list)}","batch_dict = dict()
for (key, tensor) in zip(tensor_names, batch_transposed_list):
    batch_dict[key] = tensor.to(self.device)
",0,0,0,0
stn-ocr,https://github.com/Bartzi/stn-ocr/tree/master/mxnet/eval_text_recognition_model.py,,if_main_my$78,"reverse_char_map = {v: k for (k, v) in char_map.items()}","reverse_char_map = {v: k for (k, v) in char_map.items()}","reverse_char_map = {v: k for (k, v) in char_map.items()}","reverse_char_map = dict()
for (k, v) in char_map.items():
    reverse_char_map[v] = k
",0,0,0,0
splinter,https://github.com/cobrateam/splinter/tree/master/splinter/driver/djangoclient.py,CookieManager,__eq__$53,"cookies_dict = {key: morsel.value for (key, morsel) in self.driver.cookies.items()}","cookies_dict = {key: morsel.value for (key, morsel) in self.driver.cookies.items()}","cookies_dict = {key: morsel.value for (key, morsel) in self.driver.cookies.items()}","cookies_dict = dict()
for (key, morsel) in self.driver.cookies.items():
    cookies_dict[key] = morsel.value
",0,0,0,0
Paddle,https://github.com/PaddlePaddle/Paddle/tree/master/python/paddle/fluid/tests/unittests/test_argsort_op.py,TestArgsortOpCPU,forward$104,"self.feed_map = {x: create_tensor(getattr(self.py_argsort, x), self.place) for x in self.feed_data_field}","self.feed_map = {x: create_tensor(getattr(self.py_argsort, x), self.place) for x in self.feed_data_field}","self.feed_map = {x: create_tensor(getattr(self.py_argsort, x), self.place) for x in self.feed_data_field}","tmp_DictComp0 = dict()
for x in self.feed_data_field:
    tmp_DictComp0[x] = create_tensor(getattr(self.py_argsort, x), self.place)
self.feed_map = tmp_DictComp0",1,0,1,0
praw,https://github.com/praw-dev/praw/tree/master/praw/models/inbox.py,Inbox,message$185,messages = {message.fullname: message for message in [listing[0]] + listing[0].replies},messages = {message.fullname: message for message in [listing[0]] + listing[0].replies},messages = {message.fullname: message for message in [listing[0]] + listing[0].replies},"messages = dict()
for message in [listing[0]] + listing[0].replies:
    messages[message.fullname] = message
",0,0,0,0
allennlp,https://github.com/allenai/allennlp/tree/master/tests/fairness/bias_metrics_test.py,AssociationWithoutGroundTruthTest,test_pmi_unmasked_computation$193,"test_pairwise_pmi_gaps = {k1: {k2: [e if not math.isnan(e) else np.nan for e in v2.tolist()] for (k2, v2) in v1.items()} for (k1, v1) in pairwise_pmi.get_metric(reset=True).items()}","test_pairwise_pmi_gaps = {k1: {k2: [e if not math.isnan(e) else np.nan for e in v2.tolist()] for (k2, v2) in v1.items()} for (k1, v1) in pairwise_pmi.get_metric(reset=True).items()}","test_pairwise_pmi_gaps = {k1: {k2: [e if not math.isnan(e) else np.nan for e in v2.tolist()] for (k2, v2) in v1.items()} for (k1, v1) in pairwise_pmi.get_metric(reset=True).items()}","test_pairwise_pmi_gaps = dict()
for (k1, v1) in pairwise_pmi.get_metric(reset=True).items():
    test_pairwise_pmi_gaps[k1] = {k2: [e if not math.isnan(e) else np.nan for e in v2.tolist()] for (k2, v2) in v1.items()}
",0,0,0,0
deep-ctr-prediction,https://github.com/qiaoguan/deep-ctr-prediction/tree/master/DeepFM/input_fn.py,,parse_tfrecord$85,StringVarLenFeatures = {key: tf.VarLenFeature(dtype=tf.string) for key in StringVarLenFeatureColumns},StringVarLenFeatures = {key: tf.VarLenFeature(dtype=tf.string) for key in StringVarLenFeatureColumns},StringVarLenFeatures = {key: tf.VarLenFeature(dtype=tf.string) for key in StringVarLenFeatureColumns},"StringVarLenFeatures = dict()
for key in StringVarLenFeatureColumns:
    StringVarLenFeatures[key] = tf.VarLenFeature(dtype=tf.string)
",0,0,0,0
pytorch-image-models,https://github.com/rwightman/pytorch-image-models/tree/master/timm/models/features.py,FeatureInfo,get_dicts$49,return [{k: self.info[i][k] for k in keys} for i in self.out_indices],[{k: self.info[i][k] for k in keys} for i in self.out_indices],[{k: self.info[i][k] for k in keys} for i in self.out_indices],"def my_comprehension_func(i, self):
    tmp_DictComp0 = dict()
    for k in keys:
        tmp_DictComp0[k] = self.info[i][k]
    return tmp_DictComp0
return [{k: self.info[i][k] for k in keys} for i in self.out_indices]",1,1,1,1
cloud-custodian,https://github.com/cloud-custodian/cloud-custodian/tree/master/tests/test_s3.py,S3Test,test_s3_remove_tag$3204,"tag_map = {t['Key']: t['Value'] for t in tags.get('TagSet', {})}","tag_map = {t['Key']: t['Value'] for t in tags.get('TagSet', {})}","tag_map = {t['Key']: t['Value'] for t in tags.get('TagSet', {})}","tag_map = dict()
for t in tags.get('TagSet', {}):
    tag_map[t['Key']] = t['Value']
",0,0,0,0
freeipa,https://github.com/freeipa/freeipa/tree/master/ipatests/test_ipalib/test_text.py,test_TestLang,setup_lang$55,os.environ.update({env_var: self.lang for env_var in self.lang_env_vars}),os.environ.update({env_var: self.lang for env_var in self.lang_env_vars}),os.environ.update({env_var: self.lang for env_var in self.lang_env_vars}),"tmp_DictComp0 = dict()
for env_var in self.lang_env_vars:
    tmp_DictComp0[env_var] = self.lang
os.environ.update(tmp_DictComp0)",1,0,1,0
SMARTS,https://github.com/huawei-noah/SMARTS/tree/master/smarts/env/rllib_hiway_env.py,RLlibHiWayEnv,step$98,"scores = {agent_id: score for (agent_id, score) in extras['scores'].items() if agent_id in agent_actions}","scores = {agent_id: score for (agent_id, score) in extras['scores'].items() if agent_id in agent_actions}","scores = {agent_id: score for (agent_id, score) in extras['scores'].items() if agent_id in agent_actions}","scores = dict()
for (agent_id, score) in extras['scores'].items():
    if agent_id in agent_actions:
        scores[agent_id] = score
",0,0,0,0
alphafold,https://github.com/deepmind/alphafold/tree/master/alphafold/model/modules_multimer.py,AlphaFoldIteration,__call__$307,"representations = {k: jnp.zeros(v.shape, v.dtype) for (k, v) in repr_shape.items()}","representations = {k: jnp.zeros(v.shape, v.dtype) for (k, v) in repr_shape.items()}","representations = {k: jnp.zeros(v.shape, v.dtype) for (k, v) in repr_shape.items()}","representations = dict()
for (k, v) in repr_shape.items():
    representations[k] = jnp.zeros(v.shape, v.dtype)
",0,0,0,0
mycroft-core,https://github.com/MycroftAI/mycroft-core/tree/master/mycroft/audio/services/mopidy/mopidypost.py,Mopidy,get_gmusic_albums$197,return {e.split(' - ')[1]: p[e] for e in p},return {e.split(' - ')[1]: p[e] for e in p},return {e.split(' - ')[1]: p[e] for e in p},"tmp_DictComp0 = dict()
for e in p:
    tmp_DictComp0[e.split(' - ')[1]] = p[e]
return tmp_DictComp0",1,0,1,0
horizon,https://github.com/openstack/horizon/tree/master/openstack_dashboard/dashboards/identity/projects/tests.py,UpdateQuotasWorkflowTests,_test_update_quotas_save$1025,neutron_updated_quota = {key: updated_quota[key] for key in quotas.NEUTRON_QUOTA_FIELDS},neutron_updated_quota = {key: updated_quota[key] for key in quotas.NEUTRON_QUOTA_FIELDS},neutron_updated_quota = {key: updated_quota[key] for key in quotas.NEUTRON_QUOTA_FIELDS},"neutron_updated_quota = dict()
for key in quotas.NEUTRON_QUOTA_FIELDS:
    neutron_updated_quota[key] = updated_quota[key]
",0,0,0,0
dulwich,https://github.com/dulwich/dulwich/tree/master/dulwich/tests/test_diff_tree.py,RenameDetectionTest,assertBlockCountEqual$644,"self.assertEqual({hash(l) & 4294967295: c for (l, c) in expected.items()}, {h & 4294967295: c for (h, c) in got.items()})","self.assertEqual({hash(l) & 4294967295: c for (l, c) in expected.items()}, {h & 4294967295: c for (h, c) in got.items()})","self.assertEqual({hash(l) & 4294967295: c for (l, c) in expected.items()}, {h & 4294967295: c for (h, c) in got.items()})","tmp_DictComp0 = dict()
for (l, c) in expected.items():
    tmp_DictComp0[hash(l) & 4294967295] = c
self.assertEqual(tmp_DictComp0, {h & 4294967295: c for (h, c) in got.items()})",1,0,1,0
sweetviz,https://github.com/fbdesignpro/sweetviz/tree/master/sweetviz/graph_associations.py,,heatmap$230,y_to_num = {p[1]: p[0] for p in enumerate(y_names)},y_to_num = {p[1]: p[0] for p in enumerate(y_names)},y_to_num = {p[1]: p[0] for p in enumerate(y_names)},"y_to_num = dict()
for p in enumerate(y_names):
    y_to_num[p[1]] = p[0]
",0,0,0,0
arviz,https://github.com/arviz-devs/arviz/tree/master/arviz/stats/stats.py,,summary$1033,"decimals = {col: 3 if col not in {'ess_bulk', 'ess_tail', 'r_hat'} else 2 if col == 'r_hat' else 0 for col in summary_df.columns}","decimals = {col: 3 if col not in {'ess_bulk', 'ess_tail', 'r_hat'} else 2 if col == 'r_hat' else 0 for col in summary_df.columns}","decimals = {col: 3 if col not in {'ess_bulk', 'ess_tail', 'r_hat'} else 2 if col == 'r_hat' else 0 for col in summary_df.columns}","decimals = dict()
for col in summary_df.columns:
    decimals[col] = 3 if col not in {'ess_bulk', 'ess_tail', 'r_hat'} else 2 if col == 'r_hat' else 0
",0,0,0,0
LASER,https://github.com/facebookresearch/LASER/tree/master/tasks/CCMatrix/dl_cc_matrix.py,,get_documents$106,return {d['digest']: d['raw_content'] for d in CCSegmentsReader([segment])},return {d['digest']: d['raw_content'] for d in CCSegmentsReader([segment])},return {d['digest']: d['raw_content'] for d in CCSegmentsReader([segment])},"tmp_DictComp0 = dict()
for d in CCSegmentsReader([segment]):
    tmp_DictComp0[d['digest']] = d['raw_content']
return tmp_DictComp0",1,0,1,0
CenterNet2,https://github.com/xingyizhou/CenterNet2/tree/master/tools/plain_train_net.py,,do_train$119,"loss_dict_reduced = {k: v.item() for (k, v) in comm.reduce_dict(loss_dict).items()}","loss_dict_reduced = {k: v.item() for (k, v) in comm.reduce_dict(loss_dict).items()}","loss_dict_reduced = {k: v.item() for (k, v) in comm.reduce_dict(loss_dict).items()}","loss_dict_reduced = dict()
for (k, v) in comm.reduce_dict(loss_dict).items():
    loss_dict_reduced[k] = v.item()
",0,0,0,0
nltk,https://github.com/nltk/nltk/tree/master/nltk/inference/discourse.py,DiscourseTester,add_sentence$212,"self._sentences = {'s%s' % i: sent for (i, sent) in enumerate(self._input)}","self._sentences = {'s%s' % i: sent for (i, sent) in enumerate(self._input)}","self._sentences = {'s%s' % i: sent for (i, sent) in enumerate(self._input)}","tmp_DictComp0 = dict()
for (i, sent) in enumerate(self._input):
    tmp_DictComp0['s%s' % i] = sent
self._sentences = tmp_DictComp0",1,0,1,0
lingvo,https://github.com/tensorflow/lingvo/tree/master/lingvo/executor.py,ExecutorTpu,__init__$168,"tf.logging.info('ps_params_dict=%s', {k: v.ToText() for (k, v) in ps_params_dict.items()})","tf.logging.info('ps_params_dict=%s', {k: v.ToText() for (k, v) in ps_params_dict.items()})","tf.logging.info('ps_params_dict=%s', {k: v.ToText() for (k, v) in ps_params_dict.items()})","tmp_DictComp0 = dict()
for (k, v) in ps_params_dict.items():
    tmp_DictComp0[k] = v.ToText()
tf.logging.info('ps_params_dict=%s', tmp_DictComp0)",1,0,1,0
edx-platform,https://github.com/edx/edx-platform/tree/master/lms/djangoapps/courseware/tests/test_video_mongo.py,VideoBlockTest,test_export_val_transcripts_backward_compatibility$1703,"expected_transcripts = {language: '{edx_video_id}-{language}.srt'.format(edx_video_id=self.descriptor.edx_video_id, language=language) for language in languages}","expected_transcripts = {language: '{edx_video_id}-{language}.srt'.format(edx_video_id=self.descriptor.edx_video_id, language=language) for language in languages}","expected_transcripts = {language: '{edx_video_id}-{language}.srt'.format(edx_video_id=self.descriptor.edx_video_id, language=language) for language in languages}","expected_transcripts = dict()
for language in languages:
    expected_transcripts[language] = '{edx_video_id}-{language}.srt'.format(edx_video_id=self.descriptor.edx_video_id, language=language)
",0,0,0,0
security_monkey,https://github.com/Netflix/security_monkey/tree/master/security_monkey/watcher.py,Watcher,find_modified$313,prev_map = {item.location(): item for item in previous},prev_map = {item.location(): item for item in previous},prev_map = {item.location(): item for item in previous},"prev_map = dict()
for item in previous:
    prev_map[item.location()] = item
",0,0,0,0
python-sdk,https://github.com/qiniu/python-sdk/tree/master/test/unit/test_discovery_v1.py,TestGetCollection,test_get_collection_value_error$2006,"req_copy = {key: val if key is not param else None for (key, val) in req_param_dict.items()}","req_copy = {key: val if key is not param else None for (key, val) in req_param_dict.items()}","req_copy = {key: val if key is not param else None for (key, val) in req_param_dict.items()}","req_copy = dict()
for (key, val) in req_param_dict.items():
    req_copy[key] = val if key is not param else None
",0,0,0,0
django-cachalot,https://github.com/noripyt/django-cachalot/tree/master/cachalot/utils.py,,_invalidate_tables$246,"cache.set_many({get_table_cache_key(db_alias, t): now for t in tables}, cachalot_settings.CACHALOT_TIMEOUT)","cache.set_many({get_table_cache_key(db_alias, t): now for t in tables}, cachalot_settings.CACHALOT_TIMEOUT)","cache.set_many({get_table_cache_key(db_alias, t): now for t in tables}, cachalot_settings.CACHALOT_TIMEOUT)","tmp_DictComp0 = dict()
for t in tables:
    tmp_DictComp0[get_table_cache_key(db_alias, t)] = now
cache.set_many(tmp_DictComp0, cachalot_settings.CACHALOT_TIMEOUT)",1,0,1,0
scikit-bio,https://github.com/biocore/scikit-bio/tree/master/skbio/io/format/embl.py,,_serialize_id$958,"kwargs = {k: '' if v is None else v for (k, v) in obj.items()}","kwargs = {k: '' if v is None else v for (k, v) in obj.items()}","kwargs = {k: '' if v is None else v for (k, v) in obj.items()}","kwargs = dict()
for (k, v) in obj.items():
    kwargs[k] = '' if v is None else v
",0,0,0,0
erpnext,https://github.com/frappe/erpnext/tree/master/erpnext/accounts/doctype/sales_invoice/sales_invoice.py,,get_mode_of_payments_info$2478,return {row.get('mop'): row for row in data},return {row.get('mop'): row for row in data},return {row.get('mop'): row for row in data},"tmp_DictComp0 = dict()
for row in data:
    tmp_DictComp0[row.get('mop')] = row
return tmp_DictComp0",1,0,1,0
trinity,https://github.com/ethereum/trinity/tree/master/p2p/multiplexer.py,Multiplexer,__init__$128,self._protocol_queues = {type(protocol): asyncio.Queue(max_queue_size) for protocol in self.get_protocols()},self._protocol_queues = {type(protocol): asyncio.Queue(max_queue_size) for protocol in self.get_protocols()},self._protocol_queues = {type(protocol): asyncio.Queue(max_queue_size) for protocol in self.get_protocols()},"tmp_DictComp0 = dict()
for protocol in self.get_protocols():
    tmp_DictComp0[type(protocol)] = asyncio.Queue(max_queue_size)
self._protocol_queues = tmp_DictComp0",1,0,1,0
msticpy,https://github.com/microsoft/msticpy/tree/master/msticpy/config/mp_config_control.py,MpConfigControls,_yml_extract_type$354,"val_params = {key: True if val == 'True' else False if val == 'False' else val for (key, val) in val_params.items()}","val_params = {key: True if val == 'True' else False if val == 'False' else val for (key, val) in val_params.items()}","val_params = {key: True if val == 'True' else False if val == 'False' else val for (key, val) in val_params.items()}","tmp_DictComp0 = dict()
for (key, val) in val_params.items():
    tmp_DictComp0[key] = True if val == 'True' else False if val == 'False' else val
val_params = tmp_DictComp0",1,0,1,0
SMARTS,https://github.com/huawei-noah/SMARTS/tree/master/ultra/ultra/env/ultra_env.py,UltraEnv,reset$129,"observations = {agent_id: self._agent_specs[agent_id].observation_adapter(obs) for (agent_id, obs) in observations.items()}","observations = {agent_id: self._agent_specs[agent_id].observation_adapter(obs) for (agent_id, obs) in observations.items()}","observations = {agent_id: self._agent_specs[agent_id].observation_adapter(obs) for (agent_id, obs) in observations.items()}","tmp_DictComp0 = dict()
for (agent_id, obs) in observations.items():
    tmp_DictComp0[agent_id] = self._agent_specs[agent_id].observation_adapter(obs)
observations = tmp_DictComp0",1,0,1,0
numpyro,https://github.com/pyro-ppl/numpyro/tree/master/test/test_handlers.py,,test_subsample_gradient$401,"actual_grads = {name: grad / normalizer for (name, grad) in grads.items()}","actual_grads = {name: grad / normalizer for (name, grad) in grads.items()}","actual_grads = {name: grad / normalizer for (name, grad) in grads.items()}","actual_grads = dict()
for (name, grad) in grads.items():
    actual_grads[name] = grad / normalizer
",0,0,0,0
DIG,https://github.com/divelab/DIG/tree/master/dig/xgraph/method/pgexplainer.py,PlotUtils,plot_sentence$251,"pos_coalition = {k: v for (k, v) in pos.items() if k in nodelist}","pos_coalition = {k: v for (k, v) in pos.items() if k in nodelist}","pos_coalition = {k: v for (k, v) in pos.items() if k in nodelist}","pos_coalition = dict()
for (k, v) in pos.items():
    if k in nodelist:
        pos_coalition[k] = v
",0,0,0,0
electrum,https://github.com/spesmilo/electrum/tree/master/electrum/wallet_db.py,WalletDB,_convert_version_35$732,"requests_new = {k: item for (k, item) in requests_old.items() if not (item['type'] == PR_TYPE_ONCHAIN and item['outputs'] is None)}","requests_new = {k: item for (k, item) in requests_old.items() if not (item['type'] == PR_TYPE_ONCHAIN and item['outputs'] is None)}","requests_new = {k: item for (k, item) in requests_old.items() if not (item['type'] == PR_TYPE_ONCHAIN and item['outputs'] is None)}","requests_new = dict()
for (k, item) in requests_old.items():
    if not (item['type'] == PR_TYPE_ONCHAIN and item['outputs'] is None):
        requests_new[k] = item
",0,0,0,0
pgmpy,https://github.com/pgmpy/pgmpy/tree/master/pgmpy/factors/discrete/DiscreteFactor.py,DiscreteFactor,get_cardinality$127,return {var: self.cardinality[self.variables.index(var)] for var in variables},return {var: self.cardinality[self.variables.index(var)] for var in variables},return {var: self.cardinality[self.variables.index(var)] for var in variables},"tmp_DictComp0 = dict()
for var in variables:
    tmp_DictComp0[var] = self.cardinality[self.variables.index(var)]
return tmp_DictComp0",1,0,1,0
cloud-custodian,https://github.com/cloud-custodian/cloud-custodian/tree/master/tests/test_rds.py,RDSTest,test_rds_tag_and_remove$143,tag_map = {t['Key']: t['Value'] for t in tags['TagList']},tag_map = {t['Key']: t['Value'] for t in tags['TagList']},tag_map = {t['Key']: t['Value'] for t in tags['TagList']},"tag_map = dict()
for t in tags['TagList']:
    tag_map[t['Key']] = t['Value']
",0,0,0,0
tapas,https://github.com/google-research/tapas/tree/master/tapas/utils/hybridqa_utils.py,,convert$430,"tables = {key: parse_table(json_map, descriptions[key]) for (key, json_map) in read_json_directory(table_dir).items()}","tables = {key: parse_table(json_map, descriptions[key]) for (key, json_map) in read_json_directory(table_dir).items()}","tables = {key: parse_table(json_map, descriptions[key]) for (key, json_map) in read_json_directory(table_dir).items()}","tables = dict()
for (key, json_map) in read_json_directory(table_dir).items():
    tables[key] = parse_table(json_map, descriptions[key])
",0,0,0,0
astropy,https://github.com/astropy/astropy/tree/master/astropy/nddata/bitmask.py,BitFlagNameMeta,__new__$83,"members = {k: v if k.startswith('_') else BitFlag(v) for (k, v) in members.items()}","members = {k: v if k.startswith('_') else BitFlag(v) for (k, v) in members.items()}","members = {k: v if k.startswith('_') else BitFlag(v) for (k, v) in members.items()}","tmp_DictComp0 = dict()
for (k, v) in members.items():
    tmp_DictComp0[k] = v if k.startswith('_') else BitFlag(v)
members = tmp_DictComp0",1,0,1,0
texar-pytorch,https://github.com/asyml/texar-pytorch/tree/master/texar/torch/data/tokenizers/tokenizer_base.py,TokenizerBase,add_tokens$258,"added_tok_decoder = {v: k for (k, v) in added_tok_encoder.items()}","added_tok_decoder = {v: k for (k, v) in added_tok_encoder.items()}","added_tok_decoder = {v: k for (k, v) in added_tok_encoder.items()}","added_tok_decoder = dict()
for (k, v) in added_tok_encoder.items():
    added_tok_decoder[v] = k
",0,0,0,0
projects,https://github.com/explosion/projects/tree/master/tutorials/textcat_goemotions/scripts/convert_corpus.py,,convert_record$25,doc.cats = {category: 0 for category in categories},doc.cats = {category: 0 for category in categories},doc.cats = {category: 0 for category in categories},"doc.cats = dict()
for category in categories:
    doc.cats[category] = 0
",0,0,0,0
mvt,https://github.com/mvt-project/mvt/tree/master/mvt/ios/modules/net_base.py,NetBase,find_deleted$178,results_by_proc = {proc['proc_id']: proc for proc in self.results if proc['proc_id']},results_by_proc = {proc['proc_id']: proc for proc in self.results if proc['proc_id']},results_by_proc = {proc['proc_id']: proc for proc in self.results if proc['proc_id']},"results_by_proc = dict()
for proc in self.results:
    if proc['proc_id']:
        results_by_proc[proc['proc_id']] = proc
",0,0,0,0
oppia,https://github.com/oppia/oppia/tree/master/core/controllers/base.py,BaseHandler,validate_and_normalize_args$370,normalized_payload = {arg: normalized_arg_values.get(arg) for arg in payload_arg_keys},normalized_payload = {arg: normalized_arg_values.get(arg) for arg in payload_arg_keys},normalized_payload = {arg: normalized_arg_values.get(arg) for arg in payload_arg_keys},"normalized_payload = dict()
for arg in payload_arg_keys:
    normalized_payload[arg] = normalized_arg_values.get(arg)
",0,0,0,0
fklearn,https://github.com/nubank/fklearn/tree/master/src/fklearn/training/transformation.py,,value_mapper$367,return {key: old_col_value_map[key] if key in old_keys else key for key in new_keys},return {key: old_col_value_map[key] if key in old_keys else key for key in new_keys},return {key: old_col_value_map[key] if key in old_keys else key for key in new_keys},"tmp_DictComp0 = dict()
for key in new_keys:
    tmp_DictComp0[key] = old_col_value_map[key] if key in old_keys else key
return tmp_DictComp0",1,0,1,0
pyro,https://github.com/pyro-ppl/pyro/tree/master/pyro/infer/mcmc/api.py,StreamingMCMC,run$708,"self._statistics.update({(chain_id, name): transformed_sample for (name, transformed_sample) in z_acc.items()})","self._statistics.update({(chain_id, name): transformed_sample for (name, transformed_sample) in z_acc.items()})","self._statistics.update({(chain_id, name): transformed_sample for (name, transformed_sample) in z_acc.items()})","tmp_DictComp0 = dict()
for (name, transformed_sample) in z_acc.items():
    tmp_DictComp0[chain_id, name] = transformed_sample
self._statistics.update(tmp_DictComp0)",1,0,1,0
mario,https://github.com/python-mario/mario/tree/master/src/mario/cli.py,,run$172,mapped_stage_params = {remap.old.lstrip('-'): cli_params[remap.new.lstrip('-')] for remap in stage.remap_params},mapped_stage_params = {remap.old.lstrip('-'): cli_params[remap.new.lstrip('-')] for remap in stage.remap_params},mapped_stage_params = {remap.old.lstrip('-'): cli_params[remap.new.lstrip('-')] for remap in stage.remap_params},"mapped_stage_params = dict()
for remap in stage.remap_params:
    mapped_stage_params[remap.old.lstrip('-')] = cli_params[remap.new.lstrip('-')]
",0,0,0,0
tvm,https://github.com/apache/tvm/tree/master/tests/python/contrib/test_ethosu/test_legalize.py,,test_tflite_concat_legalize$1559,"(relay_module, _) = relay.frontend.from_tflite(tflite_model, shape_dict={'ifm' + str(i): shape for (i, shape) in enumerate(shapes)}, dtype_dict={'ifm' + str(i): 'int8' for (i, _) in enumerate(shapes)})","shape_dict={'ifm' + str(i): shape for (i, shape) in enumerate(shapes)}","shape_dict={'ifm' + str(i): shape for (i, shape) in enumerate(shapes)}","tmp_DictComp0 = dict()
for (i, shape) in enumerate(shapes):
    tmp_DictComp0['ifm' + str(i)] = shape
(relay_module, _) = relay.frontend.from_tflite(tflite_model, shape_dict=tmp_DictComp0, dtype_dict={'ifm' + str(i): 'int8' for (i, _) in enumerate(shapes)})",1,0,1,0
catalyst,https://github.com/scrtlabs/catalyst/tree/master/examples/catalyst_rl/misc.py,,structed2dict$22,array = {key: array[key] for key in array.dtype.fields.keys()},array = {key: array[key] for key in array.dtype.fields.keys()},array = {key: array[key] for key in array.dtype.fields.keys()},"tmp_DictComp0 = dict()
for key in array.dtype.fields.keys():
    tmp_DictComp0[key] = array[key]
array = tmp_DictComp0",1,0,1,0
great_expectations,https://github.com/great-expectations/great_expectations/tree/master/great_expectations/rule_based_profiler/rule/rule.py,Rule,_get_expectation_configuration_builders_as_dict$297,return {expectation_configuration_builder.expectation_type: expectation_configuration_builder for expectation_configuration_builder in expectation_configuration_builders},return {expectation_configuration_builder.expectation_type: expectation_configuration_builder for expectation_configuration_builder in expectation_configuration_builders},return {expectation_configuration_builder.expectation_type: expectation_configuration_builder for expectation_configuration_builder in expectation_configuration_builders},"tmp_DictComp0 = dict()
for expectation_configuration_builder in expectation_configuration_builders:
    tmp_DictComp0[expectation_configuration_builder.expectation_type] = expectation_configuration_builder
return tmp_DictComp0",1,0,1,0
mlrun,https://github.com/mlrun/mlrun/tree/master/tests/system/demos/horovod/assets/utils_functions.py,,categories_map_builder$50,"categories = {i: category for (i, category) in enumerate(categories)}","categories = {i: category for (i, category) in enumerate(categories)}","categories = {i: category for (i, category) in enumerate(categories)}","tmp_DictComp0 = dict()
for (i, category) in enumerate(categories):
    tmp_DictComp0[i] = category
categories = tmp_DictComp0",1,0,1,0
MONAI,https://github.com/Project-MONAI/MONAI/tree/master/monai/data/utils.py,,convert_tables_to_dicts$1076,"data = [dict(d, **{k: v[i] for (k, v) in groups.items()}) for (i, d) in enumerate(data)]","**{k: v[i] for (k, v) in groups.items()}","**{k: v[i] for (k, v) in groups.items()}","def my_comprehension_func(i):
    tmp_DictComp0 = dict()
    for (k, v) in groups.items():
        tmp_DictComp0[k] = v[i]
    return tmp_DictComp0
data = [dict(d, **{k: v[i] for (k, v) in groups.items()}) for (i, d) in enumerate(data)]",1,1,1,1
LM-LSTM-CRF,https://github.com/LiyuanLucasLiu/LM-LSTM-CRF/tree/master/model/utils.py,,generate_corpus_char$124,"char_map = {shrink_char_count[ind]: ind for ind in range(0, len(shrink_char_count))}","char_map = {shrink_char_count[ind]: ind for ind in range(0, len(shrink_char_count))}","char_map = {shrink_char_count[ind]: ind for ind in range(0, len(shrink_char_count))}","char_map = dict()
for ind in range(0, len(shrink_char_count)):
    char_map[shrink_char_count[ind]] = ind
",0,0,0,0
SMARTS,https://github.com/huawei-noah/SMARTS/tree/master/baselines/marl_benchmark/agents/maddpg/tf_policy.py,MADDPG2TFPolicy,gradients$296,"self.gvs = {k: optimizer.compute_gradients(self.losses[k], self.vars[k]) for (k, optimizer) in self.optimizers.items()}","self.gvs = {k: optimizer.compute_gradients(self.losses[k], self.vars[k]) for (k, optimizer) in self.optimizers.items()}","self.gvs = {k: optimizer.compute_gradients(self.losses[k], self.vars[k]) for (k, optimizer) in self.optimizers.items()}","tmp_DictComp0 = dict()
for (k, optimizer) in self.optimizers.items():
    tmp_DictComp0[k] = optimizer.compute_gradients(self.losses[k], self.vars[k])
self.gvs = tmp_DictComp0",1,0,1,0
EasyMocap,https://github.com/zju3dv/EasyMocap/tree/master/easymocap/pyfitting/optimize_simple.py,,closure$222,"loss_dict = {key: func(kpts_est=kpts_est, **new_params) for (key, func) in loss_funcs.items()}","loss_dict = {key: func(kpts_est=kpts_est, **new_params) for (key, func) in loss_funcs.items()}","loss_dict = {key: func(kpts_est=kpts_est, **new_params) for (key, func) in loss_funcs.items()}","loss_dict = dict()
for (key, func) in loss_funcs.items():
    loss_dict[key] = func(kpts_est=kpts_est, **new_params)
",0,0,0,0
,,,,,,,,185,23,185,10
,,,,,,,,,,0.485564304,0.026246719
