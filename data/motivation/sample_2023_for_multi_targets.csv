repo_name,file_html,cl,me,complicate_code,simple_code
news-please,https://github.com/fhamborg/news-please/tree/master/newsplease/pipeline/extractor/comparer/comparer_title.py,ComparerTitle,find_matches$7,"for (a, b) in itertools.combinations(list_title, 2):
    if a == b:
        list_title_matches.append(a)","for e_target in itertools.combinations(list_title, 2):
    b = e_target[1]
    a = e_target[0]
    if a == b:
        list_title_matches.append(a)"
uvicorn,https://github.com/encode/uvicorn/tree/master/uvicorn/protocols/http/h11_impl.py,H11Protocol,handle_events$133,"for (name, value) in self.headers:
    if name == b'connection':
        tokens = [token.lower().strip() for token in value.split(b',')]
        if b'upgrade' in tokens:
            self.handle_upgrade(event)
            return","for e_target in self.headers:
    value = e_target[1]
    name = e_target[0]
    if name == b'connection':
        tokens = [token.lower().strip() for token in value.split(b',')]
        if b'upgrade' in tokens:
            self.handle_upgrade(event)
            return"
Basic_CNNs_TensorFlow2,https://github.com/calmisential/Basic_CNNs_TensorFlow2/tree/master//show_model_list.py,,if_main_my$3,"for (model_name, i) in get_model2idx_dict().items():
    print('MODEL: {}, INDEX: {}'.format(model_name, i))
    f.write('{}: {}\n'.format(model_name, i))","for e_target in get_model2idx_dict().items():
    i = e_target[1]
    model_name = e_target[0]
    print('MODEL: {}, INDEX: {}'.format(model_name, i))
    f.write('{}: {}\n'.format(model_name, i))"
moto,https://github.com/spulec/moto/tree/master/moto/core/responses.py,BaseResponse,_parse_param$637,"for (i, key) in enumerate(keylist[:-1]):
    if key in obj:
        parent = obj
        obj = obj[key]
    elif key == 'member':
        if not isinstance(obj, list):
            obj = []
            parent[keylist[i - 1]] = obj
    elif key.isdigit():
        index = int(key) - 1
        if len(obj) <= index:
            obj.insert(index, {})
        parent = obj
        obj = obj[index]
    else:
        obj[key] = {}
        parent = obj
        obj = obj[key]","for e_target in enumerate(keylist[:-1]):
    key = e_target[1]
    i = e_target[0]
    if key in obj:
        parent = obj
        obj = obj[key]
    elif key == 'member':
        if not isinstance(obj, list):
            obj = []
            parent[keylist[i - 1]] = obj
    elif key.isdigit():
        index = int(key) - 1
        if len(obj) <= index:
            obj.insert(index, {})
        parent = obj
        obj = obj[index]
    else:
        obj[key] = {}
        parent = obj
        obj = obj[key]"
SFA3D,https://github.com/maudzung/SFA3D/tree/master/sfa/utils/lr_scheduler.py,OneCyclePolicy,__init__$272,"for (i, g) in enumerate(optimizer.param_groups):
    g['momentum'] = momentum_rng[1]","for e_target in enumerate(optimizer.param_groups):
    g = e_target[1]
    i = e_target[0]
    g['momentum'] = momentum_rng[1]"
checkmk,https://github.com/tribe29/checkmk/tree/master/omd/license_sources/update_licenses.py,,update_js_dependencies$431,"for (key, data) in license_json.items():
    if key in old_pkg_keys:
        continue
    name = re.sub('@[^@]*$', '', key)
    version = re.sub('^.+@', '', key)
    license_ = data['licenses']
    license_link = license_links[license_] if license_ in license_links else ''
    rows.append([name, version, license_, license_link, ''])","for e_target in license_json.items():
    data = e_target[1]
    key = e_target[0]
    if key in old_pkg_keys:
        continue
    name = re.sub('@[^@]*$', '', key)
    version = re.sub('^.+@', '', key)
    license_ = data['licenses']
    license_link = license_links[license_] if license_ in license_links else ''
    rows.append([name, version, license_, license_link, ''])"
keras,https://github.com/keras-team/keras/tree/master/keras/saving/saved_model_experimental.py,,_create_signature_def_map$322,"for (key, value) in metrics.items():
    if isinstance(value, metrics_lib.Metric):
        vars_to_add.update(value.variables)
        metrics[key] = (value.result(), value.updates[0])","for e_target in metrics.items():
    value = e_target[1]
    key = e_target[0]
    if isinstance(value, metrics_lib.Metric):
        vars_to_add.update(value.variables)
        metrics[key] = (value.result(), value.updates[0])"
Malcolm,https://github.com/cisagov/Malcolm/tree/master/shared/bin/beat-log-temperature.py,,main$23,"for (k, v) in metric_class_values.items():
    message[f'{k}_avg'] = reduce(lambda a, b: a + b, v) / len(v)","for e_target in metric_class_values.items():
    v = e_target[1]
    k = e_target[0]
    message[f'{k}_avg'] = reduce(lambda a, b: a + b, v) / len(v)"
tmuxp,https://github.com/tmux-python/tmuxp/tree/master/tmuxp/workspacebuilder.py,WorkspaceBuilder,iter_create_panes$292,"for (pindex, pconf) in enumerate(wconf['panes'], start=pane_base_index):
    if pindex == int(pane_base_index):
        p = w.attached_pane
    else:

        def get_pane_start_directory():
            if 'start_directory' in pconf:
                return pconf['start_directory']
            elif 'start_directory' in wconf:
                return wconf['start_directory']
            else:
                return None
        p = w.split_window(attach=True, start_directory=get_pane_start_directory(), target=p.id)
    assert isinstance(p, Pane)
    if 'layout' in wconf:
        w.select_layout(wconf['layout'])
    if 'suppress_history' in pconf:
        suppress = pconf['suppress_history']
    elif 'suppress_history' in wconf:
        suppress = wconf['suppress_history']
    else:
        suppress = True
    for cmd in pconf['shell_command']:
        p.send_keys(cmd, suppress_history=suppress)
    if 'focus' in pconf and pconf['focus']:
        w.select_pane(p['pane_id'])
    w.server._update_panes()
    yield (p, pconf)","for e_target in enumerate(wconf['panes'], start=pane_base_index):
    pconf = e_target[1]
    pindex = e_target[0]
    if pindex == int(pane_base_index):
        p = w.attached_pane
    else:

        def get_pane_start_directory():
            if 'start_directory' in pconf:
                return pconf['start_directory']
            elif 'start_directory' in wconf:
                return wconf['start_directory']
            else:
                return None
        p = w.split_window(attach=True, start_directory=get_pane_start_directory(), target=p.id)
    assert isinstance(p, Pane)
    if 'layout' in wconf:
        w.select_layout(wconf['layout'])
    if 'suppress_history' in pconf:
        suppress = pconf['suppress_history']
    elif 'suppress_history' in wconf:
        suppress = wconf['suppress_history']
    else:
        suppress = True
    for cmd in pconf['shell_command']:
        p.send_keys(cmd, suppress_history=suppress)
    if 'focus' in pconf and pconf['focus']:
        w.select_pane(p['pane_id'])
    w.server._update_panes()
    yield (p, pconf)"
AIGames,https://github.com/CharlesPikachu/AIGames/tree/master/AIGobang/Algorithm_1/modules/online/server.py,gobangSever,responseForReceiveData$167,"for (i, j) in product(range(19), range(19)):
    if self.chessboard[i][j]:
        self.chessboard[i][j].close()
        self.chessboard[i][j] = None","for e_target in product(range(19), range(19)):
    j = e_target[1]
    i = e_target[0]
    if self.chessboard[i][j]:
        self.chessboard[i][j].close()
        self.chessboard[i][j] = None"
galaxy,https://github.com/ansible/galaxy/tree/master/lib/galaxy/jobs/runners/util/cli/job/slurm.py,Slurm,job_script_kwargs$23,"for (k, v) in self.params.items():
    if k == 'plugin':
        continue
    try:
        if not k.startswith('-'):
            k = argmap[k]
        scriptargs[k] = v
    except Exception:
        log.warning(f'Unrecognized long argument passed to Slurm CLI plugin: {k}')","for e_target in self.params.items():
    v = e_target[1]
    k = e_target[0]
    if k == 'plugin':
        continue
    try:
        if not k.startswith('-'):
            k = argmap[k]
        scriptargs[k] = v
    except Exception:
        log.warning(f'Unrecognized long argument passed to Slurm CLI plugin: {k}')"
PytorchSSD,https://github.com/lzx1413/PytorchSSD/tree/master/models/FSSD_mobile.py,,multibox$177,"for (i, fea_channel) in enumerate(fea_channels):
    loc_layers += [nn.Conv2d(fea_channel, cfg[i] * 4, kernel_size=3, padding=1)]
    conf_layers += [nn.Conv2d(fea_channel, cfg[i] * num_classes, kernel_size=3, padding=1)]","for e_target in enumerate(fea_channels):
    fea_channel = e_target[1]
    i = e_target[0]
    loc_layers += [nn.Conv2d(fea_channel, cfg[i] * 4, kernel_size=3, padding=1)]
    conf_layers += [nn.Conv2d(fea_channel, cfg[i] * num_classes, kernel_size=3, padding=1)]"
pyro,https://github.com/pyro-ppl/pyro/tree/master/tests/contrib/mue/test_dataloaders.py,,test_biosequencedataset$13,"for (j, s) in enumerate(seqs[i] + include_stop * '*'):
    seq_data_check[i, j, list(alphabet_list).index(s)] = 1","for e_target in enumerate(seqs[i] + include_stop * '*'):
    s = e_target[1]
    j = e_target[0]
    seq_data_check[i, j, list(alphabet_list).index(s)] = 1"
pytest-benchmark,https://github.com/ionelmc/pytest-benchmark/tree/master/src/pytest_benchmark/utils.py,,commonpath$602,"for (i, c) in enumerate(s1):
    if c != s2[i]:
        common = common[:i]
        break
else:
    common = common[:len(s1)]","for e_target in enumerate(s1):
    c = e_target[1]
    i = e_target[0]
    if c != s2[i]:
        common = common[:i]
        break
else:
    common = common[:len(s1)]"
DetectoRS,https://github.com/joe-siyuan-qiao/DetectoRS/tree/master/mmdet/utils/flops_counter.py,,add_flops_counter_hook_function$420,"for (mod_type, counter_hook) in hook_mapping.items():
    if issubclass(type(module), mod_type):
        handle = module.register_forward_hook(counter_hook)
        break","for e_target in hook_mapping.items():
    counter_hook = e_target[1]
    mod_type = e_target[0]
    if issubclass(type(module), mod_type):
        handle = module.register_forward_hook(counter_hook)
        break"
AugLy,https://github.com/facebookresearch/AugLy/tree/master/augly/tests/text_tests/transforms_unit_test.py,,are_equal_metadata$15,"for ((act_k, act_v), (exp_k, exp_v)) in zip(sorted(actual_dict.items(), key=lambda kv: kv[0]), sorted(expected_dict.items(), key=lambda kv: kv[0])):
    if act_k != exp_k:
        return False
    if act_v == exp_v:
        continue
    '\n            Allow relative paths in expected metadata: just check that the end of the\n            actual path matches the expected path\n            '
    if not (isinstance(act_v, str) and isinstance(exp_v, str) and (os.path.normpath(act_v[-len(exp_v):]).split(os.path.sep) == os.path.normpath(exp_v).split(os.path.sep))):
        return False","for e_target in zip(sorted(actual_dict.items(), key=lambda kv: kv[0]), sorted(expected_dict.items(), key=lambda kv: kv[0])):
    exp_v = e_target[1][1]
    exp_k = e_target[1][0]
    act_v = e_target[0][1]
    act_k = e_target[0][0]
    if act_k != exp_k:
        return False
    if act_v == exp_v:
        continue
    '\n            Allow relative paths in expected metadata: just check that the end of the\n            actual path matches the expected path\n            '
    if not (isinstance(act_v, str) and isinstance(exp_v, str) and (os.path.normpath(act_v[-len(exp_v):]).split(os.path.sep) == os.path.normpath(exp_v).split(os.path.sep))):
        return False"
imgaug,https://github.com/aleju/imgaug/tree/master/imgaug/augmenters/geometric.py,AffineCv2,_augment_heatmaps$2778,"for (heatmap_i, arr_aug) in zip(heatmaps, arrs_aug):
    heatmap_i.arr_0to1 = arr_aug","for e_target in zip(heatmaps, arrs_aug):
    arr_aug = e_target[1]
    heatmap_i = e_target[0]
    heatmap_i.arr_0to1 = arr_aug"
open_model_zoo,https://github.com/openvinotoolkit/open_model_zoo/tree/master/tools/accuracy_checker/tests/test_segmentation_metrics.py,TestPixelAccuracy,test_one_class$39,"for (_, evaluation_result) in dispatcher.iterate_metrics(annotations, predictions):
    assert evaluation_result == expected","for e_target in dispatcher.iterate_metrics(annotations, predictions):
    evaluation_result = e_target[1]
    _ = e_target[0]
    assert evaluation_result == expected"
micronet,https://github.com/666DZY666/micronet/tree/master/micronet/compression/quantization/wqaq/iao/bn_fuse/bn_fused_model_test.py,,test_quant_bn_fused_model_train$24,"for (data, target) in testloader:
    if not args.cpu:
        (data, target) = (data.cuda(), target.cuda())
    (data, target) = (Variable(data), Variable(target))
    quant_bn_fused_model_train_output = quant_bn_fused_model_train(data)
    quant_bn_fused_model_train_test_loss += criterion(quant_bn_fused_model_train_output, target).data.item()
    quant_bn_fused_model_train_pred = quant_bn_fused_model_train_output.data.max(1, keepdim=True)[1]
    quant_bn_fused_model_train_correct += quant_bn_fused_model_train_pred.eq(target.data.view_as(quant_bn_fused_model_train_pred)).cpu().sum()","for e_target in testloader:
    target = e_target[1]
    data = e_target[0]
    if not args.cpu:
        (data, target) = (data.cuda(), target.cuda())
    (data, target) = (Variable(data), Variable(target))
    quant_bn_fused_model_train_output = quant_bn_fused_model_train(data)
    quant_bn_fused_model_train_test_loss += criterion(quant_bn_fused_model_train_output, target).data.item()
    quant_bn_fused_model_train_pred = quant_bn_fused_model_train_output.data.max(1, keepdim=True)[1]
    quant_bn_fused_model_train_correct += quant_bn_fused_model_train_pred.eq(target.data.view_as(quant_bn_fused_model_train_pred)).cpu().sum()"
pyradio,https://github.com/coderholic/pyradio/tree/master/pyradio/browser.py,,capitalize_comma_separated_string$108,"for (i, n) in enumerate(sp):
    sp[i] = n.strip().capitalize()","for e_target in enumerate(sp):
    n = e_target[1]
    i = e_target[0]
    sp[i] = n.strip().capitalize()"
python,https://github.com/zhanghe06/python/tree/master/kubernetes/client/models/v1_cross_version_object_reference.py,V1CrossVersionObjectReference,to_dict$136,"for (attr, _) in six.iteritems(self.openapi_types):
    value = getattr(self, attr)
    if isinstance(value, list):
        result[attr] = list(map(lambda x: x.to_dict() if hasattr(x, 'to_dict') else x, value))
    elif hasattr(value, 'to_dict'):
        result[attr] = value.to_dict()
    elif isinstance(value, dict):
        result[attr] = dict(map(lambda item: (item[0], item[1].to_dict()) if hasattr(item[1], 'to_dict') else item, value.items()))
    else:
        result[attr] = value","for e_target in six.iteritems(self.openapi_types):
    _ = e_target[1]
    attr = e_target[0]
    value = getattr(self, attr)
    if isinstance(value, list):
        result[attr] = list(map(lambda x: x.to_dict() if hasattr(x, 'to_dict') else x, value))
    elif hasattr(value, 'to_dict'):
        result[attr] = value.to_dict()
    elif isinstance(value, dict):
        result[attr] = dict(map(lambda item: (item[0], item[1].to_dict()) if hasattr(item[1], 'to_dict') else item, value.items()))
    else:
        result[attr] = value"
few-shot-vid2vid,https://github.com/NVlabs/few-shot-vid2vid/tree/master/data/image_folder.py,,make_dataset$33,"for (root, dnames, fnames) in sorted(os.walk(dir)):
    for fname in fnames:
        if is_image_file(fname):
            path = os.path.join(root, fname)
            images.append(path)","for e_target in sorted(os.walk(dir)):
    fnames = e_target[2]
    dnames = e_target[1]
    root = e_target[0]
    for fname in fnames:
        if is_image_file(fname):
            path = os.path.join(root, fname)
            images.append(path)"
django,https://github.com/django/django/tree/master/tests/template_tests/syntax_tests/i18n/test_blocktranslate.py,,setup$19,"for (tag_name, setup_func) in tags.items():
    if 'tag_name' in signature.parameters:
        setup_func(partial(func, tag_name=tag_name))(self)
    else:
        setup_func(func)(self)","for e_target in tags.items():
    setup_func = e_target[1]
    tag_name = e_target[0]
    if 'tag_name' in signature.parameters:
        setup_func(partial(func, tag_name=tag_name))(self)
    else:
        setup_func(func)(self)"
PathPlanning,https://github.com/zhm-real/PathPlanning/tree/master/Sampling_based_Planning/rrt_2D/batch_informed_trees.py,BITStar,animation$316,"for (v, w) in self.Tree.E:
    plt.plot([v.x, w.x], [v.y, w.y], '-g')","for e_target in self.Tree.E:
    w = e_target[1]
    v = e_target[0]
    plt.plot([v.x, w.x], [v.y, w.y], '-g')"
category_encoders,https://github.com/scikit-learn-contrib/category_encoders/tree/master/category_encoders/one_hot.py,OneHotEncoder,generate_mapping$184,"for (cat_name, class_) in values.iteritems():
    if pd.isna(cat_name) and self.handle_missing == 'return_nan':
        append_nan_to_index = class_
        continue
    if self.use_cat_names:
        n_col_name = str(col) + '_%s' % (cat_name,)
        found_count = found_column_counts.get(n_col_name, 0)
        found_column_counts[n_col_name] = found_count + 1
        n_col_name += '#' * found_count
    else:
        n_col_name = str(col) + '_%s' % (class_,)
    index.append(class_)
    new_columns.append(n_col_name)","for e_target in values.iteritems():
    class_ = e_target[1]
    cat_name = e_target[0]
    if pd.isna(cat_name) and self.handle_missing == 'return_nan':
        append_nan_to_index = class_
        continue
    if self.use_cat_names:
        n_col_name = str(col) + '_%s' % (cat_name,)
        found_count = found_column_counts.get(n_col_name, 0)
        found_column_counts[n_col_name] = found_count + 1
        n_col_name += '#' * found_count
    else:
        n_col_name = str(col) + '_%s' % (class_,)
    index.append(class_)
    new_columns.append(n_col_name)"
django-anymail,https://github.com/anymail/django-anymail/tree/master/anymail/webhooks/mailjet.py,MailjetInboundWebhookView,_construct_mailjet_attachment$173,"for (name, value) in self._flatten_mailjet_headers(part.get('Headers', {})):
    part_headers.add_header(name, value)","for e_target in self._flatten_mailjet_headers(part.get('Headers', {})):
    value = e_target[1]
    name = e_target[0]
    part_headers.add_header(name, value)"
skll,https://github.com/EducationalTestingService/skll/tree/master/skll/data/readers.py,Reader,_sub_read_rows$215,"for (ex_num, (_, _, feat_dict)) in enumerate(self._sub_read(f)):
    yield feat_dict
    if ex_num % 100 == 0:
        self._print_progress(f'{100 * ex_num / total:.8}%')","for e_target in enumerate(self._sub_read(f)):
    feat_dict = e_target[1][2]
    _ = e_target[1][1]
    ex_num = e_target[0]
    yield feat_dict
    if ex_num % 100 == 0:
        self._print_progress(f'{100 * ex_num / total:.8}%')"
aliyun-openapi-python-sdk,https://github.com/aliyun/aliyun-openapi-python-sdk/tree/master/aliyun-python-sdk-alb/aliyunsdkalb/request/v20200616/DissociateAdditionalCertificatesFromListenerRequest.py,DissociateAdditionalCertificatesFromListenerRequest,set_Certificates$52,"for (index1, value1) in enumerate(Certificates):
    if value1.get('CertificateId') is not None:
        self.add_query_param('Certificates.' + str(index1 + 1) + '.CertificateId', value1.get('CertificateId'))","for e_target in enumerate(Certificates):
    value1 = e_target[1]
    index1 = e_target[0]
    if value1.get('CertificateId') is not None:
        self.add_query_param('Certificates.' + str(index1 + 1) + '.CertificateId', value1.get('CertificateId'))"
airflow,https://github.com/apache/airflow/tree/master/airflow/providers/apache/sqoop/hooks/sqoop.py,SqoopHook,_export_cmd$290,"for (key, value) in extra_export_options.items():
    cmd += [f'--{key}']
    if value:
        cmd += [str(value)]","for e_target in extra_export_options.items():
    value = e_target[1]
    key = e_target[0]
    cmd += [f'--{key}']
    if value:
        cmd += [str(value)]"
calamari,https://github.com/Calamari-OCR/calamari/tree/master/calamari_ocr/ocr/dataset/textprocessors/text_synchronizer.py,,synchronize$50,"for (i, text) in enumerate(texts):
    sync.set_all(i, [0, len(text) - 1, len(text)])","for e_target in enumerate(texts):
    text = e_target[1]
    i = e_target[0]
    sync.set_all(i, [0, len(text) - 1, len(text)])"
dask-ml,https://github.com/dask/dask-ml/tree/master/tests/model_selection/test_hyperband.py,,test_params_passed$309,"for (k, v) in params.items():
    if k == 'random_state':
        continue
    assert SHA_params[k] == v","for e_target in params.items():
    v = e_target[1]
    k = e_target[0]
    if k == 'random_state':
        continue
    assert SHA_params[k] == v"
LibCST,https://github.com/Instagram/LibCST/tree/master/libcst/testing/utils.py,,validate_provider_tests$106,"for (member_name, member) in dct.items():
    test_limit = try_get_provider_attr(member_name, member, PROVIDER_TEST_LIMIT_ATTR_NAME)
    if test_limit is not None:
        data = try_get_provider_attr(member_name, member, DATA_PROVIDER_DATA_ATTR_NAME)
        num_tests = len(data) if data else 1
        if num_tests > test_limit:

            def test_replacement(self: Any, member_name: Any=member_name, num_tests: Any=num_tests, test_limit: Any=test_limit) -> None:
                raise AssertionError(f'{member_name} generated {num_tests} tests but the limit is ' + f'{test_limit}. You can increase the number of ' + 'allowed tests by specifying test_limit, but please ' + 'consider whether you really need to test all of ' + 'these combinations.')
            test_replacement.__name__ = member_name
            members_to_replace[member_name] = test_replacement","for e_target in dct.items():
    member = e_target[1]
    member_name = e_target[0]
    test_limit = try_get_provider_attr(member_name, member, PROVIDER_TEST_LIMIT_ATTR_NAME)
    if test_limit is not None:
        data = try_get_provider_attr(member_name, member, DATA_PROVIDER_DATA_ATTR_NAME)
        num_tests = len(data) if data else 1
        if num_tests > test_limit:

            def test_replacement(self: Any, member_name: Any=member_name, num_tests: Any=num_tests, test_limit: Any=test_limit) -> None:
                raise AssertionError(f'{member_name} generated {num_tests} tests but the limit is ' + f'{test_limit}. You can increase the number of ' + 'allowed tests by specifying test_limit, but please ' + 'consider whether you really need to test all of ' + 'these combinations.')
            test_replacement.__name__ = member_name
            members_to_replace[member_name] = test_replacement"
meshio,https://github.com/nschloe/meshio/tree/master/tests/legacy_reader.py,,_read_cells$22,"for (vtk_type, meshio_type) in vtk_to_meshio_type.items():
    os = offsets[np.argwhere(types == vtk_type).transpose()[0]]
    num_cells = len(os)
    if num_cells > 0:
        if meshio_type == 'polygon':
            for idx_cell in range(num_cells):
                num_pts = data[os[idx_cell]]
                cell = data[os[idx_cell] + 1:os[idx_cell] + 1 + num_pts]
                key = meshio_type + str(num_pts)
                if key in cells:
                    cells[key] = np.vstack([cells[key], cell])
                else:
                    cells[key] = cell
        else:
            num_pts = data[os[0]]
            arr = np.empty((num_cells, num_pts), dtype=int)
            for k in range(num_pts):
                arr[:, k] = data[os + k + 1]
            cells[meshio_type] = arr","for e_target in vtk_to_meshio_type.items():
    meshio_type = e_target[1]
    vtk_type = e_target[0]
    os = offsets[np.argwhere(types == vtk_type).transpose()[0]]
    num_cells = len(os)
    if num_cells > 0:
        if meshio_type == 'polygon':
            for idx_cell in range(num_cells):
                num_pts = data[os[idx_cell]]
                cell = data[os[idx_cell] + 1:os[idx_cell] + 1 + num_pts]
                key = meshio_type + str(num_pts)
                if key in cells:
                    cells[key] = np.vstack([cells[key], cell])
                else:
                    cells[key] = cell
        else:
            num_pts = data[os[0]]
            arr = np.empty((num_cells, num_pts), dtype=int)
            for k in range(num_pts):
                arr[:, k] = data[os + k + 1]
            cells[meshio_type] = arr"
salt,https://github.com/saltstack/salt/tree/master/salt/utils/dns.py,,_weighted_order$168,"for (i, w) in enumerate(weights):
    rnd -= w
    if rnd < 0:
        res.append(recs.pop(i)['name'])
        weights.pop(i)
        break","for e_target in enumerate(weights):
    w = e_target[1]
    i = e_target[0]
    rnd -= w
    if rnd < 0:
        res.append(recs.pop(i)['name'])
        weights.pop(i)
        break"
msg-extractor,https://github.com/TeamMsgExtractor/msg-extractor/tree/master/extract_msg/utils.py,,parseType$389,"for (x, y) in enumerate(extras):
    if lengths[x] != len(y):
        logger.warning('Error while parsing multiple type. Expected length {}, got {}. Ignoring.'.format(lengths[x], len(y)))","for e_target in enumerate(extras):
    y = e_target[1]
    x = e_target[0]
    if lengths[x] != len(y):
        logger.warning('Error while parsing multiple type. Expected length {}, got {}. Ignoring.'.format(lengths[x], len(y)))"
auditok,https://github.com/amsehili/auditok/tree/master/tests/test_workers.py,TestWorkers,test_CommandLineWorker$179,"for (i, (det, exp, log_line)) in enumerate(zip(tokenizer.detections, self.expected, log_lines), 1):
    (start, end) = exp
    exp_log_line = log_fmt.format(i, command_format)
    self.assertAlmostEqual(det.start, start)
    self.assertAlmostEqual(det.end, end)
    self.assertEqual(log_line[28:].strip(), exp_log_line)","for e_target in enumerate(zip(tokenizer.detections, self.expected, log_lines), 1):
    log_line = e_target[1][2]
    exp = e_target[1][1]
    det = e_target[1][0]
    i = e_target[0]
    (start, end) = exp
    exp_log_line = log_fmt.format(i, command_format)
    self.assertAlmostEqual(det.start, start)
    self.assertAlmostEqual(det.end, end)
    self.assertEqual(log_line[28:].strip(), exp_log_line)"
Pancakeswap-and-uniswap-trading-bot,https://github.com/aviddot/Pancakeswap-and-uniswap-trading-bot/tree/master/source files/pancakeswap_bot.py,Worker,getprice$504,"for (token_number, eth_address, high, low, activate, stoploss_value, stoploss_activate, trade_with_ERC, trade_with_ETH, fast_token, small_case_name, decimals, balance, price, dollar_balance) in all_token_information:
    if eth_address != '0':
        if priceright == 'sell':
            token1eth = uniswap_wrapper.get_eth_token_input_price(w33.toChecksumAddress(eth_address), threeeth)
            token1eth2 = token1eth / threeeth
            if decimals != 18:
                pricetoken1usd = priceeth / token1eth2 / 10 ** (18 - decimals)
                dollarbalancetoken1 = pricetoken1usd * balance
                all_token_information[token_number - 1] = all_token_information[token_number - 1][:13] + (pricetoken1usd, dollarbalancetoken1)
            else:
                pricetoken1usd = priceeth / token1eth2
                dollarbalancetoken1 = pricetoken1usd * balance
                if eth_address == '0x8ac76a51cc950d9822d68b83fe1ad97b32cd580d':
                    pricetoken1usd = 1.032319
                all_token_information[token_number - 1] = all_token_information[token_number - 1][:13] + (pricetoken1usd, dollarbalancetoken1)
        else:
            token1eth = uniswap_wrapper.get_token_eth_output_price(w33.toChecksumAddress(eth_address), threeeth)
            token1eth2 = token1eth / threeeth
            if decimals != 18:
                pricetoken1usd = priceeth / token1eth2 / 10 ** (18 - decimals)
                dollarbalancetoken1 = pricetoken1usd * balance
                all_token_information[token_number - 1] = all_token_information[token_number - 1][:13] + (pricetoken1usd, dollarbalancetoken1)
            else:
                pricetoken1usd = priceeth / token1eth2
                dollarbalancetoken1 = pricetoken1usd * balance
                if eth_address == '0x8ac76a51cc950d9822d68b83fe1ad97b32cd580d':
                    pricetoken1usd = 1.032319
                all_token_information[token_number - 1] = all_token_information[token_number - 1][:13] + (pricetoken1usd, dollarbalancetoken1)
    else:
        pricetoken1usd = 0
        dollarbalancetoken1 = 0
        all_token_information[token_number - 1] = all_token_information[token_number - 1][:13] + (pricetoken1usd, dollarbalancetoken1)","for e_target in all_token_information:
    dollar_balance = e_target[14]
    price = e_target[13]
    balance = e_target[12]
    decimals = e_target[11]
    small_case_name = e_target[10]
    fast_token = e_target[9]
    trade_with_ETH = e_target[8]
    trade_with_ERC = e_target[7]
    stoploss_activate = e_target[6]
    stoploss_value = e_target[5]
    activate = e_target[4]
    low = e_target[3]
    high = e_target[2]
    eth_address = e_target[1]
    token_number = e_target[0]
    if eth_address != '0':
        if priceright == 'sell':
            token1eth = uniswap_wrapper.get_eth_token_input_price(w33.toChecksumAddress(eth_address), threeeth)
            token1eth2 = token1eth / threeeth
            if decimals != 18:
                pricetoken1usd = priceeth / token1eth2 / 10 ** (18 - decimals)
                dollarbalancetoken1 = pricetoken1usd * balance
                all_token_information[token_number - 1] = all_token_information[token_number - 1][:13] + (pricetoken1usd, dollarbalancetoken1)
            else:
                pricetoken1usd = priceeth / token1eth2
                dollarbalancetoken1 = pricetoken1usd * balance
                if eth_address == '0x8ac76a51cc950d9822d68b83fe1ad97b32cd580d':
                    pricetoken1usd = 1.032319
                all_token_information[token_number - 1] = all_token_information[token_number - 1][:13] + (pricetoken1usd, dollarbalancetoken1)
        else:
            token1eth = uniswap_wrapper.get_token_eth_output_price(w33.toChecksumAddress(eth_address), threeeth)
            token1eth2 = token1eth / threeeth
            if decimals != 18:
                pricetoken1usd = priceeth / token1eth2 / 10 ** (18 - decimals)
                dollarbalancetoken1 = pricetoken1usd * balance
                all_token_information[token_number - 1] = all_token_information[token_number - 1][:13] + (pricetoken1usd, dollarbalancetoken1)
            else:
                pricetoken1usd = priceeth / token1eth2
                dollarbalancetoken1 = pricetoken1usd * balance
                if eth_address == '0x8ac76a51cc950d9822d68b83fe1ad97b32cd580d':
                    pricetoken1usd = 1.032319
                all_token_information[token_number - 1] = all_token_information[token_number - 1][:13] + (pricetoken1usd, dollarbalancetoken1)
    else:
        pricetoken1usd = 0
        dollarbalancetoken1 = 0
        all_token_information[token_number - 1] = all_token_information[token_number - 1][:13] + (pricetoken1usd, dollarbalancetoken1)"
boofuzz,https://github.com/jtpereyda/boofuzz/tree/master/boofuzz/sessions.py,Session,_mutations_contain_duplicate$1536,"for (name1, name2) in itertools.combinations(names, r=2):
    if name1 in name2 or name2 in name1:
        return True","for e_target in itertools.combinations(names, r=2):
    name2 = e_target[1]
    name1 = e_target[0]
    if name1 in name2 or name2 in name1:
        return True"
salt,https://github.com/saltstack/salt/tree/master/salt/modules/yumpkg.py,,_yum_pkginfo$205,"for (key, value) in zip(keys, values):
    if key == 'name':
        try:
            (cur['name'], cur['arch']) = value.rsplit('.', 1)
        except ValueError:
            cur['name'] = value
            cur['arch'] = osarch
        cur['name'] = salt.utils.pkg.rpm.resolve_name(cur['name'], cur['arch'], osarch)
    else:
        if key == 'version':
            value = value.rstrip('-')
        elif key == 'repoid':
            value = value.lstrip('@')
        cur[key] = value
        if key == 'repoid':
            pkginfo = salt.utils.pkg.rpm.pkginfo(**cur)
            cur = {}
            if pkginfo is not None:
                yield pkginfo","for e_target in zip(keys, values):
    value = e_target[1]
    key = e_target[0]
    if key == 'name':
        try:
            (cur['name'], cur['arch']) = value.rsplit('.', 1)
        except ValueError:
            cur['name'] = value
            cur['arch'] = osarch
        cur['name'] = salt.utils.pkg.rpm.resolve_name(cur['name'], cur['arch'], osarch)
    else:
        if key == 'version':
            value = value.rstrip('-')
        elif key == 'repoid':
            value = value.lstrip('@')
        cur[key] = value
        if key == 'repoid':
            pkginfo = salt.utils.pkg.rpm.pkginfo(**cur)
            cur = {}
            if pkginfo is not None:
                yield pkginfo"
3D-ResNets-PyTorch,https://github.com/kenshohara/3D-ResNets-PyTorch/tree/master//validation.py,,val_epoch$11,"for (i, (inputs, targets)) in enumerate(data_loader):
    data_time.update(time.time() - end_time)
    targets = targets.to(device, non_blocking=True)
    outputs = model(inputs)
    loss = criterion(outputs, targets)
    acc = calculate_accuracy(outputs, targets)
    losses.update(loss.item(), inputs.size(0))
    accuracies.update(acc, inputs.size(0))
    batch_time.update(time.time() - end_time)
    end_time = time.time()
    print('Epoch: [{0}][{1}/{2}]\tTime {batch_time.val:.3f} ({batch_time.avg:.3f})\tData {data_time.val:.3f} ({data_time.avg:.3f})\tLoss {loss.val:.4f} ({loss.avg:.4f})\tAcc {acc.val:.3f} ({acc.avg:.3f})'.format(epoch, i + 1, len(data_loader), batch_time=batch_time, data_time=data_time, loss=losses, acc=accuracies))","for e_target in enumerate(data_loader):
    targets = e_target[1][1]
    inputs = e_target[1][0]
    i = e_target[0]
    data_time.update(time.time() - end_time)
    targets = targets.to(device, non_blocking=True)
    outputs = model(inputs)
    loss = criterion(outputs, targets)
    acc = calculate_accuracy(outputs, targets)
    losses.update(loss.item(), inputs.size(0))
    accuracies.update(acc, inputs.size(0))
    batch_time.update(time.time() - end_time)
    end_time = time.time()
    print('Epoch: [{0}][{1}/{2}]\tTime {batch_time.val:.3f} ({batch_time.avg:.3f})\tData {data_time.val:.3f} ({data_time.avg:.3f})\tLoss {loss.val:.4f} ({loss.avg:.4f})\tAcc {acc.val:.3f} ({acc.avg:.3f})'.format(epoch, i + 1, len(data_loader), batch_time=batch_time, data_time=data_time, loss=losses, acc=accuracies))"
kivy,https://github.com/kivy/kivy/tree/master/kivy/uix/textinput.py,TextInput,cursor_index$615,"for (_, line, flag) in zip(range(min(cursor_row, len(lines))), lines, flags):
    index += len(line)
    if flag & FL_IS_LINEBREAK:
        index += 1","for e_target in zip(range(min(cursor_row, len(lines))), lines, flags):
    flag = e_target[2]
    line = e_target[1]
    _ = e_target[0]
    index += len(line)
    if flag & FL_IS_LINEBREAK:
        index += 1"
checkmk,https://github.com/tribe29/checkmk/tree/master/cmk/gui/valuespec.py,CascadingDropdown,validate_datatype$3252,"for (nr, (val, _title, vs)) in enumerate(choices):
    if value == val or (isinstance(value, tuple) and value[0] == val):
        if vs:
            if not isinstance(value, tuple) or len(value) != 2:
                raise MKUserError(varprefix + '_sel', _('Value must be a tuple with two elements.'))
            vs.validate_datatype(value[1], varprefix + '_%d' % nr)
        return","for e_target in enumerate(choices):
    vs = e_target[1][2]
    _title = e_target[1][1]
    val = e_target[1][0]
    nr = e_target[0]
    if value == val or (isinstance(value, tuple) and value[0] == val):
        if vs:
            if not isinstance(value, tuple) or len(value) != 2:
                raise MKUserError(varprefix + '_sel', _('Value must be a tuple with two elements.'))
            vs.validate_datatype(value[1], varprefix + '_%d' % nr)
        return"
open_model_zoo,https://github.com/openvinotoolkit/open_model_zoo/tree/master/tools/accuracy_checker/openvino/tools/accuracy_checker/metrics/coco_metrics.py,MSCOCOAveragePrecision,prepare_summary$197,"for (i, ref_i) in enumerate(ref):
    j = np.where(fppi_tmp <= ref_i)[-1][-1]
    ref[i] = mr_tmp[j]","for e_target in enumerate(ref):
    ref_i = e_target[1]
    i = e_target[0]
    j = np.where(fppi_tmp <= ref_i)[-1][-1]
    ref[i] = mr_tmp[j]"
ansible-modules-core,https://github.com/ansible/ansible-modules-core/tree/master/network/nxos/nxos_pim.py,CustomNetworkConfig,add$136,"for (index, p) in enumerate(parents):
    try:
        i = index + 1
        obj = self.get_section_objects(parents[:i])[0]
        ancestors.append(obj)
    except ValueError:
        offset = index * self.indent
        obj = ConfigLine(p)
        obj.raw = p.rjust(len(p) + offset)
        if ancestors:
            obj.parents = list(ancestors)
            ancestors[-1].children.append(obj)
        self.items.append(obj)
        ancestors.append(obj)","for e_target in enumerate(parents):
    p = e_target[1]
    index = e_target[0]
    try:
        i = index + 1
        obj = self.get_section_objects(parents[:i])[0]
        ancestors.append(obj)
    except ValueError:
        offset = index * self.indent
        obj = ConfigLine(p)
        obj.raw = p.rjust(len(p) + offset)
        if ancestors:
            obj.parents = list(ancestors)
            ancestors[-1].children.append(obj)
        self.items.append(obj)
        ancestors.append(obj)"
deoplete-go,https://github.com/deoplete-plugins/deoplete-go/tree/master/benchmark/benchmark.py,,print_results$38,"for (suite_name, suite_results) in results.items():
    print(suite_name)
    print('-' * 20)
    for (module_name, result) in sorted(suite_results.items(), key=lambda x: x[1]):
        print('{:10} {:.5f} s'.format(module_name, result))
    print()","for e_target in results.items():
    suite_results = e_target[1]
    suite_name = e_target[0]
    print(suite_name)
    print('-' * 20)
    for (module_name, result) in sorted(suite_results.items(), key=lambda x: x[1]):
        print('{:10} {:.5f} s'.format(module_name, result))
    print()"
PaddleDetection,https://github.com/PaddlePaddle/PaddleDetection/tree/master/ppdet/modeling/necks/yolo_fpn.py,PPYOLOPAN,__init__$791,"for (i, ch_in) in enumerate(self.in_channels[::-1]):
    if i > 0:
        ch_in += 512 // 2 ** (i - 1)
    channel = 512 // 2 ** i
    base_cfg = []
    for j in range(self.conv_block_num):
        base_cfg += [['{}.0'.format(j), ConvBNLayer, [channel, channel, 1], dict(padding=0, act=act, norm_type=norm_type)], ['{}.1'.format(j), ConvBNLayer, [channel, channel, 3], dict(padding=1, act=act, norm_type=norm_type)]]
    if i == 0 and self.spp:
        base_cfg[3] = ['spp', SPP, [channel * 4, channel, 1], dict(pool_size=[5, 9, 13], act=act, norm_type=norm_type)]
    cfg = base_cfg[:4] + dropblock_cfg + base_cfg[4:]
    name = 'fpn.{}'.format(i)
    fpn_block = self.add_sublayer(name, PPYOLODetBlockCSP(cfg, ch_in, channel, act, norm_type, name, data_format))
    self.fpn_blocks.append(fpn_block)
    fpn_channels.append(channel * 2)
    if i < self.num_blocks - 1:
        name = 'fpn_transition.{}'.format(i)
        route = self.add_sublayer(name, ConvBNLayer(ch_in=channel * 2, ch_out=channel, filter_size=1, stride=1, padding=0, act=act, norm_type=norm_type, data_format=data_format, name=name))
        self.fpn_routes.append(route)","for e_target in enumerate(self.in_channels[::-1]):
    ch_in = e_target[1]
    i = e_target[0]
    if i > 0:
        ch_in += 512 // 2 ** (i - 1)
    channel = 512 // 2 ** i
    base_cfg = []
    for j in range(self.conv_block_num):
        base_cfg += [['{}.0'.format(j), ConvBNLayer, [channel, channel, 1], dict(padding=0, act=act, norm_type=norm_type)], ['{}.1'.format(j), ConvBNLayer, [channel, channel, 3], dict(padding=1, act=act, norm_type=norm_type)]]
    if i == 0 and self.spp:
        base_cfg[3] = ['spp', SPP, [channel * 4, channel, 1], dict(pool_size=[5, 9, 13], act=act, norm_type=norm_type)]
    cfg = base_cfg[:4] + dropblock_cfg + base_cfg[4:]
    name = 'fpn.{}'.format(i)
    fpn_block = self.add_sublayer(name, PPYOLODetBlockCSP(cfg, ch_in, channel, act, norm_type, name, data_format))
    self.fpn_blocks.append(fpn_block)
    fpn_channels.append(channel * 2)
    if i < self.num_blocks - 1:
        name = 'fpn_transition.{}'.format(i)
        route = self.add_sublayer(name, ConvBNLayer(ch_in=channel * 2, ch_out=channel, filter_size=1, stride=1, padding=0, act=act, norm_type=norm_type, data_format=data_format, name=name))
        self.fpn_routes.append(route)"
checkmk,https://github.com/tribe29/checkmk/tree/master/cmk/base/plugins/agent_based/utils/interfaces.py,,discover_interfaces$514,"for (item, discovered_params, index, index_as_item, labels) in pre_inventory:
    if not index_as_item and n_times_item_seen[item] > 1:
        new_item = '%s %d' % (item, index)
    else:
        new_item = item
    yield Service(item=new_item, parameters=dict(discovered_params), labels=[ServiceLabel(key, value) for (key, value) in labels.items()] if labels else None)","for e_target in pre_inventory:
    labels = e_target[4]
    index_as_item = e_target[3]
    index = e_target[2]
    discovered_params = e_target[1]
    item = e_target[0]
    if not index_as_item and n_times_item_seen[item] > 1:
        new_item = '%s %d' % (item, index)
    else:
        new_item = item
    yield Service(item=new_item, parameters=dict(discovered_params), labels=[ServiceLabel(key, value) for (key, value) in labels.items()] if labels else None)"
nilearn,https://github.com/nilearn/nilearn/tree/master/nilearn/_utils/niimg_conversions.py,,_check_same_fov$33,"for ((a_name, a_img), (b_name, b_img)) in itertools.combinations(kwargs.items(), 2):
    if not a_img.shape[:3] == b_img.shape[:3]:
        errors.append((a_name, b_name, 'shape'))
    if not np.allclose(a_img.affine, b_img.affine):
        errors.append((a_name, b_name, 'affine'))","for e_target in itertools.combinations(kwargs.items(), 2):
    b_img = e_target[1][1]
    b_name = e_target[1][0]
    a_img = e_target[0][1]
    a_name = e_target[0][0]
    if not a_img.shape[:3] == b_img.shape[:3]:
        errors.append((a_name, b_name, 'shape'))
    if not np.allclose(a_img.affine, b_img.affine):
        errors.append((a_name, b_name, 'affine'))"
capa,https://github.com/mandiant/capa/tree/master/capa/main.py,,get_signatures$542,"for (root, dirs, files) in os.walk(sigs_path):
    for file in files:
        if file.endswith(('.pat', '.pat.gz', '.sig')):
            sig_path = os.path.join(root, file)
            paths.append(sig_path)","for e_target in os.walk(sigs_path):
    files = e_target[2]
    dirs = e_target[1]
    root = e_target[0]
    for file in files:
        if file.endswith(('.pat', '.pat.gz', '.sig')):
            sig_path = os.path.join(root, file)
            paths.append(sig_path)"
nova,https://github.com/openstack/nova/tree/master/nova/virt/libvirt/config.py,LibvirtConfigCapsGuestDomain,format_dom$657,"for (alias, machine) in self.aliases.items():
    domain.append(self._text_node('machine', alias, **machine))","for e_target in self.aliases.items():
    machine = e_target[1]
    alias = e_target[0]
    domain.append(self._text_node('machine', alias, **machine))"
graph_nets,https://github.com/deepmind/graph_nets/tree/master/graph_nets/utils_np.py,,_populate_number_fields$475,"for (number_field, data_field) in [[N_NODE, NODES], [N_EDGE, RECEIVERS]]:
    if dct.get(number_field) is None:
        if dct[data_field] is not None:
            dct[number_field] = np.array(np.shape(dct[data_field])[0], dtype=np.int32)
        else:
            dct[number_field] = np.array(0, dtype=np.int32)","for e_target in [[N_NODE, NODES], [N_EDGE, RECEIVERS]]:
    data_field = e_target[1]
    number_field = e_target[0]
    if dct.get(number_field) is None:
        if dct[data_field] is not None:
            dct[number_field] = np.array(np.shape(dct[data_field])[0], dtype=np.int32)
        else:
            dct[number_field] = np.array(0, dtype=np.int32)"
calamari,https://github.com/Calamari-OCR/calamari/tree/master/calamari_ocr/ocr/dataset/datareader/pagexml/reader.py,PageXMLReader,__init__$247,"for (img, xml) in zip(params.images, params.xml_files):
    loader = PageXMLDatasetLoader(self.mode, params.non_existing_as_empty, params.text_index, params.skip_invalid, params.skip_commented)
    for sample in loader.load(img, xml):
        self.add_sample(sample)
    self.pages[split_all_ext(xml)[0]] = loader.root","for e_target in zip(params.images, params.xml_files):
    xml = e_target[1]
    img = e_target[0]
    loader = PageXMLDatasetLoader(self.mode, params.non_existing_as_empty, params.text_index, params.skip_invalid, params.skip_commented)
    for sample in loader.load(img, xml):
        self.add_sample(sample)
    self.pages[split_all_ext(xml)[0]] = loader.root"
petastorm,https://github.com/uber/petastorm/tree/master/petastorm/tf_utils.py,,_flatten$140,"for (index, key) in enumerate(sorted(data.keys())):
    data_dict = data[key]._asdict()
    for subkey in data_dict:
        encoded_key = subkey + '_' + str(index)
        flattened[encoded_key] = data_dict[subkey]","for e_target in enumerate(sorted(data.keys())):
    key = e_target[1]
    index = e_target[0]
    data_dict = data[key]._asdict()
    for subkey in data_dict:
        encoded_key = subkey + '_' + str(index)
        flattened[encoded_key] = data_dict[subkey]"
learnable-triangulation-pytorch,https://github.com/karfly/learnable-triangulation-pytorch/tree/master/mvn/utils/vis.py,,draw_3d_pose$355,"for (i, joint) in enumerate(connectivity):
    if keypoints_mask[joint[0]] and keypoints_mask[joint[1]]:
        (xs, ys, zs) = [np.array([keypoints[joint[0], j], keypoints[joint[1], j]]) for j in range(3)]
        if kind in COLOR_DICT:
            color = COLOR_DICT[kind][i]
        else:
            color = (0, 0, 255)
        color = np.array(color) / 255
        ax.plot(xs, ys, zs, lw=line_width, c=color)","for e_target in enumerate(connectivity):
    joint = e_target[1]
    i = e_target[0]
    if keypoints_mask[joint[0]] and keypoints_mask[joint[1]]:
        (xs, ys, zs) = [np.array([keypoints[joint[0], j], keypoints[joint[1], j]]) for j in range(3)]
        if kind in COLOR_DICT:
            color = COLOR_DICT[kind][i]
        else:
            color = (0, 0, 255)
        color = np.array(color) / 255
        ax.plot(xs, ys, zs, lw=line_width, c=color)"
wagtail,https://github.com/wagtail/wagtail/tree/master/wagtail/admin/tests/pages/test_edit_page.py,TestPageEdit,test_override_default_action_menu_item$970,"for (index, item) in enumerate(menu_items):
    if item.name == 'action-publish':
        menu_items.pop(index)
        menu_items.insert(0, item)
        break","for e_target in enumerate(menu_items):
    item = e_target[1]
    index = e_target[0]
    if item.name == 'action-publish':
        menu_items.pop(index)
        menu_items.insert(0, item)
        break"
dcc,https://github.com/amimo/dcc/tree/master/androguard/decompiler/dad/dataflow.py,,split_variables$359,"for (var, versions) in variables.items():
    nversions = len(versions)
    if nversions == 1:
        continue
    orig_var = lvars.pop(var)
    for (i, (defs, uses)) in enumerate(versions):
        if min(defs) < 0:
            if orig_var.this:
                new_version = ThisParam(var, orig_var.type)
            else:
                new_version = Param(var, orig_var.type)
            lvars[var] = new_version
        else:
            new_version = Variable(nb_vars)
            new_version.type = orig_var.type
            lvars[nb_vars] = new_version
            nb_vars += 1
        new_version.name = '%d_%d' % (var, i)
        for loc in defs:
            if loc < 0:
                continue
            ins = graph.get_ins_from_loc(loc)
            ins.replace_lhs(new_version)
            DU[new_version.value(), loc] = DU.pop((var, loc))
        for loc in uses:
            ins = graph.get_ins_from_loc(loc)
            ins.replace_var(var, new_version)
            UD[new_version.value(), loc] = UD.pop((var, loc))","for e_target in variables.items():
    versions = e_target[1]
    var = e_target[0]
    nversions = len(versions)
    if nversions == 1:
        continue
    orig_var = lvars.pop(var)
    for (i, (defs, uses)) in enumerate(versions):
        if min(defs) < 0:
            if orig_var.this:
                new_version = ThisParam(var, orig_var.type)
            else:
                new_version = Param(var, orig_var.type)
            lvars[var] = new_version
        else:
            new_version = Variable(nb_vars)
            new_version.type = orig_var.type
            lvars[nb_vars] = new_version
            nb_vars += 1
        new_version.name = '%d_%d' % (var, i)
        for loc in defs:
            if loc < 0:
                continue
            ins = graph.get_ins_from_loc(loc)
            ins.replace_lhs(new_version)
            DU[new_version.value(), loc] = DU.pop((var, loc))
        for loc in uses:
            ins = graph.get_ins_from_loc(loc)
            ins.replace_var(var, new_version)
            UD[new_version.value(), loc] = UD.pop((var, loc))"
orator,https://github.com/sdispater/orator/tree/master/orator/dbal/column.py,Column,set_options$25,"for (key, value) in options.items():
    method = 'set_%s' % key
    if hasattr(self, method):
        getattr(self, method)(value)","for e_target in options.items():
    value = e_target[1]
    key = e_target[0]
    method = 'set_%s' % key
    if hasattr(self, method):
        getattr(self, method)(value)"
vecstack,https://github.com/vecxoz/vecstack/tree/master/vecstack/coresk.py,StackingTransformer,transform$603,"for (estimator_counter, (name, estimator)) in enumerate(self.estimators_):
    if self.verbose > 0:
        estimator_str = 'estimator %2d: [%s: %s]' % (estimator_counter, name, estimator.__class__.__name__)
        print(estimator_str)
    for (fold_counter, (tr_index, te_index)) in enumerate(self.kf_.split(X, self._y_)):
        X_te = X[te_index]
        if 'predict_proba' == self.action_:
            col_slice_estimator = slice(estimator_counter * self.n_classes_implicit_, estimator_counter * self.n_classes_implicit_ + self.n_classes_implicit_)
        else:
            col_slice_estimator = estimator_counter
        S_train[te_index, col_slice_estimator] = self._estimator_action(self.models_A_[estimator_counter][fold_counter], None, None, X_te, action=self.action_, transform=self.transform_pred)
        if self.verbose > 1:
            fold_str = '    model from fold %2d: done' % fold_counter
            print(fold_str)
    if self.verbose > 1:
        sep_str = '    ----'
        print(sep_str)
    if self.verbose > 0:
        done_str = '    DONE\n'
        print(done_str)","for e_target in enumerate(self.estimators_):
    estimator = e_target[1][1]
    name = e_target[1][0]
    estimator_counter = e_target[0]
    if self.verbose > 0:
        estimator_str = 'estimator %2d: [%s: %s]' % (estimator_counter, name, estimator.__class__.__name__)
        print(estimator_str)
    for (fold_counter, (tr_index, te_index)) in enumerate(self.kf_.split(X, self._y_)):
        X_te = X[te_index]
        if 'predict_proba' == self.action_:
            col_slice_estimator = slice(estimator_counter * self.n_classes_implicit_, estimator_counter * self.n_classes_implicit_ + self.n_classes_implicit_)
        else:
            col_slice_estimator = estimator_counter
        S_train[te_index, col_slice_estimator] = self._estimator_action(self.models_A_[estimator_counter][fold_counter], None, None, X_te, action=self.action_, transform=self.transform_pred)
        if self.verbose > 1:
            fold_str = '    model from fold %2d: done' % fold_counter
            print(fold_str)
    if self.verbose > 1:
        sep_str = '    ----'
        print(sep_str)
    if self.verbose > 0:
        done_str = '    DONE\n'
        print(done_str)"
neupy,https://github.com/itdxer/neupy/tree/master/neupy/core/docs.py,,iter_doc_parameters$41,"for (name, type_, desc, _, _, _) in parser.findall(docs):
    if type_ or name.startswith('*'):
        parameter_name = name.lstrip('*')
        parameter_description = ''.join([name, type_, desc])
        yield (parameter_name, parameter_description.rstrip())","for e_target in parser.findall(docs):
    _ = e_target[5]
    desc = e_target[2]
    type_ = e_target[1]
    name = e_target[0]
    if type_ or name.startswith('*'):
        parameter_name = name.lstrip('*')
        parameter_description = ''.join([name, type_, desc])
        yield (parameter_name, parameter_description.rstrip())"
pysnmp,https://github.com/etingof/pysnmp/tree/master/pysnmp/entity/rfc3413/cmdrsp.py,NextCommandResponder,_getManagedObjectsInstances$339,"for (idx, varBind) in enumerate(varBinds):
    (name, val) = varBind
    if exval.noSuchObject.isSameTypeWith(val) or exval.noSuchInstance.isSameTypeWith(val):
        varBindsMap[len(rtrVarBinds)] = varBindsMap.pop(idx, idx)
        rtrVarBinds.append(varBind)
    else:
        rspVarBinds[varBindsMap.pop(idx, idx)] = varBind","for e_target in enumerate(varBinds):
    varBind = e_target[1]
    idx = e_target[0]
    (name, val) = varBind
    if exval.noSuchObject.isSameTypeWith(val) or exval.noSuchInstance.isSameTypeWith(val):
        varBindsMap[len(rtrVarBinds)] = varBindsMap.pop(idx, idx)
        rtrVarBinds.append(varBind)
    else:
        rspVarBinds[varBindsMap.pop(idx, idx)] = varBind"
modin,https://github.com/modin-project/modin/tree/master/modin/pandas/test/test_groupby.py,,test_multi_column_groupby$976,"for (k, _) in modin_df.groupby(by):
    assert isinstance(k, tuple)","for e_target in modin_df.groupby(by):
    _ = e_target[1]
    k = e_target[0]
    assert isinstance(k, tuple)"
FACT_core,https://github.com/fkie-cad/FACT_core/tree/master/src/plugins/analysis/cve_lookup/test/test_setup_repository.py,,test_get_cve_import_content$393,"for (item, expected) in zip(feeds, EXPECTED_GET_CVE_FEEDS_UPDATE_CONTENT):
    assert item.cve_id == expected.cve_id
    assert item.impact == expected.impact
    assert sorted(item.cpe_list) == sorted(expected.cpe_list)","for e_target in zip(feeds, EXPECTED_GET_CVE_FEEDS_UPDATE_CONTENT):
    expected = e_target[1]
    item = e_target[0]
    assert item.cve_id == expected.cve_id
    assert item.impact == expected.impact
    assert sorted(item.cpe_list) == sorted(expected.cpe_list)"
2019-CCF-BDCI-OCR-MCZJ-OCR-IdentificationIDElement,https://github.com/Mingtzge/2019-CCF-BDCI-OCR-MCZJ-OCR-IdentificationIDElement/tree/master/pytorch-CycleGAN-and-pix2pix/test.py,,if_main_my$37,"for (i, data) in enumerate(dataset):
    if i >= opt.num_test:
        break
    model.set_input(data)
    model.test()
    visuals = model.get_current_visuals()
    img_path = model.get_image_paths()
    if i % 5 == 0:
        print('processing (%04d)-th image... %s' % (i, img_path))
    save_images(webpage, visuals, img_path, aspect_ratio=opt.aspect_ratio, width=opt.display_winsize)","for e_target in enumerate(dataset):
    data = e_target[1]
    i = e_target[0]
    if i >= opt.num_test:
        break
    model.set_input(data)
    model.test()
    visuals = model.get_current_visuals()
    img_path = model.get_image_paths()
    if i % 5 == 0:
        print('processing (%04d)-th image... %s' % (i, img_path))
    save_images(webpage, visuals, img_path, aspect_ratio=opt.aspect_ratio, width=opt.display_winsize)"
checkmk,https://github.com/tribe29/checkmk/tree/master/cmk/gui/plugins/views/mobile.py,,render_mobile_list$778,"for (rendered_cell, cell) in zip(rendered_cells[num_columns + 1:], cells[num_columns + 1:]):
    (rendered_class, rendered_content) = rendered_cell
    html.open_p(class_='ui-li-desc')
    cell.paint_as_header()
    html.write_text(': ')
    html.span(rendered_content, class_=rendered_class)
    html.close_p()","for e_target in zip(rendered_cells[num_columns + 1:], cells[num_columns + 1:]):
    cell = e_target[1]
    rendered_cell = e_target[0]
    (rendered_class, rendered_content) = rendered_cell
    html.open_p(class_='ui-li-desc')
    cell.paint_as_header()
    html.write_text(': ')
    html.span(rendered_content, class_=rendered_class)
    html.close_p()"
angr,https://github.com/angr/angr/tree/master/angr/analyses/disassembly.py,Disassembly,render$967,"for (i, line) in enumerate(buf):
    buf[i] = ' ' * (max_edge_depth - len(ref_buf[i])) + edge_buf[i] + line","for e_target in enumerate(buf):
    line = e_target[1]
    i = e_target[0]
    buf[i] = ' ' * (max_edge_depth - len(ref_buf[i])) + edge_buf[i] + line"
convmixer,https://github.com/tmp-iclr/convmixer/tree/master/pytorch-image-models/timm/models/convit.py,ConViT,__init__$218,"for (n, m) in self.named_modules():
    if hasattr(m, 'local_init'):
        m.local_init()","for e_target in self.named_modules():
    m = e_target[1]
    n = e_target[0]
    if hasattr(m, 'local_init'):
        m.local_init()"
jiant,https://github.com/nyu-mll/jiant/tree/master/jiant/tasks/lib/ropes.py,Example,to_feature_list$24,"for (i, token) in enumerate(self.doc_tokens):
    orig_to_tok_index.append(len(all_doc_tokens))
    if tokenizer.__class__.__name__ in ['RobertaTokenizer', 'LongformerTokenizer', 'BartTokenizer', 'RobertaTokenizerFast', 'LongformerTokenizerFast', 'BartTokenizerFast']:
        sub_tokens = tokenizer.tokenize(token, add_prefix_space=True)
    else:
        sub_tokens = tokenizer.tokenize(token)
    for sub_token in sub_tokens:
        tok_to_orig_index.append(i)
        all_doc_tokens.append(sub_token)","for e_target in enumerate(self.doc_tokens):
    token = e_target[1]
    i = e_target[0]
    orig_to_tok_index.append(len(all_doc_tokens))
    if tokenizer.__class__.__name__ in ['RobertaTokenizer', 'LongformerTokenizer', 'BartTokenizer', 'RobertaTokenizerFast', 'LongformerTokenizerFast', 'BartTokenizerFast']:
        sub_tokens = tokenizer.tokenize(token, add_prefix_space=True)
    else:
        sub_tokens = tokenizer.tokenize(token)
    for sub_token in sub_tokens:
        tok_to_orig_index.append(i)
        all_doc_tokens.append(sub_token)"
airflow,https://github.com/apache/airflow/tree/master/airflow/configuration.py,AirflowConfigParser,_include_secrets$637,"for (section, key) in self.sensitive_config_values:
    opt = self._get_secret_option(section, key)
    if opt:
        if not display_sensitive:
            opt = '< hidden >'
        if display_source:
            opt = (opt, 'secret')
        elif raw:
            opt = opt.replace('%', '%%')
        config_sources.setdefault(section, OrderedDict()).update({key: opt})
        del config_sources[section][key + '_secret']","for e_target in self.sensitive_config_values:
    key = e_target[1]
    section = e_target[0]
    opt = self._get_secret_option(section, key)
    if opt:
        if not display_sensitive:
            opt = '< hidden >'
        if display_source:
            opt = (opt, 'secret')
        elif raw:
            opt = opt.replace('%', '%%')
        config_sources.setdefault(section, OrderedDict()).update({key: opt})
        del config_sources[section][key + '_secret']"
CrypTen,https://github.com/facebookresearch/CrypTen/tree/master/test/test_nn.py,TestNN,test_linear$693,"for (dim, size) in zip(dims, sizes):
    input = get_random_test_tensor(size=size, is_float=True)
    input.requires_grad = True
    encr_input = crypten.cryptensor(input)
    encr_input.requires_grad = compute_gradients
    module = torch.nn.Linear(*dim)
    module.train()
    encr_module = crypten.nn.Linear(*dim)
    for (n, p) in module.named_parameters():
        p = comm.get().broadcast(p, 0)
        encr_module.set_parameter(n, p)
    encr_module.encrypt().train()
    self.assertTrue(encr_module.training, 'training value incorrect')
    reference = module(input)
    encr_output = encr_module(encr_input)
    self._check(encr_output, reference, 'Linear forward failed')
    reference.backward(torch.ones(reference.size()))
    encr_output.backward(encr_output.new(torch.ones(encr_output.size())))
    if compute_gradients:
        self._check(encr_input.grad, input.grad, 'Linear backward on input failed')
    else:
        self.assertIsNone(encr_input.grad)
    for (name, param) in module.named_parameters():
        encr_param = getattr(encr_module, name)
        self._check(encr_param.grad, param.grad, 'Linear backward on %s failed' % name)","for e_target in zip(dims, sizes):
    size = e_target[1]
    dim = e_target[0]
    input = get_random_test_tensor(size=size, is_float=True)
    input.requires_grad = True
    encr_input = crypten.cryptensor(input)
    encr_input.requires_grad = compute_gradients
    module = torch.nn.Linear(*dim)
    module.train()
    encr_module = crypten.nn.Linear(*dim)
    for (n, p) in module.named_parameters():
        p = comm.get().broadcast(p, 0)
        encr_module.set_parameter(n, p)
    encr_module.encrypt().train()
    self.assertTrue(encr_module.training, 'training value incorrect')
    reference = module(input)
    encr_output = encr_module(encr_input)
    self._check(encr_output, reference, 'Linear forward failed')
    reference.backward(torch.ones(reference.size()))
    encr_output.backward(encr_output.new(torch.ones(encr_output.size())))
    if compute_gradients:
        self._check(encr_input.grad, input.grad, 'Linear backward on input failed')
    else:
        self.assertIsNone(encr_input.grad)
    for (name, param) in module.named_parameters():
        encr_param = getattr(encr_module, name)
        self._check(encr_param.grad, param.grad, 'Linear backward on %s failed' % name)"
hydrus,https://github.com/hydrusnetwork/hydrus/tree/master/hydrus/client/media/ClientMedia.py,MediaList,ProcessServiceUpdates$1656,"for (service_key, service_updates) in list(service_keys_to_service_updates.items()):
    for service_update in service_updates:
        (action, row) = service_update.ToTuple()
        if action == HC.SERVICE_UPDATE_DELETE_PENDING:
            self.DeletePending(service_key)
        elif action == HC.SERVICE_UPDATE_RESET:
            self.ResetService(service_key)","for e_target in list(service_keys_to_service_updates.items()):
    service_updates = e_target[1]
    service_key = e_target[0]
    for service_update in service_updates:
        (action, row) = service_update.ToTuple()
        if action == HC.SERVICE_UPDATE_DELETE_PENDING:
            self.DeletePending(service_key)
        elif action == HC.SERVICE_UPDATE_RESET:
            self.ResetService(service_key)"
espnet,https://github.com/espnet/espnet/tree/master/test/espnet2/gan_tts/vits/test_generator.py,,test_vits_generator_forward$98,"for (j, output_) in enumerate(output):
    print(f'{i + j + 1}: {output_.shape}')","for e_target in enumerate(output):
    output_ = e_target[1]
    j = e_target[0]
    print(f'{i + j + 1}: {output_.shape}')"
cvpods,https://github.com/Megvii-BaseDetection/cvpods/tree/master/cvpods/modeling/proposal_generator/rrpn_outputs.py,RRPNOutputs,_get_ground_truth$195,"for (image_size_i, anchors_i, gt_boxes_i) in zip(self.image_sizes, anchors, self.gt_boxes):
    '\n            image_size_i: (h, w) for the i-th image\n            anchors_i: anchors for i-th image\n            gt_boxes_i: ground-truth boxes for i-th image\n            '
    match_quality_matrix = pairwise_iou_rotated(gt_boxes_i, anchors_i)
    (matched_idxs, gt_objectness_logits_i) = self.anchor_matcher(match_quality_matrix)
    if self.boundary_threshold >= 0:
        anchors_inside_image = anchors_i.inside_box(image_size_i, self.boundary_threshold)
        gt_objectness_logits_i[~anchors_inside_image] = -1
    if len(gt_boxes_i) == 0:
        gt_anchor_deltas_i = torch.zeros_like(anchors_i.tensor)
    else:
        matched_gt_boxes = gt_boxes_i[matched_idxs]
        gt_anchor_deltas_i = self.box2box_transform.get_deltas(anchors_i.tensor, matched_gt_boxes.tensor)
    gt_objectness_logits.append(gt_objectness_logits_i)
    gt_anchor_deltas.append(gt_anchor_deltas_i)","for e_target in zip(self.image_sizes, anchors, self.gt_boxes):
    gt_boxes_i = e_target[2]
    anchors_i = e_target[1]
    image_size_i = e_target[0]
    '\n            image_size_i: (h, w) for the i-th image\n            anchors_i: anchors for i-th image\n            gt_boxes_i: ground-truth boxes for i-th image\n            '
    match_quality_matrix = pairwise_iou_rotated(gt_boxes_i, anchors_i)
    (matched_idxs, gt_objectness_logits_i) = self.anchor_matcher(match_quality_matrix)
    if self.boundary_threshold >= 0:
        anchors_inside_image = anchors_i.inside_box(image_size_i, self.boundary_threshold)
        gt_objectness_logits_i[~anchors_inside_image] = -1
    if len(gt_boxes_i) == 0:
        gt_anchor_deltas_i = torch.zeros_like(anchors_i.tensor)
    else:
        matched_gt_boxes = gt_boxes_i[matched_idxs]
        gt_anchor_deltas_i = self.box2box_transform.get_deltas(anchors_i.tensor, matched_gt_boxes.tensor)
    gt_objectness_logits.append(gt_objectness_logits_i)
    gt_anchor_deltas.append(gt_anchor_deltas_i)"
checkmk,https://github.com/tribe29/checkmk/tree/master/cmk/gui/plugins/views/perfometers/check_mk.py,,perfometer_temperature_multi$257,"for (_sensor, value, _uom, warn, crit, _min, _max) in perf_data:
    value = int(value)
    if value > display_value:
        display_value = value
        if warn is not None and display_value > int(warn):
            display_color = '#FFC840'
        if crit is not None and display_value > int(crit):
            display_color = '#FF0000'","for e_target in perf_data:
    _max = e_target[6]
    _min = e_target[5]
    crit = e_target[4]
    warn = e_target[3]
    _uom = e_target[2]
    value = e_target[1]
    _sensor = e_target[0]
    value = int(value)
    if value > display_value:
        display_value = value
        if warn is not None and display_value > int(warn):
            display_color = '#FFC840'
        if crit is not None and display_value > int(crit):
            display_color = '#FF0000'"
CellProfiler,https://github.com/CellProfiler/CellProfiler/tree/master/tests/modules/test_straightenworms.py,,test_get_measurement_columns_horizontal$600,"for (ftr, image) in zip([cellprofiler.modules.straightenworms.FTR_MEAN_INTENSITY, cellprofiler.modules.straightenworms.FTR_STD_INTENSITY] * 2, [STRAIGHTENED_IMAGE_NAME] * 2 + [AUX_STRAIGHTENED_IMAGE_NAME] * 2):
    scales = module.get_measurement_scales(workspace.pipeline, OBJECTS_NAME, cellprofiler.modules.straightenworms.C_WORM, ftr, image)
    assert len(scales) == 5
    for expected_scale in range(5):
        assert module.get_scale_name(expected_scale, None) in scales","for e_target in zip([cellprofiler.modules.straightenworms.FTR_MEAN_INTENSITY, cellprofiler.modules.straightenworms.FTR_STD_INTENSITY] * 2, [STRAIGHTENED_IMAGE_NAME] * 2 + [AUX_STRAIGHTENED_IMAGE_NAME] * 2):
    image = e_target[1]
    ftr = e_target[0]
    scales = module.get_measurement_scales(workspace.pipeline, OBJECTS_NAME, cellprofiler.modules.straightenworms.C_WORM, ftr, image)
    assert len(scales) == 5
    for expected_scale in range(5):
        assert module.get_scale_name(expected_scale, None) in scales"
ludwig,https://github.com/ludwig-ai/ludwig/tree/master/ludwig/visualize.py,,validate_conf_treshholds_and_probabilities_2d_3d$50,"for (item, value) in validation_mapping.items():
    item_len = len(value)
    if item_len != 2:
        exception_message = 'Two {} should be provided - {} was given.'.format(item, item_len)
        logging.error(exception_message)
        raise RuntimeError(exception_message)","for e_target in validation_mapping.items():
    value = e_target[1]
    item = e_target[0]
    item_len = len(value)
    if item_len != 2:
        exception_message = 'Two {} should be provided - {} was given.'.format(item, item_len)
        logging.error(exception_message)
        raise RuntimeError(exception_message)"
arviz,https://github.com/arviz-devs/arviz/tree/master/arviz/plots/backends/bokeh/forestplot.py,PlotHandler,labels_and_ticks$275,"for (label, idx) in zip(sub_labels, sub_idxs):
    labels_to_idxs[label].append(idx)","for e_target in zip(sub_labels, sub_idxs):
    idx = e_target[1]
    label = e_target[0]
    labels_to_idxs[label].append(idx)"
pdf2image,https://github.com/Belval/pdf2image/tree/master/pdf2image/pdf2image.py,,_parse_jpegopt$407,"for (k, v) in jpegopt.items():
    if v is True:
        v = 'y'
    if v is False:
        v = 'n'
    parts.append('{}={}'.format(k, v))","for e_target in jpegopt.items():
    v = e_target[1]
    k = e_target[0]
    if v is True:
        v = 'y'
    if v is False:
        v = 'n'
    parts.append('{}={}'.format(k, v))"
scipy,https://github.com/scipy/scipy/tree/master/scipy/interpolate/_interpolate.py,NdPPoly,derivative$2179,"for (axis, n) in enumerate(nu):
    p._derivative_inplace(n, axis)","for e_target in enumerate(nu):
    n = e_target[1]
    axis = e_target[0]
    p._derivative_inplace(n, axis)"
django-oscar,https://github.com/django-oscar/django-oscar/tree/master/src/oscar/apps/catalogue/managers.py,AttributeFilter,fast_query$51,"for (code, (lookup, value)) in self.items():
    selected_values = self._select_value(typedict[code], lookup, value)
    if not selected_values:
        return queryset.none()
    qs = qs.filter(selected_values, attribute_values__attribute__code=code)","for e_target in self.items():
    value = e_target[1][1]
    lookup = e_target[1][0]
    code = e_target[0]
    selected_values = self._select_value(typedict[code], lookup, value)
    if not selected_values:
        return queryset.none()
    qs = qs.filter(selected_values, attribute_values__attribute__code=code)"
alexa-skills-kit-sdk-for-python,https://github.com/alexa/alexa-skills-kit-sdk-for-python/tree/master/ask-sdk-core/ask_sdk_core/api_client.py,DefaultApiClient,_convert_dict_to_list_tuples$135,"for (key, values) in six.iteritems(headers_dict):
    for value in values.split(','):
        value = value.strip()
        if value is not None and value != '':
            headers_list.append((key, value.strip()))","for e_target in six.iteritems(headers_dict):
    values = e_target[1]
    key = e_target[0]
    for value in values.split(','):
        value = value.strip()
        if value is not None and value != '':
            headers_list.append((key, value.strip()))"
pymorphy2,https://github.com/kmike/pymorphy2/tree/master/benchmarks/speed.py,,_run_nofreq$67,"for (word, cnt) in words:
    morph.parse(word)","for e_target in words:
    cnt = e_target[1]
    word = e_target[0]
    morph.parse(word)"
cmake_format,https://github.com/cheshirekow/cmake_format/tree/master/cmakelang/parse/printer.py,,dump_tree$12,"for (idx, node) in enumerate(nodes):
    outfile.write(indent)
    if idx + 1 == len(nodes):
        outfile.write('閳规柡鏀 ')
    else:
        outfile.write('閳规壕鏀 ')
    noderep = repr(node)
    if sys.version_info[0] < 3:
        outfile.write(getattr(noderep, 'decode')('utf-8'))
    else:
        outfile.write(noderep)
    outfile.write('\n')
    if not hasattr(node, 'children'):
        continue
    if idx + 1 == len(nodes):
        dump_tree(node.children, outfile, indent + '    ')
    else:
        dump_tree(node.children, outfile, indent + '閳   ')","for e_target in enumerate(nodes):
    node = e_target[1]
    idx = e_target[0]
    outfile.write(indent)
    if idx + 1 == len(nodes):
        outfile.write('閳规柡鏀 ')
    else:
        outfile.write('閳规壕鏀 ')
    noderep = repr(node)
    if sys.version_info[0] < 3:
        outfile.write(getattr(noderep, 'decode')('utf-8'))
    else:
        outfile.write(noderep)
    outfile.write('\n')
    if not hasattr(node, 'children'):
        continue
    if idx + 1 == len(nodes):
        dump_tree(node.children, outfile, indent + '    ')
    else:
        dump_tree(node.children, outfile, indent + '閳   ')"
fonttools,https://github.com/fonttools/fonttools/tree/master/Lib/fontTools/ttLib/tables/otTables.py,ClipList,preWrite$1333,"for (start, end) in ranges:
    assert (start, end) not in clipBoxRanges
    clipBoxRanges[start, end] = clipBox","for e_target in ranges:
    end = e_target[1]
    start = e_target[0]
    assert (start, end) not in clipBoxRanges
    clipBoxRanges[start, end] = clipBox"
serverless-application-model,https://github.com/aws/serverless-application-model/tree/master/tests/translator/test_translator.py,AbstractTestTranslator,_update_logical_id_hash$185,"for (logical_id_to_remove, tuple_to_add) in dict_of_things_to_delete.items():
    output_resources[tuple_to_add[0]] = tuple_to_add[1]
    del output_resources[logical_id_to_remove]","for e_target in dict_of_things_to_delete.items():
    tuple_to_add = e_target[1]
    logical_id_to_remove = e_target[0]
    output_resources[tuple_to_add[0]] = tuple_to_add[1]
    del output_resources[logical_id_to_remove]"
crossbar,https://github.com/crossbario/crossbar/tree/master/test/functests/_work/_test_cfc_master.py,,test_master_pair_node$72,"for (node_id, pubkey) in nodes.items():
    while True:
        try:
            result = (yield management_session.call('crossbarfabriccenter.mrealm.pair_node', pubkey, new_mrealm['name'], node_id, {}))
            print(hl(result, bold=True))
        except ApplicationError as e:
            if e.error != 'fabric.node-already-paired':
                raise
            else:
                result = (yield management_session.call('crossbarfabriccenter.mrealm.unpair_node_by_pubkey', pubkey))
                print(hl(result, bold=True))
        break","for e_target in nodes.items():
    pubkey = e_target[1]
    node_id = e_target[0]
    while True:
        try:
            result = (yield management_session.call('crossbarfabriccenter.mrealm.pair_node', pubkey, new_mrealm['name'], node_id, {}))
            print(hl(result, bold=True))
        except ApplicationError as e:
            if e.error != 'fabric.node-already-paired':
                raise
            else:
                result = (yield management_session.call('crossbarfabriccenter.mrealm.unpair_node_by_pubkey', pubkey))
                print(hl(result, bold=True))
        break"
colour,https://github.com/colour-science/colour/tree/master/colour/models/rgb/transfer_functions/tests/test_sony_slog.py,TestLogEncoding_SLog2,test_domain_range_scale_log_encoding_SLog2$246,"for (scale, factor) in d_r:
    with domain_range_scale(scale):
        np.testing.assert_almost_equal(log_encoding_SLog2(x * factor), y * factor, decimal=7)","for e_target in d_r:
    factor = e_target[1]
    scale = e_target[0]
    with domain_range_scale(scale):
        np.testing.assert_almost_equal(log_encoding_SLog2(x * factor), y * factor, decimal=7)"
yt-dlc,https://github.com/blackjack4494/yt-dlc/tree/master/youtube_dlc/extractor/bilibili.py,BiliBiliIE,_real_extract$123,"for (idx, durl) in enumerate(video_info['durl']):
    formats = [{'url': durl['url'], 'filesize': int_or_none(durl['size'])}]
    for backup_url in durl.get('backup_url', []):
        formats.append({'url': backup_url, 'preference': -2 if 'hd.mp4' in backup_url else -3})
    for a_format in formats:
        a_format.setdefault('http_headers', {}).update({'Referer': url})
    self._sort_formats(formats)
    entries.append({'id': '%s_part%s' % (video_id, idx), 'duration': float_or_none(durl.get('length'), 1000), 'formats': formats})","for e_target in enumerate(video_info['durl']):
    durl = e_target[1]
    idx = e_target[0]
    formats = [{'url': durl['url'], 'filesize': int_or_none(durl['size'])}]
    for backup_url in durl.get('backup_url', []):
        formats.append({'url': backup_url, 'preference': -2 if 'hd.mp4' in backup_url else -3})
    for a_format in formats:
        a_format.setdefault('http_headers', {}).update({'Referer': url})
    self._sort_formats(formats)
    entries.append({'id': '%s_part%s' % (video_id, idx), 'duration': float_or_none(durl.get('length'), 1000), 'formats': formats})"
eo-learn,https://github.com/sentinel-hub/eo-learn/tree/master/geometry/eolearn/geometry/transformations.py,RasterToVectorTask,_vectorize_single_raster$368,"for (geojson, value) in rasterio.features.shapes(raster[..., idx], mask=None if mask is None else mask[..., idx], transform=affine_transform, **self.rasterio_params):
    geo_list.append(shapely.geometry.shape(geojson))
    value_list.append(value)","for e_target in rasterio.features.shapes(raster[..., idx], mask=None if mask is None else mask[..., idx], transform=affine_transform, **self.rasterio_params):
    value = e_target[1]
    geojson = e_target[0]
    geo_list.append(shapely.geometry.shape(geojson))
    value_list.append(value)"
djongo,https://github.com/nesdis/djongo/tree/master/tests/django_tests/tests/v21/tests/model_meta/tests.py,DataTests,test_local_fields$69,"for (model, expected_result) in TEST_RESULTS['local_fields'].items():
    fields = model._meta.local_fields
    self.assertEqual([f.attname for f in fields], expected_result)
    for f in fields:
        self.assertEqual(f.model, model)
        self.assertTrue(is_data_field(f))","for e_target in TEST_RESULTS['local_fields'].items():
    expected_result = e_target[1]
    model = e_target[0]
    fields = model._meta.local_fields
    self.assertEqual([f.attname for f in fields], expected_result)
    for f in fields:
        self.assertEqual(f.model, model)
        self.assertTrue(is_data_field(f))"
ignite,https://github.com/pytorch/ignite/tree/master/ignite/handlers/param_scheduler.py,ParamGroupScheduler,load_state_dict$1222,"for (req_n, s, (n, sd)) in zip(self.names, self.schedulers, sds):
    if req_n != n:
        raise ValueError(f'Name of scheduler from input state dict does not correspond to required one, {n} vs {req_n}')
    s.load_state_dict(sd)","for e_target in zip(self.names, self.schedulers, sds):
    sd = e_target[2][1]
    n = e_target[2][0]
    s = e_target[1]
    req_n = e_target[0]
    if req_n != n:
        raise ValueError(f'Name of scheduler from input state dict does not correspond to required one, {n} vs {req_n}')
    s.load_state_dict(sd)"
sktime,https://github.com/alan-turing-institute/sktime/tree/master/sktime/transformations/panel/dictionary_based/_sfa.py,SFA,_binning_dft$460,"for (i, row) in enumerate(split):
    result[i] = self._discrete_fourier_transform(row, self.dft_length, self.norm, self.inverse_sqrt_win_size, self.lower_bounding) if self.use_fallback_dft else self._fast_fourier_transform(row)","for e_target in enumerate(split):
    row = e_target[1]
    i = e_target[0]
    result[i] = self._discrete_fourier_transform(row, self.dft_length, self.norm, self.inverse_sqrt_win_size, self.lower_bounding) if self.use_fallback_dft else self._fast_fourier_transform(row)"
pattern,https://github.com/clips/pattern/tree/master/pattern/graph/commonsense.py,,download$244,"for ((concept1, relation, concept2), (context, weight)) in r.items():
    s.append('""%s"",""%s"",""%s"",""%s"",%s' % (concept1, relation, concept2, context, weight))","for e_target in r.items():
    weight = e_target[1][1]
    context = e_target[1][0]
    concept2 = e_target[0][2]
    relation = e_target[0][1]
    concept1 = e_target[0][0]
    s.append('""%s"",""%s"",""%s"",""%s"",%s' % (concept1, relation, concept2, context, weight))"
lore,https://github.com/instacart/lore/tree/master/lore/estimators/keras.py,Base,__setstate__$118,"for (key, default) in backward_compatible_defaults.items():
    if key not in self.__dict__.keys():
        self.__dict__[key] = default","for e_target in backward_compatible_defaults.items():
    default = e_target[1]
    key = e_target[0]
    if key not in self.__dict__.keys():
        self.__dict__[key] = default"
imgaug,https://github.com/aleju/imgaug/tree/master/test/test_imgaug.py,Test_apply_lut_,test_HWC_image$1647,"for (subtable_descr, subtable) in tables_by_type:
    with self.subTest(nb_channels=nb_channels, table_type=subtable_descr):
        if subtable_descr == 'array-1d':
            tables_3d = np.stack([table_base] * nb_channels, axis=-1)
        else:
            tables_3d = np.stack(tables, axis=-1).reshape((256, -1))
        image = np.int32([[0, 50, 100, 245, 254, 255], [1, 51, 101, 246, 255, 0]])
        image = image[:, :, np.newaxis]
        image = np.tile(image, (1, 1, nb_channels))
        for c in np.arange(nb_channels):
            image[:, :, c] += c
        image = np.mod(image, 256).astype(np.uint8)
        image_orig = np.copy(image)
        image_aug = ia.apply_lut_(image, subtable)
        expected = np.zeros_like(image_orig)
        for c in np.arange(nb_channels):
            for x in np.arange(image.shape[1]):
                for y in np.arange(image.shape[0]):
                    v = image_orig[y, x, c]
                    v_proj = tables_3d[v, c]
                    expected[y, x, c] = v_proj
        assert np.array_equal(image_aug, expected)
        if nb_channels < 512:
            assert image_aug is image
        assert image_aug.shape == (2, 6, nb_channels)
        assert image_aug.dtype.name == 'uint8'","for e_target in tables_by_type:
    subtable = e_target[1]
    subtable_descr = e_target[0]
    with self.subTest(nb_channels=nb_channels, table_type=subtable_descr):
        if subtable_descr == 'array-1d':
            tables_3d = np.stack([table_base] * nb_channels, axis=-1)
        else:
            tables_3d = np.stack(tables, axis=-1).reshape((256, -1))
        image = np.int32([[0, 50, 100, 245, 254, 255], [1, 51, 101, 246, 255, 0]])
        image = image[:, :, np.newaxis]
        image = np.tile(image, (1, 1, nb_channels))
        for c in np.arange(nb_channels):
            image[:, :, c] += c
        image = np.mod(image, 256).astype(np.uint8)
        image_orig = np.copy(image)
        image_aug = ia.apply_lut_(image, subtable)
        expected = np.zeros_like(image_orig)
        for c in np.arange(nb_channels):
            for x in np.arange(image.shape[1]):
                for y in np.arange(image.shape[0]):
                    v = image_orig[y, x, c]
                    v_proj = tables_3d[v, c]
                    expected[y, x, c] = v_proj
        assert np.array_equal(image_aug, expected)
        if nb_channels < 512:
            assert image_aug is image
        assert image_aug.shape == (2, 6, nb_channels)
        assert image_aug.dtype.name == 'uint8'"
datadogpy,https://github.com/DataDog/datadogpy/tree/master/tests/util/system_info_observer.py,SysInfoObserver,stats$79,"for (key, val) in agg_stats.items():
    if not key.startswith('cpu'):
        agg_stats[key] = val / datapoints","for e_target in agg_stats.items():
    val = e_target[1]
    key = e_target[0]
    if not key.startswith('cpu'):
        agg_stats[key] = val / datapoints"
scipy,https://github.com/scipy/scipy/tree/master/scipy/linalg/tests/test_procrustes.py,,test_orthogonal_procrustes_array_conversion$62,"for (m, n) in ((6, 4), (4, 4), (4, 6)):
    A_arr = np.random.randn(m, n)
    B_arr = np.random.randn(m, n)
    As = (A_arr, A_arr.tolist(), matrix(A_arr))
    Bs = (B_arr, B_arr.tolist(), matrix(B_arr))
    (R_arr, s) = orthogonal_procrustes(A_arr, B_arr)
    AR_arr = A_arr.dot(R_arr)
    for (A, B) in product(As, Bs):
        (R, s) = orthogonal_procrustes(A, B)
        AR = A_arr.dot(R)
        assert_allclose(AR, AR_arr)","for e_target in ((6, 4), (4, 4), (4, 6)):
    n = e_target[1]
    m = e_target[0]
    A_arr = np.random.randn(m, n)
    B_arr = np.random.randn(m, n)
    As = (A_arr, A_arr.tolist(), matrix(A_arr))
    Bs = (B_arr, B_arr.tolist(), matrix(B_arr))
    (R_arr, s) = orthogonal_procrustes(A_arr, B_arr)
    AR_arr = A_arr.dot(R_arr)
    for (A, B) in product(As, Bs):
        (R, s) = orthogonal_procrustes(A, B)
        AR = A_arr.dot(R)
        assert_allclose(AR, AR_arr)"
migen,https://github.com/m-labs/migen/tree/master/migen/fhdl/decorators.py,ClockDomainsRenamer,transform_fragment$108,"for (old, new) in self.cd_remapping.items():
    rename_clock_domain(f, old, new)","for e_target in self.cd_remapping.items():
    new = e_target[1]
    old = e_target[0]
    rename_clock_domain(f, old, new)"
pyro,https://github.com/pyro-ppl/pyro/tree/master/pyro/infer/inspect.py,,get_dependencies$54,"for (i, downstream) in enumerate(sample_sites):
    upstreams = [u for u in sample_sites[:i] if not u['is_observed'] if u['value'].numel()]
    if not upstreams:
        continue
    log_prob = downstream['fn'].log_prob(downstream['value'])
    provenance = get_provenance(log_prob)
    for upstream in upstreams:
        u = upstream['name']
        if u in provenance:
            d = downstream['name']
            prior_dependencies[d][u] = set()","for e_target in enumerate(sample_sites):
    downstream = e_target[1]
    i = e_target[0]
    upstreams = [u for u in sample_sites[:i] if not u['is_observed'] if u['value'].numel()]
    if not upstreams:
        continue
    log_prob = downstream['fn'].log_prob(downstream['value'])
    provenance = get_provenance(log_prob)
    for upstream in upstreams:
        u = upstream['name']
        if u in provenance:
            d = downstream['name']
            prior_dependencies[d][u] = set()"
TextAttack,https://github.com/QData/TextAttack/tree/master/textattack/trainer.py,Trainer,get_eval_dataloader$418,"for (_input, label) in data:
    if isinstance(_input, collections.OrderedDict):
        _input = tuple(_input.values())
    else:
        _input = tuple(_input)
    if len(_input) == 1:
        _input = _input[0]
    input_texts.append(_input)
    targets.append(label)","for e_target in data:
    label = e_target[1]
    _input = e_target[0]
    if isinstance(_input, collections.OrderedDict):
        _input = tuple(_input.values())
    else:
        _input = tuple(_input)
    if len(_input) == 1:
        _input = _input[0]
    input_texts.append(_input)
    targets.append(label)"
backtrader,https://github.com/mementum/backtrader/tree/master/backtrader/metabase.py,MetaParams,donew$243,"for (p, frompackage) in cls.frompackages:
    if isinstance(frompackage, string_types):
        frompackage = (frompackage,)
    for fp in frompackage:
        if isinstance(fp, (tuple, list)):
            (fp, falias) = fp
        else:
            (fp, falias) = (fp, fp)
        pmod = __import__(p, fromlist=[str(fp)])
        pattr = getattr(pmod, fp)
        setattr(clsmod, falias, pattr)
        for basecls in cls.__bases__:
            setattr(sys.modules[basecls.__module__], falias, pattr)","for e_target in cls.frompackages:
    frompackage = e_target[1]
    p = e_target[0]
    if isinstance(frompackage, string_types):
        frompackage = (frompackage,)
    for fp in frompackage:
        if isinstance(fp, (tuple, list)):
            (fp, falias) = fp
        else:
            (fp, falias) = (fp, fp)
        pmod = __import__(p, fromlist=[str(fp)])
        pattr = getattr(pmod, fp)
        setattr(clsmod, falias, pattr)
        for basecls in cls.__bases__:
            setattr(sys.modules[basecls.__module__], falias, pattr)"
maia-chess,https://github.com/CSSLab/maia-chess/tree/master/blunder_prediction/maia_chess_backend/maia/lc0_az_policy_map.py,,if_main_my$116,"for (e, i) in enumerate(az_to_lc0):
    if e % line_length == 0 and e > 0:
        f.write('\n')
    f.write(str(i).rjust(5))
    if e != len(az_to_lc0) - 1:
        f.write(',')","for e_target in enumerate(az_to_lc0):
    i = e_target[1]
    e = e_target[0]
    if e % line_length == 0 and e > 0:
        f.write('\n')
    f.write(str(i).rjust(5))
    if e != len(az_to_lc0) - 1:
        f.write(',')"
rdflib,https://github.com/RDFLib/rdflib/tree/master/rdflib/tools/csv2rdf.py,CSV2RDF,convert$325,"for (i, x) in enumerate(l_):
    x = x.strip()
    if x != '':
        if self.COLUMNS.get(i, self.DEFAULT) == 'ignore':
            continue
        try:
            o = self.COLUMNS.get(i, rdflib.Literal)(x)
            if isinstance(o, list):
                for _o in o:
                    self.triple(uri, headers[i], _o)
            else:
                self.triple(uri, headers[i], o)
        except Exception as e:
            warnings.warn('Could not process value for column ' + '%d:%s in row %d, ignoring: %s ' % (i, headers[i], rows, e.message))","for e_target in enumerate(l_):
    x = e_target[1]
    i = e_target[0]
    x = x.strip()
    if x != '':
        if self.COLUMNS.get(i, self.DEFAULT) == 'ignore':
            continue
        try:
            o = self.COLUMNS.get(i, rdflib.Literal)(x)
            if isinstance(o, list):
                for _o in o:
                    self.triple(uri, headers[i], _o)
            else:
                self.triple(uri, headers[i], o)
        except Exception as e:
            warnings.warn('Could not process value for column ' + '%d:%s in row %d, ignoring: %s ' % (i, headers[i], rows, e.message))"
lbry-sdk,https://github.com/lbryio/lbry-sdk/tree/master/lbry/extras/daemon/daemon.py,Daemon,jsonrpc_account_set$1719,"for (chain_name, changes) in address_changes.items():
    chain = getattr(account, chain_name)
    for (attr, value) in changes.items():
        if value is not None:
            setattr(chain, attr, value)
            change_made = True","for e_target in address_changes.items():
    changes = e_target[1]
    chain_name = e_target[0]
    chain = getattr(account, chain_name)
    for (attr, value) in changes.items():
        if value is not None:
            setattr(chain, attr, value)
            change_made = True"
yt-dlc,https://github.com/blackjack4494/yt-dlc/tree/master/youtube_dlc/extractor/plays.py,PlaysTVIE,_real_extract$26,"for (format_id, height, format_url) in re.findall('<source\\s+res=""((\\d+)h?)""\\s+src=""([^""]+)""', sources):
    formats.append({'url': self._proto_relative_url(format_url), 'format_id': 'http-' + format_id, 'height': int_or_none(height)})","for e_target in re.findall('<source\\s+res=""((\\d+)h?)""\\s+src=""([^""]+)""', sources):
    format_url = e_target[2]
    height = e_target[1]
    format_id = e_target[0]
    formats.append({'url': self._proto_relative_url(format_url), 'format_id': 'http-' + format_id, 'height': int_or_none(height)})"
drf-nested-routers,https://github.com/alanjds/drf-nested-routers/tree/master/rest_framework_nested/viewsets.py,NestedViewSetMixin,get_queryset$42,"for (query_param, field_name) in parent_lookup_kwargs.items():
    orm_filters[field_name] = self.kwargs[query_param]","for e_target in parent_lookup_kwargs.items():
    field_name = e_target[1]
    query_param = e_target[0]
    orm_filters[field_name] = self.kwargs[query_param]"
DeepPavlov,https://github.com/deepmipt/DeepPavlov/tree/master/deeppavlov/models/kbqa/utils.py,,fill_query$80,"for (n, entity_type) in enumerate(type_comb[:-1]):
    query = query.replace(f't{n + 1}', entity_type)","for e_target in enumerate(type_comb[:-1]):
    entity_type = e_target[1]
    n = e_target[0]
    query = query.replace(f't{n + 1}', entity_type)"
gmplot,https://github.com/gmplot/gmplot/tree/master/gmplot/google_map_plotter.py,GoogleMapPlotter,scatter$401,"for (i, location) in enumerate(zip(lats, lngs)):
    point_options = {option: value[i] for (option, value) in options.items()}
    if point_options.get('marker'):
        self._markers.append(_Marker(location[0], location[1], point_options.get('face_color'), point_options.get('precision'), title=point_options.get('title'), label=point_options.get('label'), info_window=point_options.get('info_window'), draggable=point_options.get('draggable')))
    else:
        self._drawables.append(_Symbol(location[0], location[1], point_options.get('symbol'), point_options.get('size'), point_options.get('precision'), edge_color=point_options.get('edge_color'), edge_alpha=point_options.get('edge_alpha'), edge_width=point_options.get('edge_width'), face_color=point_options.get('face_color'), face_alpha=point_options.get('face_alpha')))","for e_target in enumerate(zip(lats, lngs)):
    location = e_target[1]
    i = e_target[0]
    point_options = {option: value[i] for (option, value) in options.items()}
    if point_options.get('marker'):
        self._markers.append(_Marker(location[0], location[1], point_options.get('face_color'), point_options.get('precision'), title=point_options.get('title'), label=point_options.get('label'), info_window=point_options.get('info_window'), draggable=point_options.get('draggable')))
    else:
        self._drawables.append(_Symbol(location[0], location[1], point_options.get('symbol'), point_options.get('size'), point_options.get('precision'), edge_color=point_options.get('edge_color'), edge_alpha=point_options.get('edge_alpha'), edge_width=point_options.get('edge_width'), face_color=point_options.get('face_color'), face_alpha=point_options.get('face_alpha')))"
scanpy,https://github.com/theislab/scanpy/tree/master/scanpy/plotting/_tools/paga.py,,paga_path$981,"for (igroup, group) in enumerate(nodes_ints):
    idcs = np.arange(adata.n_obs)[adata.obs[groups_key].values == nodes_strs[igroup]]
    if len(idcs) == 0:
        raise ValueError(f'Did not find data points that match `adata.obs[{groups_key!r}].values == {str(group)!r}`. Check whether `adata.obs[{groups_key!r}]` actually contains what you expect.')
    idcs_group = np.argsort(adata.obs['dpt_pseudotime'].values[adata.obs[groups_key].values == nodes_strs[igroup]])
    idcs = idcs[idcs_group]
    values = (adata.obs[key].values if key in adata.obs_keys() else adata_X[:, key].X)[idcs]
    x += (values.A if issparse(values) else values).tolist()
    if ikey == 0:
        groups += [group] * len(idcs)
        x_tick_locs.append(len(x))
        for anno in annotations:
            series = adata.obs[anno]
            if is_categorical_dtype(series):
                series = series.cat.codes
            anno_dict[anno] += list(series.values[idcs])","for e_target in enumerate(nodes_ints):
    group = e_target[1]
    igroup = e_target[0]
    idcs = np.arange(adata.n_obs)[adata.obs[groups_key].values == nodes_strs[igroup]]
    if len(idcs) == 0:
        raise ValueError(f'Did not find data points that match `adata.obs[{groups_key!r}].values == {str(group)!r}`. Check whether `adata.obs[{groups_key!r}]` actually contains what you expect.')
    idcs_group = np.argsort(adata.obs['dpt_pseudotime'].values[adata.obs[groups_key].values == nodes_strs[igroup]])
    idcs = idcs[idcs_group]
    values = (adata.obs[key].values if key in adata.obs_keys() else adata_X[:, key].X)[idcs]
    x += (values.A if issparse(values) else values).tolist()
    if ikey == 0:
        groups += [group] * len(idcs)
        x_tick_locs.append(len(x))
        for anno in annotations:
            series = adata.obs[anno]
            if is_categorical_dtype(series):
                series = series.cat.codes
            anno_dict[anno] += list(series.values[idcs])"
pyro,https://github.com/pyro-ppl/pyro/tree/master/pyro/util.py,,_are_independent$467,"for (name, counter1) in counters1.items():
    if name in counters2:
        if counters2[name] != counter1:
            return True","for e_target in counters1.items():
    counter1 = e_target[1]
    name = e_target[0]
    if name in counters2:
        if counters2[name] != counter1:
            return True"
borb,https://github.com/jorisschellekens/borb/tree/master/borb/io/write/page/pages_transformer.py,PagesTransformer,transform$31,"for (i, k) in enumerate(queue):
    object_to_transform['Kids'][i] = k","for e_target in enumerate(queue):
    k = e_target[1]
    i = e_target[0]
    object_to_transform['Kids'][i] = k"
hydrus,https://github.com/hydrusnetwork/hydrus/tree/master/hydrus/client/gui/ClientGUIDialogsManage.py,_NumericalPanel,SetRatingClipboardPairs$432,"for (service_key, rating) in rating_clipboard_pairs:
    if service_key in self._service_keys_to_controls:
        control = self._service_keys_to_controls[service_key]
        if rating is None:
            control.SetRatingState(ClientRatings.NULL)
        elif isinstance(rating, (int, float)) and 0 <= rating <= 1:
            control.SetRating(rating)","for e_target in rating_clipboard_pairs:
    rating = e_target[1]
    service_key = e_target[0]
    if service_key in self._service_keys_to_controls:
        control = self._service_keys_to_controls[service_key]
        if rating is None:
            control.SetRatingState(ClientRatings.NULL)
        elif isinstance(rating, (int, float)) and 0 <= rating <= 1:
            control.SetRating(rating)"
galaxy,https://github.com/ansible/galaxy/tree/master/lib/galaxy/datatypes/data.py,Data,get_display_applications_by_dataset$618,"for (key, value) in self.display_applications.items():
    value = value.filter_by_dataset(dataset, trans)
    if value.links:
        rval[key] = value","for e_target in self.display_applications.items():
    value = e_target[1]
    key = e_target[0]
    value = value.filter_by_dataset(dataset, trans)
    if value.links:
        rval[key] = value"
ludwig,https://github.com/ludwig-ai/ludwig/tree/master/tests/integration_tests/test_visualization.py,,test_visualization_cconfidence_thresholding_output_saved$939,"for (command, viz_pattern) in zip(commands, vis_patterns):
    result = subprocess.run(command)
    figure_cnt = glob.glob(viz_pattern)
    assert 0 == result.returncode
    assert 1 == len(figure_cnt)","for e_target in zip(commands, vis_patterns):
    viz_pattern = e_target[1]
    command = e_target[0]
    result = subprocess.run(command)
    figure_cnt = glob.glob(viz_pattern)
    assert 0 == result.returncode
    assert 1 == len(figure_cnt)"
FoxDot,https://github.com/Qirky/FoxDot/tree/master/FoxDot/lib/Patterns/Main.py,PGroup,ne$1218,"for (i, item) in enumerate(self.data):
    item = item != modi(other, i)
    if not isinstance(item, metaPattern):
        item = int(item)
    values.append(item)","for e_target in enumerate(self.data):
    item = e_target[1]
    i = e_target[0]
    item = item != modi(other, i)
    if not isinstance(item, metaPattern):
        item = int(item)
    values.append(item)"
sktime,https://github.com/alan-turing-institute/sktime/tree/master/sktime/forecasting/compose/_pipeline.py,ForecastingPipeline,_fit$185,"for (step_idx, name, transformer) in self._iter_transformers():
    t = clone(transformer)
    X = t.fit_transform(X=X, y=y)
    self.steps_[step_idx] = (name, t)","for e_target in self._iter_transformers():
    transformer = e_target[2]
    name = e_target[1]
    step_idx = e_target[0]
    t = clone(transformer)
    X = t.fit_transform(X=X, y=y)
    self.steps_[step_idx] = (name, t)"
tensorforce,https://github.com/tensorforce/tensorforce/tree/master/tensorforce/core/models/model.py,Model,fn_terminal$716,"for (name, previous, initial) in self.previous_internals.zip_items(initials):
    updates = tf.expand_dims(input=initial, axis=0)
    value = tf.tensor_scatter_nd_update(tensor=previous, indices=expanded_parallel, updates=updates)
    operations.append(previous.assign(value=value))","for e_target in self.previous_internals.zip_items(initials):
    initial = e_target[2]
    previous = e_target[1]
    name = e_target[0]
    updates = tf.expand_dims(input=initial, axis=0)
    value = tf.tensor_scatter_nd_update(tensor=previous, indices=expanded_parallel, updates=updates)
    operations.append(previous.assign(value=value))"
hamster,https://github.com/projecthamster/hamster/tree/master/src/hamster/storage/db.py,Storage,execute$922,"for (state, param) in zip(statement, params):
    logger.debug('%s %s' % (state, param))
    cur.execute(state, param)","for e_target in zip(statement, params):
    param = e_target[1]
    state = e_target[0]
    logger.debug('%s %s' % (state, param))
    cur.execute(state, param)"
d2go,https://github.com/facebookresearch/d2go/tree/master/projects_oss/detr/detr/d2/detr.py,ResNetMaskedBackbone,forward$46,"for (i, k) in enumerate(features.keys()):
    features[k] = NestedTensor(features[k], masks[i])","for e_target in enumerate(features.keys()):
    k = e_target[1]
    i = e_target[0]
    features[k] = NestedTensor(features[k], masks[i])"
scanpy,https://github.com/theislab/scanpy/tree/master/scanpy/tools/_marker_gene_overlap.py,,_calc_jaccard$52,"for (j, marker_group) in enumerate(markers1):
    tmp = [len(markers2[i].intersection(markers1[marker_group])) / len(markers2[i].union(markers1[marker_group])) for i in markers2.keys()]
    jacc_results[j, :] = tmp","for e_target in enumerate(markers1):
    marker_group = e_target[1]
    j = e_target[0]
    tmp = [len(markers2[i].intersection(markers1[marker_group])) / len(markers2[i].union(markers1[marker_group])) for i in markers2.keys()]
    jacc_results[j, :] = tmp"
armory,https://github.com/armory3d/armory/tree/master/blender/arm/logicnode/arm_nodes.py,ArmNodeAddInputOutputButton,execute$351,"for (socket_type, name_format) in zip(in_socket_types, in_name_formats):
    inps.new(socket_type, name_format.format(str(in_format_index)))","for e_target in zip(in_socket_types, in_name_formats):
    name_format = e_target[1]
    socket_type = e_target[0]
    inps.new(socket_type, name_format.format(str(in_format_index)))"
mars,https://github.com/mars-project/mars/tree/master/mars/dataframe/groupby/sample.py,GroupBySampleILoc,tile$181,"for (group_chunk, seed) in zip(grouped.chunks, seeds):
    new_op = op.copy().reset_key()
    new_op.stage = OperandStage.reduce
    new_op._weights = None
    new_op._random_state = None
    new_op._seed = seed
    result_chunks.append(new_op.new_chunk([group_chunk], shape=(np.nan,), index=(group_chunk.index[0],), dtype=out_tensor.dtype))","for e_target in zip(grouped.chunks, seeds):
    seed = e_target[1]
    group_chunk = e_target[0]
    new_op = op.copy().reset_key()
    new_op.stage = OperandStage.reduce
    new_op._weights = None
    new_op._random_state = None
    new_op._seed = seed
    result_chunks.append(new_op.new_chunk([group_chunk], shape=(np.nan,), index=(group_chunk.index[0],), dtype=out_tensor.dtype))"
checkmk,https://github.com/tribe29/checkmk/tree/master/cmk/base/plugins/agent_based/site_object_counts.py,,check_site_object_counts$56,"for (counted_obj, count) in counts.items():
    yield Metric(('%s %s' % (cmds_or_tags, counted_obj)).lower().replace(' ', '_').replace('.', '_'), count)
    cmds_or_tags_info.append('%s %s' % (count, counted_obj))","for e_target in counts.items():
    count = e_target[1]
    counted_obj = e_target[0]
    yield Metric(('%s %s' % (cmds_or_tags, counted_obj)).lower().replace(' ', '_').replace('.', '_'), count)
    cmds_or_tags_info.append('%s %s' % (count, counted_obj))"
graph_nets,https://github.com/deepmind/graph_nets/tree/master/graph_nets/utils_tf.py,,_populate_number_fields$553,"for (number_field, data_field) in [[N_NODE, NODES], [N_EDGE, RECEIVERS]]:
    if dct.get(number_field) is None:
        if dct[data_field] is not None:
            dct[number_field] = tf.shape(dct[data_field])[0]
        else:
            dct[number_field] = tf.constant(0, dtype=tf.int32)","for e_target in [[N_NODE, NODES], [N_EDGE, RECEIVERS]]:
    data_field = e_target[1]
    number_field = e_target[0]
    if dct.get(number_field) is None:
        if dct[data_field] is not None:
            dct[number_field] = tf.shape(dct[data_field])[0]
        else:
            dct[number_field] = tf.constant(0, dtype=tf.int32)"
django-modeltranslation,https://github.com/deschler/django-modeltranslation/tree/master/modeltranslation/models.py,,autodiscover$4,"for (app, mod) in mods:
    module = '%s.translation' % app
    before_import_registry = copy.copy(translator._registry)
    try:
        import_module(module)
    except:
        translator._registry = before_import_registry
        if module_has_submodule(mod, 'translation'):
            raise","for e_target in mods:
    mod = e_target[1]
    app = e_target[0]
    module = '%s.translation' % app
    before_import_registry = copy.copy(translator._registry)
    try:
        import_module(module)
    except:
        translator._registry = before_import_registry
        if module_has_submodule(mod, 'translation'):
            raise"
trankit,https://github.com/nlp-uoregon/trankit/tree/master/trankit/adapter_transformers/data/processors/glue.py,RteProcessor,_create_examples$506,"for (i, line) in enumerate(lines):
    if i == 0:
        continue
    guid = '%s-%s' % (set_type, line[0])
    text_a = line[1]
    text_b = line[2]
    label = None if set_type == 'test' else line[-1]
    examples.append(InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))","for e_target in enumerate(lines):
    line = e_target[1]
    i = e_target[0]
    if i == 0:
        continue
    guid = '%s-%s' % (set_type, line[0])
    text_a = line[1]
    text_b = line[2]
    label = None if set_type == 'test' else line[-1]
    examples.append(InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))"
detect-secrets,https://github.com/Yelp/detect-secrets/tree/master/detect_secrets/audit/common.py,,get_raw_secrets_from_file$65,"for (line_number, line) in zip(line_numbers, lines_to_scan):
    identified_secrets = call_function_with_arguments(plugin.analyze_line, filename=secret.filename, line=line, line_number=line_number + 1, enable_eager_search=bool(secret.line_number))
    for identified_secret in identified_secrets or []:
        if identified_secret == secret:
            all_secrets.append(identified_secret)","for e_target in zip(line_numbers, lines_to_scan):
    line = e_target[1]
    line_number = e_target[0]
    identified_secrets = call_function_with_arguments(plugin.analyze_line, filename=secret.filename, line=line, line_number=line_number + 1, enable_eager_search=bool(secret.line_number))
    for identified_secret in identified_secrets or []:
        if identified_secret == secret:
            all_secrets.append(identified_secret)"
evalml,https://github.com/alteryx/evalml/tree/master/evalml/tests/automl_tests/test_automl.py,,test_pipeline_fit_raises$333,"for (name, score) in cv_scores['all_objective_scores'].items():
    if name in ['# Training', '# Validation']:
        assert score > 0
    else:
        assert np.isnan(score)","for e_target in cv_scores['all_objective_scores'].items():
    score = e_target[1]
    name = e_target[0]
    if name in ['# Training', '# Validation']:
        assert score > 0
    else:
        assert np.isnan(score)"
OpenFermion,https://github.com/quantumlib/OpenFermion/tree/master/src/openfermion/testing/testing_utils_test.py,RandomInteractionOperatorTest,test_symmetry$256,"for (p, q, r, s) in itertools.product(range(n_orbitals), repeat=4):
    self.assertAlmostEqual(two_body_coefficients[p, q, r, s], two_body_coefficients[r, q, p, s])
    self.assertAlmostEqual(two_body_coefficients[p, q, r, s], two_body_coefficients[p, s, r, q])
    self.assertAlmostEqual(two_body_coefficients[p, q, r, s], two_body_coefficients[s, r, q, p])
    self.assertAlmostEqual(two_body_coefficients[p, q, r, s], two_body_coefficients[q, p, s, r])
    self.assertAlmostEqual(two_body_coefficients[p, q, r, s], two_body_coefficients[r, s, p, q])
    self.assertAlmostEqual(two_body_coefficients[p, q, r, s], two_body_coefficients[s, p, q, r])
    self.assertAlmostEqual(two_body_coefficients[p, q, r, s], two_body_coefficients[q, r, s, p])","for e_target in itertools.product(range(n_orbitals), repeat=4):
    s = e_target[3]
    r = e_target[2]
    q = e_target[1]
    p = e_target[0]
    self.assertAlmostEqual(two_body_coefficients[p, q, r, s], two_body_coefficients[r, q, p, s])
    self.assertAlmostEqual(two_body_coefficients[p, q, r, s], two_body_coefficients[p, s, r, q])
    self.assertAlmostEqual(two_body_coefficients[p, q, r, s], two_body_coefficients[s, r, q, p])
    self.assertAlmostEqual(two_body_coefficients[p, q, r, s], two_body_coefficients[q, p, s, r])
    self.assertAlmostEqual(two_body_coefficients[p, q, r, s], two_body_coefficients[r, s, p, q])
    self.assertAlmostEqual(two_body_coefficients[p, q, r, s], two_body_coefficients[s, p, q, r])
    self.assertAlmostEqual(two_body_coefficients[p, q, r, s], two_body_coefficients[q, r, s, p])"
networkx,https://github.com/networkx/networkx/tree/master/networkx/algorithms/efficiency_measures.py,,global_efficiency$55,"for (source, targets) in lengths:
    for (target, distance) in targets.items():
        if distance > 0:
            g_eff += 1 / distance","for e_target in lengths:
    targets = e_target[1]
    source = e_target[0]
    for (target, distance) in targets.items():
        if distance > 0:
            g_eff += 1 / distance"
river,https://github.com/online-ml/river/tree/master/river/compat/river_to_sklearn.py,River2SKLTransformer,_partial_fit$418,"for (x, yi) in STREAM_METHODS[type(X)](X, y):
    self.instance_.learn_one(x, yi)","for e_target in STREAM_METHODS[type(X)](X, y):
    yi = e_target[1]
    x = e_target[0]
    self.instance_.learn_one(x, yi)"
dynaconf,https://github.com/rochacbruno/dynaconf/tree/master/dynaconf/vendor/box/box.py,Box,__init__$58,"for (D, C) in B[0]:
    A.__setitem__(D, C)","for e_target in B[0]:
    C = e_target[1]
    D = e_target[0]
    A.__setitem__(D, C)"
Red-DiscordBot,https://github.com/Cog-Creators/Red-DiscordBot/tree/master/redbot/cogs/trivia/trivia.py,Trivia,_get_leaderboard$527,"for (member, stats) in data.items():
    if stats['games'] != 0:
        stats['average_score'] = stats['total_score'] / stats['games']
    else:
        stats['average_score'] = 0.0","for e_target in data.items():
    stats = e_target[1]
    member = e_target[0]
    if stats['games'] != 0:
        stats['average_score'] = stats['total_score'] / stats['games']
    else:
        stats['average_score'] = 0.0"
anaconda,https://github.com/DamnWidget/anaconda/tree/master/anaconda_lib/jedi/api/helpers.py,CallDetails,calculate_index$223,"for (i, (star_count, key_start, had_equal)) in enumerate(args):
    is_kwarg |= had_equal | (star_count == 2)
    if star_count:
        pass
    elif i + 1 != len(args):
        if had_equal:
            used_names.add(key_start)
        else:
            positional_count += 1","for e_target in enumerate(args):
    had_equal = e_target[1][2]
    key_start = e_target[1][1]
    star_count = e_target[1][0]
    i = e_target[0]
    is_kwarg |= had_equal | (star_count == 2)
    if star_count:
        pass
    elif i + 1 != len(args):
        if had_equal:
            used_names.add(key_start)
        else:
            positional_count += 1"
soynlp,https://github.com/lovit/soynlp/tree/master/soynlp/predicator/_stem.py,StemExtractor,_conjugate_stem_and_eomi$36,"for (i, stem) in enumerate(stems):
    if self.verbose and i % 100 == 0:
        message = 'Checking combination of {} / {} stems + {} eomis'.format(i, n_stems, n_eomis)
        self._print(message, replace=True, newline=False)
    stem_len = len(stem)
    for eomi in eomis:
        try:
            for word in conjugate(stem, eomi):
                if eojeol_counter[word] == 0 or len(word) <= stem_len:
                    continue
                (l, r) = (word[:stem_len], word[stem_len:])
                stem_surfaces.add(l)
                eomi_surfaces.add(r)
        except:
            continue","for e_target in enumerate(stems):
    stem = e_target[1]
    i = e_target[0]
    if self.verbose and i % 100 == 0:
        message = 'Checking combination of {} / {} stems + {} eomis'.format(i, n_stems, n_eomis)
        self._print(message, replace=True, newline=False)
    stem_len = len(stem)
    for eomi in eomis:
        try:
            for word in conjugate(stem, eomi):
                if eojeol_counter[word] == 0 or len(word) <= stem_len:
                    continue
                (l, r) = (word[:stem_len], word[stem_len:])
                stem_surfaces.add(l)
                eomi_surfaces.add(r)
        except:
            continue"
capa,https://github.com/mandiant/capa/tree/master/scripts/lint.py,,collect_samples$859,"for (root, dirs, files) in os.walk(path):
    for name in files:
        if name.endswith('.viv'):
            continue
        if name.endswith('.idb'):
            continue
        if name.endswith('.i64'):
            continue
        if name.endswith('.frz'):
            continue
        if name.endswith('.fnames'):
            continue
        path = pathlib.Path(os.path.join(root, name))
        try:
            with path.open('rb') as f:
                buf = f.read()
        except IOError:
            continue
        sha256 = hashlib.sha256()
        sha256.update(buf)
        md5 = hashlib.md5()
        md5.update(buf)
        samples[sha256.hexdigest().lower()] = path
        samples[sha256.hexdigest().upper()] = path
        samples[md5.hexdigest().lower()] = path
        samples[md5.hexdigest().upper()] = path
        samples[name] = path","for e_target in os.walk(path):
    files = e_target[2]
    dirs = e_target[1]
    root = e_target[0]
    for name in files:
        if name.endswith('.viv'):
            continue
        if name.endswith('.idb'):
            continue
        if name.endswith('.i64'):
            continue
        if name.endswith('.frz'):
            continue
        if name.endswith('.fnames'):
            continue
        path = pathlib.Path(os.path.join(root, name))
        try:
            with path.open('rb') as f:
                buf = f.read()
        except IOError:
            continue
        sha256 = hashlib.sha256()
        sha256.update(buf)
        md5 = hashlib.md5()
        md5.update(buf)
        samples[sha256.hexdigest().lower()] = path
        samples[sha256.hexdigest().upper()] = path
        samples[md5.hexdigest().lower()] = path
        samples[md5.hexdigest().upper()] = path
        samples[name] = path"
discord.py,https://github.com/Rapptz/discord.py/tree/master/discord/client.py,Client,dispatch$365,"for (i, (future, condition)) in enumerate(listeners):
    if future.cancelled():
        removed.append(i)
        continue
    try:
        result = condition(*args)
    except Exception as exc:
        future.set_exception(exc)
        removed.append(i)
    else:
        if result:
            if len(args) == 0:
                future.set_result(None)
            elif len(args) == 1:
                future.set_result(args[0])
            else:
                future.set_result(args)
            removed.append(i)","for e_target in enumerate(listeners):
    condition = e_target[1][1]
    future = e_target[1][0]
    i = e_target[0]
    if future.cancelled():
        removed.append(i)
        continue
    try:
        result = condition(*args)
    except Exception as exc:
        future.set_exception(exc)
        removed.append(i)
    else:
        if result:
            if len(args) == 0:
                future.set_result(None)
            elif len(args) == 1:
                future.set_result(args[0])
            else:
                future.set_result(args)
            removed.append(i)"
pyvista,https://github.com/pyvista/pyvista/tree/master/tests/test_filters.py,,test_slice_orthogonal_filter$233,"for (i, dataset) in enumerate(datasets):
    slices = dataset.slice_orthogonal(progress_bar=True)
    assert slices is not None
    assert isinstance(slices, pyvista.MultiBlock)
    assert slices.n_blocks == 3
    for slc in slices:
        assert isinstance(slc, pyvista.PolyData)","for e_target in enumerate(datasets):
    dataset = e_target[1]
    i = e_target[0]
    slices = dataset.slice_orthogonal(progress_bar=True)
    assert slices is not None
    assert isinstance(slices, pyvista.MultiBlock)
    assert slices.n_blocks == 3
    for slc in slices:
        assert isinstance(slc, pyvista.PolyData)"
imgaug,https://github.com/aleju/imgaug/tree/master/test/test_parameters.py,TestDeterministicList,test_draw_samples_float$2559,"for (shape, expected) in zip(shapes, expecteds):
    with self.subTest(shape=shape):
        samples = param.draw_samples(shape)
        shape_expected = shape if isinstance(shape, tuple) else tuple([shape])
        assert samples.shape == shape_expected
        assert np.allclose(samples, expected, rtol=0, atol=1e-05)","for e_target in zip(shapes, expecteds):
    expected = e_target[1]
    shape = e_target[0]
    with self.subTest(shape=shape):
        samples = param.draw_samples(shape)
        shape_expected = shape if isinstance(shape, tuple) else tuple([shape])
        assert samples.shape == shape_expected
        assert np.allclose(samples, expected, rtol=0, atol=1e-05)"
KL-Loss,https://github.com/yihui-he/KL-Loss/tree/master/detectron/datasets/voc_dataset_evaluator.py,,_write_voc_results_files$57,"for (im_ind, index) in enumerate(image_index):
    dets = all_boxes[cls_ind][im_ind]
    if type(dets) == list:
        assert len(dets) == 0, 'dets should be numpy.ndarray or empty list'
        continue
    for k in range(dets.shape[0]):
        f.write('{:s} {:.3f} {:.1f} {:.1f} {:.1f} {:.1f}\n'.format(index, dets[k, -1], dets[k, 0] + 1, dets[k, 1] + 1, dets[k, 2] + 1, dets[k, 3] + 1))","for e_target in enumerate(image_index):
    index = e_target[1]
    im_ind = e_target[0]
    dets = all_boxes[cls_ind][im_ind]
    if type(dets) == list:
        assert len(dets) == 0, 'dets should be numpy.ndarray or empty list'
        continue
    for k in range(dets.shape[0]):
        f.write('{:s} {:.3f} {:.1f} {:.1f} {:.1f} {:.1f}\n'.format(index, dets[k, -1], dets[k, 0] + 1, dets[k, 1] + 1, dets[k, 2] + 1, dets[k, 3] + 1))"
MONAI,https://github.com/Project-MONAI/MONAI/tree/master/monai/transforms/utils.py,,compute_divisible_spatial_size$1224,"for (k_d, dim) in zip(k, spatial_shape):
    new_dim = int(np.ceil(dim / k_d) * k_d) if k_d > 0 else dim
    new_size.append(new_dim)","for e_target in zip(k, spatial_shape):
    dim = e_target[1]
    k_d = e_target[0]
    new_dim = int(np.ceil(dim / k_d) * k_d) if k_d > 0 else dim
    new_size.append(new_dim)"
Deep-Reinforcement-Learning-Hands-On,https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On/tree/master/Chapter10/02_pong_a2c.py,,if_main_my$95,"for (step_idx, exp) in enumerate(exp_source):
    batch.append(exp)
    new_rewards = exp_source.pop_total_rewards()
    if new_rewards:
        if tracker.reward(new_rewards[0], step_idx):
            break
    if len(batch) < BATCH_SIZE:
        continue
    (states_v, actions_t, vals_ref_v) = unpack_batch(batch, net, device=device)
    batch.clear()
    optimizer.zero_grad()
    (logits_v, value_v) = net(states_v)
    loss_value_v = F.mse_loss(value_v.squeeze(-1), vals_ref_v)
    log_prob_v = F.log_softmax(logits_v, dim=1)
    adv_v = vals_ref_v - value_v.squeeze(-1).detach()
    log_prob_actions_v = adv_v * log_prob_v[range(BATCH_SIZE), actions_t]
    loss_policy_v = -log_prob_actions_v.mean()
    prob_v = F.softmax(logits_v, dim=1)
    entropy_loss_v = ENTROPY_BETA * (prob_v * log_prob_v).sum(dim=1).mean()
    loss_policy_v.backward(retain_graph=True)
    grads = np.concatenate([p.grad.data.cpu().numpy().flatten() for p in net.parameters() if p.grad is not None])
    loss_v = entropy_loss_v + loss_value_v
    loss_v.backward()
    nn_utils.clip_grad_norm_(net.parameters(), CLIP_GRAD)
    optimizer.step()
    loss_v += loss_policy_v
    tb_tracker.track('advantage', adv_v, step_idx)
    tb_tracker.track('values', value_v, step_idx)
    tb_tracker.track('batch_rewards', vals_ref_v, step_idx)
    tb_tracker.track('loss_entropy', entropy_loss_v, step_idx)
    tb_tracker.track('loss_policy', loss_policy_v, step_idx)
    tb_tracker.track('loss_value', loss_value_v, step_idx)
    tb_tracker.track('loss_total', loss_v, step_idx)
    tb_tracker.track('grad_l2', np.sqrt(np.mean(np.square(grads))), step_idx)
    tb_tracker.track('grad_max', np.max(np.abs(grads)), step_idx)
    tb_tracker.track('grad_var', np.var(grads), step_idx)","for e_target in enumerate(exp_source):
    exp = e_target[1]
    step_idx = e_target[0]
    batch.append(exp)
    new_rewards = exp_source.pop_total_rewards()
    if new_rewards:
        if tracker.reward(new_rewards[0], step_idx):
            break
    if len(batch) < BATCH_SIZE:
        continue
    (states_v, actions_t, vals_ref_v) = unpack_batch(batch, net, device=device)
    batch.clear()
    optimizer.zero_grad()
    (logits_v, value_v) = net(states_v)
    loss_value_v = F.mse_loss(value_v.squeeze(-1), vals_ref_v)
    log_prob_v = F.log_softmax(logits_v, dim=1)
    adv_v = vals_ref_v - value_v.squeeze(-1).detach()
    log_prob_actions_v = adv_v * log_prob_v[range(BATCH_SIZE), actions_t]
    loss_policy_v = -log_prob_actions_v.mean()
    prob_v = F.softmax(logits_v, dim=1)
    entropy_loss_v = ENTROPY_BETA * (prob_v * log_prob_v).sum(dim=1).mean()
    loss_policy_v.backward(retain_graph=True)
    grads = np.concatenate([p.grad.data.cpu().numpy().flatten() for p in net.parameters() if p.grad is not None])
    loss_v = entropy_loss_v + loss_value_v
    loss_v.backward()
    nn_utils.clip_grad_norm_(net.parameters(), CLIP_GRAD)
    optimizer.step()
    loss_v += loss_policy_v
    tb_tracker.track('advantage', adv_v, step_idx)
    tb_tracker.track('values', value_v, step_idx)
    tb_tracker.track('batch_rewards', vals_ref_v, step_idx)
    tb_tracker.track('loss_entropy', entropy_loss_v, step_idx)
    tb_tracker.track('loss_policy', loss_policy_v, step_idx)
    tb_tracker.track('loss_value', loss_value_v, step_idx)
    tb_tracker.track('loss_total', loss_v, step_idx)
    tb_tracker.track('grad_l2', np.sqrt(np.mean(np.square(grads))), step_idx)
    tb_tracker.track('grad_max', np.max(np.abs(grads)), step_idx)
    tb_tracker.track('grad_var', np.var(grads), step_idx)"
pororo,https://github.com/kakaobrain/pororo/tree/master/pororo/tasks/named_entity_recognition.py,PororoBertCharNer,_apply_wsd$289,"for (idx, ner_token) in enumerate(tags):
    (surface, tag) = ner_token
    surface = surface.replace('{', '閿')
    surface = surface.replace('}', '閿')
    if tag == 'TERM':
        cat = self._template_match(surface, self._term2cat)
        if cat is not None:
            tags[idx] = (surface, cat)
        input_text_with_markers += surface
    elif tag == 'QUANTITY':
        cat = self._template_match(surface, self._quant2cat)
        if cat is not None:
            tags[idx] = (surface, cat)
            input_text_with_markers += surface
        else:
            target_token_ids.append(idx)
            input_text_with_markers += '{' + surface + '}'
    else:
        input_text_with_markers += surface","for e_target in enumerate(tags):
    ner_token = e_target[1]
    idx = e_target[0]
    (surface, tag) = ner_token
    surface = surface.replace('{', '閿')
    surface = surface.replace('}', '閿')
    if tag == 'TERM':
        cat = self._template_match(surface, self._term2cat)
        if cat is not None:
            tags[idx] = (surface, cat)
        input_text_with_markers += surface
    elif tag == 'QUANTITY':
        cat = self._template_match(surface, self._quant2cat)
        if cat is not None:
            tags[idx] = (surface, cat)
            input_text_with_markers += surface
        else:
            target_token_ids.append(idx)
            input_text_with_markers += '{' + surface + '}'
    else:
        input_text_with_markers += surface"
d2l-vn,https://github.com/mlbvn/d2l-vn/tree/master/d2l/d2l.py,,tokenize_nmt$755,"for (i, line) in enumerate(text.split('\n')):
    if num_examples and i > num_examples:
        break
    parts = line.split('\t')
    if len(parts) == 2:
        source.append(parts[0].split(' '))
        target.append(parts[1].split(' '))","for e_target in enumerate(text.split('\n')):
    line = e_target[1]
    i = e_target[0]
    if num_examples and i > num_examples:
        break
    parts = line.split('\t')
    if len(parts) == 2:
        source.append(parts[0].split(' '))
        target.append(parts[1].split(' '))"
CrypTen,https://github.com/facebookresearch/CrypTen/tree/master/test/test_models.py,TestModels,_check_parameters$57,"for (name, crypten_param) in crypten_named_params.items():
    self.assertTrue(name in torchvision_named_params, msg)
    torchvision_param = torchvision_named_params[name]
    self.assertEqual(crypten_param.size(), torchvision_param.size(), f'{msg}: {name} size')
    if pretrained:
        if isinstance(crypten_param, crypten.CrypTensor):
            crypten_param = crypten_param.get_plain_text()
        self.assertTrue(torch.allclose(crypten_param, torchvision_param, atol=0.0001), f'{msg}: {name}')","for e_target in crypten_named_params.items():
    crypten_param = e_target[1]
    name = e_target[0]
    self.assertTrue(name in torchvision_named_params, msg)
    torchvision_param = torchvision_named_params[name]
    self.assertEqual(crypten_param.size(), torchvision_param.size(), f'{msg}: {name} size')
    if pretrained:
        if isinstance(crypten_param, crypten.CrypTensor):
            crypten_param = crypten_param.get_plain_text()
        self.assertTrue(torch.allclose(crypten_param, torchvision_param, atol=0.0001), f'{msg}: {name}')"
upvote_py2,https://github.com/google/upvote_py2/tree/master/upvote/gae/lib/bit9/utils_test.py,CamelToSnakeCaseTest,testConversions$29,"for (test, expected) in tests:
    self.assertEqual(expected, utils.camel_to_snake_case(test))","for e_target in tests:
    expected = e_target[1]
    test = e_target[0]
    self.assertEqual(expected, utils.camel_to_snake_case(test))"
Telethon,https://github.com/LonamiWebs/Telethon/tree/master/telethon_generator/generators/tlobject.py,,_write_modules$54,"for (namespace, names) in imports.items():
    builder.writeln('from {} import {}', namespace, ', '.join(sorted(names)))","for e_target in imports.items():
    names = e_target[1]
    namespace = e_target[0]
    builder.writeln('from {} import {}', namespace, ', '.join(sorted(names)))"
sec-edgar,https://github.com/sec-edgar/sec-edgar/tree/master/secedgar/core/combo.py,ComboFilings,reducer$133,"for (key, value) in dictionary.items():
    accumulator[key] = accumulator.get(key, []) + value","for e_target in dictionary.items():
    value = e_target[1]
    key = e_target[0]
    accumulator[key] = accumulator.get(key, []) + value"
sympy,https://github.com/sympy/sympy/tree/master/sympy/solvers/ode/ode.py,,check_linear_2eq_order1$1349,"for (n, i) in enumerate([p1, p2]):
    for j in Mul.make_args(collect_const(i)):
        if not j.has(t):
            q = j
        if q and n == 0:
            if (r['b2'] / j - r['b1']) / (r['c1'] - r['c2'] / j) == j:
                p = 1
        elif q and n == 1:
            if (r['b1'] / j - r['b2']) / (r['c2'] - r['c1'] / j) == j:
                p = 2","for e_target in enumerate([p1, p2]):
    i = e_target[1]
    n = e_target[0]
    for j in Mul.make_args(collect_const(i)):
        if not j.has(t):
            q = j
        if q and n == 0:
            if (r['b2'] / j - r['b1']) / (r['c1'] - r['c2'] / j) == j:
                p = 1
        elif q and n == 1:
            if (r['b1'] / j - r['b2']) / (r['c2'] - r['c1'] / j) == j:
                p = 2"
trankit,https://github.com/nlp-uoregon/trankit/tree/master/trankit/adapter_transformers/modeling_transfo_xl.py,,build_tf_to_pytorch_map$42,"for (i, (out_l, proj_l, tie_proj)) in enumerate(zip(model.crit.out_layers, model.crit.out_projs, config.tie_projs)):
    layer_str = 'transformer/adaptive_softmax/cutoff_%d/' % i
    if config.tie_weight:
        tf_to_pt_map.update({layer_str + 'b': out_l.bias})
    else:
        raise NotImplementedError
        tf_to_pt_map.update({layer_str + 'lookup_table': out_l.weight, layer_str + 'b': out_l.bias})
    if not tie_proj:
        tf_to_pt_map.update({layer_str + 'proj': proj_l})","for e_target in enumerate(zip(model.crit.out_layers, model.crit.out_projs, config.tie_projs)):
    tie_proj = e_target[1][2]
    proj_l = e_target[1][1]
    out_l = e_target[1][0]
    i = e_target[0]
    layer_str = 'transformer/adaptive_softmax/cutoff_%d/' % i
    if config.tie_weight:
        tf_to_pt_map.update({layer_str + 'b': out_l.bias})
    else:
        raise NotImplementedError
        tf_to_pt_map.update({layer_str + 'lookup_table': out_l.weight, layer_str + 'b': out_l.bias})
    if not tie_proj:
        tf_to_pt_map.update({layer_str + 'proj': proj_l})"
piano_transcription,https://github.com/bytedance/piano_transcription/tree/master/pytorch/models.py,,_concat_init$34,"for (i, init_func) in enumerate(init_funcs):
    init_func(tensor[i * fan_in:(i + 1) * fan_in, :])","for e_target in enumerate(init_funcs):
    init_func = e_target[1]
    i = e_target[0]
    init_func(tensor[i * fan_in:(i + 1) * fan_in, :])"
hyperparameter_hunter,https://github.com/HunterMcGushion/hyperparameter_hunter/tree/master/hyperparameter_hunter/experiments.py,BaseCVExperiment,cross_validation_workflow$609,"for (self._fold, (self.train_index, self.validation_index)) in enumerate(rep_indices):
    self.cv_fold_workflow()","for e_target in enumerate(rep_indices):
    self.validation_index = e_target[1][1]
    self.train_index = e_target[1][0]
    self._fold = e_target[0]
    self.cv_fold_workflow()"
freemocap,https://github.com/jonmatthis/freemocap/tree/master/freemocap/calibrate.py,,CalibrateCaptureVolume$21,"for (count, thisVidPath) in enumerate(calibrationVideoPath.glob('*.mp4'), start=1):
    vidnames.append([str(thisVidPath)])
    cam_names.append(str(count))
    session.numCams = count","for e_target in enumerate(calibrationVideoPath.glob('*.mp4'), start=1):
    thisVidPath = e_target[1]
    count = e_target[0]
    vidnames.append([str(thisVidPath)])
    cam_names.append(str(count))
    session.numCams = count"
rdflib,https://github.com/RDFLib/rdflib/tree/master/rdflib/plugins/stores/sparqlstore.py,SPARQLUpdateStore,predicates$828,"for (t, c) in self.triples((subject, None, object)):
    yield t[1]","for e_target in self.triples((subject, None, object)):
    c = e_target[1]
    t = e_target[0]
    yield t[1]"
dulwich,https://github.com/dulwich/dulwich/tree/master/dulwich/config.py,ConfigFile,from_file$391,"for (lineno, line) in enumerate(f.readlines()):
    if lineno == 0 and line.startswith(b'\xef\xbb\xbf'):
        line = line[3:]
    line = line.lstrip()
    if setting is None:
        if len(line) > 0 and line[:1] == b'[':
            line = _strip_comments(line).rstrip()
            try:
                last = line.index(b']')
            except ValueError:
                raise ValueError('expected trailing ]')
            pts = line[1:last].split(b' ', 1)
            line = line[last + 1:]
            if len(pts) == 2:
                if pts[1][:1] != b'""' or pts[1][-1:] != b'""':
                    raise ValueError('Invalid subsection %r' % pts[1])
                else:
                    pts[1] = pts[1][1:-1]
                if not _check_section_name(pts[0]):
                    raise ValueError('invalid section name %r' % pts[0])
                section = (pts[0], pts[1])
            else:
                if not _check_section_name(pts[0]):
                    raise ValueError('invalid section name %r' % pts[0])
                pts = pts[0].split(b'.', 1)
                if len(pts) == 2:
                    section = (pts[0], pts[1])
                else:
                    section = (pts[0],)
            ret._values.setdefault(section)
        if _strip_comments(line).strip() == b'':
            continue
        if section is None:
            raise ValueError('setting %r without section' % line)
        try:
            (setting, value) = line.split(b'=', 1)
        except ValueError:
            setting = line
            value = b'true'
        setting = setting.strip()
        if not _check_variable_name(setting):
            raise ValueError('invalid variable name %r' % setting)
        if value.endswith(b'\\\n'):
            continuation = value[:-2]
        else:
            continuation = None
            value = _parse_string(value)
            ret._values[section][setting] = value
            setting = None
    elif line.endswith(b'\\\n'):
        continuation += line[:-2]
    else:
        continuation += line
        value = _parse_string(continuation)
        ret._values[section][setting] = value
        continuation = None
        setting = None","for e_target in enumerate(f.readlines()):
    line = e_target[1]
    lineno = e_target[0]
    if lineno == 0 and line.startswith(b'\xef\xbb\xbf'):
        line = line[3:]
    line = line.lstrip()
    if setting is None:
        if len(line) > 0 and line[:1] == b'[':
            line = _strip_comments(line).rstrip()
            try:
                last = line.index(b']')
            except ValueError:
                raise ValueError('expected trailing ]')
            pts = line[1:last].split(b' ', 1)
            line = line[last + 1:]
            if len(pts) == 2:
                if pts[1][:1] != b'""' or pts[1][-1:] != b'""':
                    raise ValueError('Invalid subsection %r' % pts[1])
                else:
                    pts[1] = pts[1][1:-1]
                if not _check_section_name(pts[0]):
                    raise ValueError('invalid section name %r' % pts[0])
                section = (pts[0], pts[1])
            else:
                if not _check_section_name(pts[0]):
                    raise ValueError('invalid section name %r' % pts[0])
                pts = pts[0].split(b'.', 1)
                if len(pts) == 2:
                    section = (pts[0], pts[1])
                else:
                    section = (pts[0],)
            ret._values.setdefault(section)
        if _strip_comments(line).strip() == b'':
            continue
        if section is None:
            raise ValueError('setting %r without section' % line)
        try:
            (setting, value) = line.split(b'=', 1)
        except ValueError:
            setting = line
            value = b'true'
        setting = setting.strip()
        if not _check_variable_name(setting):
            raise ValueError('invalid variable name %r' % setting)
        if value.endswith(b'\\\n'):
            continuation = value[:-2]
        else:
            continuation = None
            value = _parse_string(value)
            ret._values[section][setting] = value
            setting = None
    elif line.endswith(b'\\\n'):
        continuation += line[:-2]
    else:
        continuation += line
        value = _parse_string(continuation)
        ret._values[section][setting] = value
        continuation = None
        setting = None"
CellProfiler,https://github.com/CellProfiler/CellProfiler/tree/master/cellprofiler/modules/calculatemath.py,CalculateMath,run$402,"for (object_name, r) in zip(all_object_names, result):
    m.add_measurement(object_name, feature, r)","for e_target in zip(all_object_names, result):
    r = e_target[1]
    object_name = e_target[0]
    m.add_measurement(object_name, feature, r)"
maskscoring_rcnn,https://github.com/zjhuang22/maskscoring_rcnn/tree/master/maskrcnn_benchmark/engine/inference.py,,compute_on_dataset$23,"for (i, batch) in tqdm(enumerate(data_loader)):
    (images, targets, image_ids) = batch
    images = images.to(device)
    with torch.no_grad():
        output = model(images)
        output = [o.to(cpu_device) for o in output]
    results_dict.update({img_id: result for (img_id, result) in zip(image_ids, output)})","for e_target in tqdm(enumerate(data_loader)):
    batch = e_target[1]
    i = e_target[0]
    (images, targets, image_ids) = batch
    images = images.to(device)
    with torch.no_grad():
        output = model(images)
        output = [o.to(cpu_device) for o in output]
    results_dict.update({img_id: result for (img_id, result) in zip(image_ids, output)})"
cantools,https://github.com/cantools/cantools/tree/master/cantools/subparsers/dump/formatting.py,,signal_tree_string$17,"for (index, signal_name) in enumerate(signal_names):
    if isinstance(signal_name, dict):
        (signal_name_line, signal_lines) = format_mux(signal_name)
        signal_lines = add_prefix(get_prefix(index, len(signal_names)), signal_lines)
    else:
        signal_name_line = format_signal_line(signal_name)
        signal_lines = []
    lines.append(signal_name_line)
    lines += signal_lines","for e_target in enumerate(signal_names):
    signal_name = e_target[1]
    index = e_target[0]
    if isinstance(signal_name, dict):
        (signal_name_line, signal_lines) = format_mux(signal_name)
        signal_lines = add_prefix(get_prefix(index, len(signal_names)), signal_lines)
    else:
        signal_name_line = format_signal_line(signal_name)
        signal_lines = []
    lines.append(signal_name_line)
    lines += signal_lines"
spacy-transformers,https://github.com/explosion/spacy-transformers/tree/master/spacy_transformers/tests/test_pipeline_component.py,,custom_annotation_setter$90,"for (doc, data) in zip(docs, doc_data):
    doc._.custom_attr = data","for e_target in zip(docs, doc_data):
    data = e_target[1]
    doc = e_target[0]
    doc._.custom_attr = data"
demucs,https://github.com/facebookresearch/demucs/tree/master/tools/convert.py,,transform$53,"for (old, new) in TO_REPLACE:
    argv[:] = [a.replace(old, new) for a in argv]","for e_target in TO_REPLACE:
    new = e_target[1]
    old = e_target[0]
    argv[:] = [a.replace(old, new) for a in argv]"
ivre,https://github.com/ivre/ivre/tree/master/ivre/target.py,TargetZMapPreScan,__init__$412,"for (start, count) in target.targets.ranges.values():
    for net in utils.range2nets((start, start + count - 1)):
        self.tmpfile.write('%s\n' % net)","for e_target in target.targets.ranges.values():
    count = e_target[1]
    start = e_target[0]
    for net in utils.range2nets((start, start + count - 1)):
        self.tmpfile.write('%s\n' % net)"
kivy,https://github.com/kivy/kivy/tree/master/kivy/uix/recycleview/views.py,RecycleDataAdapter,make_views_dirty$333,"for (index, view) in views.items():
    dirty_views[view.__class__][index] = view","for e_target in views.items():
    view = e_target[1]
    index = e_target[0]
    dirty_views[view.__class__][index] = view"
jsonschema,https://github.com/Julian/jsonschema/tree/master/jsonschema/_validators.py,,oneOf$379,"for (index, subschema) in subschemas:
    errs = list(validator.descend(instance, subschema, schema_path=index))
    if not errs:
        first_valid = subschema
        break
    all_errors.extend(errs)
else:
    yield ValidationError(f'{instance!r} is not valid under any of the given schemas', context=all_errors)","for e_target in subschemas:
    subschema = e_target[1]
    index = e_target[0]
    errs = list(validator.descend(instance, subschema, schema_path=index))
    if not errs:
        first_valid = subschema
        break
    all_errors.extend(errs)
else:
    yield ValidationError(f'{instance!r} is not valid under any of the given schemas', context=all_errors)"
scipy,https://github.com/scipy/scipy/tree/master/scipy/stats/tests/test_distributions.py,TestTruncnorm,test_gh_9403_medium_tail_values$858,"for (low, high) in [[39, 40], [-40, -39]]:
    xvals = np.array([-np.inf, low, high, np.inf])
    xmid = (high + low) / 2.0
    cdfs = stats.truncnorm.cdf(xvals, low, high)
    sfs = stats.truncnorm.sf(xvals, low, high)
    pdfs = stats.truncnorm.pdf(xvals, low, high)
    expected_cdfs = np.array([0, 0, 1, 1])
    expected_sfs = np.array([1.0, 1.0, 0.0, 0.0])
    expected_pdfs = np.array([0, 39.0256074, 2.73349092e-16, 0])
    if low < 0:
        expected_pdfs = np.array([0, 2.73349092e-16, 39.0256074, 0])
    assert_almost_equal(cdfs, expected_cdfs)
    assert_almost_equal(sfs, expected_sfs)
    assert_almost_equal(pdfs, expected_pdfs)
    assert_almost_equal(np.log(expected_pdfs[1] / expected_pdfs[2]), low + 0.5)
    pvals = np.array([0, 0.5, 1.0])
    ppfs = stats.truncnorm.ppf(pvals, low, high)
    expected_ppfs = np.array([low, np.sign(low) * 39.01775731, high])
    assert_almost_equal(ppfs, expected_ppfs)
    cdfs = stats.truncnorm.cdf(ppfs, low, high)
    assert_almost_equal(cdfs, pvals)
    if low < 0:
        assert_almost_equal(stats.truncnorm.sf(xmid, low, high), 0.9999999970389126)
        assert_almost_equal(stats.truncnorm.cdf(xmid, low, high), 2.961048103554866e-09)
    else:
        assert_almost_equal(stats.truncnorm.cdf(xmid, low, high), 0.9999999970389126)
        assert_almost_equal(stats.truncnorm.sf(xmid, low, high), 2.961048103554866e-09)
    pdf = stats.truncnorm.pdf(xmid, low, high)
    assert_almost_equal(np.log(pdf / expected_pdfs[2]), (xmid + 0.25) / 2)
    xvals = np.linspace(low, high, 11)
    xvals2 = -xvals[::-1]
    assert_almost_equal(stats.truncnorm.cdf(xvals, low, high), stats.truncnorm.sf(xvals2, -high, -low)[::-1])
    assert_almost_equal(stats.truncnorm.sf(xvals, low, high), stats.truncnorm.cdf(xvals2, -high, -low)[::-1])
    assert_almost_equal(stats.truncnorm.pdf(xvals, low, high), stats.truncnorm.pdf(xvals2, -high, -low)[::-1])","for e_target in [[39, 40], [-40, -39]]:
    high = e_target[1]
    low = e_target[0]
    xvals = np.array([-np.inf, low, high, np.inf])
    xmid = (high + low) / 2.0
    cdfs = stats.truncnorm.cdf(xvals, low, high)
    sfs = stats.truncnorm.sf(xvals, low, high)
    pdfs = stats.truncnorm.pdf(xvals, low, high)
    expected_cdfs = np.array([0, 0, 1, 1])
    expected_sfs = np.array([1.0, 1.0, 0.0, 0.0])
    expected_pdfs = np.array([0, 39.0256074, 2.73349092e-16, 0])
    if low < 0:
        expected_pdfs = np.array([0, 2.73349092e-16, 39.0256074, 0])
    assert_almost_equal(cdfs, expected_cdfs)
    assert_almost_equal(sfs, expected_sfs)
    assert_almost_equal(pdfs, expected_pdfs)
    assert_almost_equal(np.log(expected_pdfs[1] / expected_pdfs[2]), low + 0.5)
    pvals = np.array([0, 0.5, 1.0])
    ppfs = stats.truncnorm.ppf(pvals, low, high)
    expected_ppfs = np.array([low, np.sign(low) * 39.01775731, high])
    assert_almost_equal(ppfs, expected_ppfs)
    cdfs = stats.truncnorm.cdf(ppfs, low, high)
    assert_almost_equal(cdfs, pvals)
    if low < 0:
        assert_almost_equal(stats.truncnorm.sf(xmid, low, high), 0.9999999970389126)
        assert_almost_equal(stats.truncnorm.cdf(xmid, low, high), 2.961048103554866e-09)
    else:
        assert_almost_equal(stats.truncnorm.cdf(xmid, low, high), 0.9999999970389126)
        assert_almost_equal(stats.truncnorm.sf(xmid, low, high), 2.961048103554866e-09)
    pdf = stats.truncnorm.pdf(xmid, low, high)
    assert_almost_equal(np.log(pdf / expected_pdfs[2]), (xmid + 0.25) / 2)
    xvals = np.linspace(low, high, 11)
    xvals2 = -xvals[::-1]
    assert_almost_equal(stats.truncnorm.cdf(xvals, low, high), stats.truncnorm.sf(xvals2, -high, -low)[::-1])
    assert_almost_equal(stats.truncnorm.sf(xvals, low, high), stats.truncnorm.cdf(xvals2, -high, -low)[::-1])
    assert_almost_equal(stats.truncnorm.pdf(xvals, low, high), stats.truncnorm.pdf(xvals2, -high, -low)[::-1])"
violent-python3,https://github.com/EONRaider/violent-python3/tree/master/chapter04/geo_print.py,,print_pcap$26,"for (ts, buf) in pcap_file:
    try:
        eth = dpkt.ethernet.Ethernet(buf)
        ip = eth.data
        src = socket.inet_ntoa(ip.src)
        dst = socket.inet_ntoa(ip.dst)
        print(f'[+] Src: {ret_geo_str(src)} --> Dst: {ret_geo_str(dst)}')
    except Exception as e:
        print(f""{'':>3}[-] Exception: {e.__class__.__name__}"")
        pass","for e_target in pcap_file:
    buf = e_target[1]
    ts = e_target[0]
    try:
        eth = dpkt.ethernet.Ethernet(buf)
        ip = eth.data
        src = socket.inet_ntoa(ip.src)
        dst = socket.inet_ntoa(ip.dst)
        print(f'[+] Src: {ret_geo_str(src)} --> Dst: {ret_geo_str(dst)}')
    except Exception as e:
        print(f""{'':>3}[-] Exception: {e.__class__.__name__}"")
        pass"
in-toto,https://github.com/in-toto/in-toto/tree/master/in_toto/runlib.py,,record_artifacts_as_dict$115,"for (prefix_one, prefix_two) in itertools.combinations(lstrip_paths, 2):
    if prefix_one.startswith(prefix_two) or prefix_two.startswith(prefix_one):
        raise in_toto.exceptions.PrefixError(""'{}' and '{}' triggered a left substring error"".format(prefix_one, prefix_two))","for e_target in itertools.combinations(lstrip_paths, 2):
    prefix_two = e_target[1]
    prefix_one = e_target[0]
    if prefix_one.startswith(prefix_two) or prefix_two.startswith(prefix_one):
        raise in_toto.exceptions.PrefixError(""'{}' and '{}' triggered a left substring error"".format(prefix_one, prefix_two))"
data-science-competition,https://github.com/DLLXW/data-science-competition/tree/master/else/婢垛晠鈹堥弶--AI+z閺呴缚鍏樼拹銊/code/scripts/average_model.py,,average_models$13,"for (i, model_path) in enumerate(model_list_path):
    model = torch.load(model_path)
    if i == 0:
        avg_model = model
    else:
        for (k, v) in avg_model.items():
            avg_model[k].mul_(i).add_(model[k]).div_(i + 1)","for e_target in enumerate(model_list_path):
    model_path = e_target[1]
    i = e_target[0]
    model = torch.load(model_path)
    if i == 0:
        avg_model = model
    else:
        for (k, v) in avg_model.items():
            avg_model[k].mul_(i).add_(model[k]).div_(i + 1)"
crowbar,https://github.com/galkan/crowbar/tree/master/lib/main.py,Main,sshkey$520,"for (dirname, dirnames, filenames) in os.walk(self.args.key_file):
    for keyfile in filenames:
        keyfile_path = dirname + '/' + keyfile
        if keyfile.endswith('.pub', 4):
            self.logger.output_file('LOG-SSH: Skipping Public Key - %s' % keyfile_path)
            continue
        pool.add_task(self.sshlogin, ip, port, self.args.username, keyfile_path, self.args.timeout)","for e_target in os.walk(self.args.key_file):
    filenames = e_target[2]
    dirnames = e_target[1]
    dirname = e_target[0]
    for keyfile in filenames:
        keyfile_path = dirname + '/' + keyfile
        if keyfile.endswith('.pub', 4):
            self.logger.output_file('LOG-SSH: Skipping Public Key - %s' % keyfile_path)
            continue
        pool.add_task(self.sshlogin, ip, port, self.args.username, keyfile_path, self.args.timeout)"
Glass,https://github.com/s7ckTeam/Glass/tree/master/lib/option.py,,add_options$41,"for (key, value) in cmdlines:
    confs[key] = value","for e_target in cmdlines:
    value = e_target[1]
    key = e_target[0]
    confs[key] = value"
kivy-designer,https://github.com/kivy/kivy-designer/tree/master/designer/uix/contextual.py,ContextSubMenu,remove_children$487,"for (child, index) in self._list_children:
    self.container.remove_widget(child)","for e_target in self._list_children:
    index = e_target[1]
    child = e_target[0]
    self.container.remove_widget(child)"
airflow,https://github.com/apache/airflow/tree/master/tests/models/test_taskinstance.py,TestTaskInstance,_test_previous_dates_setup$1376,"for (idx, state) in enumerate(scenario):
    new_date = date.add(days=idx)
    ti = get_test_ti(new_date, state)
    ret.append(ti)","for e_target in enumerate(scenario):
    state = e_target[1]
    idx = e_target[0]
    new_date = date.add(days=idx)
    ti = get_test_ti(new_date, state)
    ret.append(ti)"
xarray,https://github.com/pydata/xarray/tree/master/xarray/core/dataset.py,Dataset,_validate_interp_indexers$2241,"for (k, v) in self._validate_indexers(indexers):
    if isinstance(v, Variable):
        if v.ndim == 1:
            yield (k, v.to_index_variable())
        else:
            yield (k, v)
    elif isinstance(v, int):
        yield (k, Variable((), v, attrs=self.coords[k].attrs))
    elif isinstance(v, np.ndarray):
        if v.ndim == 0:
            yield (k, Variable((), v, attrs=self.coords[k].attrs))
        elif v.ndim == 1:
            yield (k, IndexVariable((k,), v, attrs=self.coords[k].attrs))
        else:
            raise AssertionError()
    else:
        raise TypeError(type(v))","for e_target in self._validate_indexers(indexers):
    v = e_target[1]
    k = e_target[0]
    if isinstance(v, Variable):
        if v.ndim == 1:
            yield (k, v.to_index_variable())
        else:
            yield (k, v)
    elif isinstance(v, int):
        yield (k, Variable((), v, attrs=self.coords[k].attrs))
    elif isinstance(v, np.ndarray):
        if v.ndim == 0:
            yield (k, Variable((), v, attrs=self.coords[k].attrs))
        elif v.ndim == 1:
            yield (k, IndexVariable((k,), v, attrs=self.coords[k].attrs))
        else:
            raise AssertionError()
    else:
        raise TypeError(type(v))"
IDArling,https://github.com/IDArlingTeam/IDArling/tree/master/idarling/core/events.py,UserIflagsEvent,__call__$983,"for ((cl_ea, cl_op), f) in self.iflags:
    cl = ida_hexrays.citem_locator_t(cl_ea, cl_op)
    cfunc.set_user_iflags(cl, f)","for e_target in self.iflags:
    f = e_target[1]
    cl_op = e_target[0][1]
    cl_ea = e_target[0][0]
    cl = ida_hexrays.citem_locator_t(cl_ea, cl_op)
    cfunc.set_user_iflags(cl, f)"
hydrus,https://github.com/hydrusnetwork/hydrus/tree/master/hydrus/test/TestClientMigration.py,TestMigration,_test_mappings_list_to_service$587,"for (hash, tags) in expected_data:
    media_result = hashes_to_media_results[hash]
    t_m = media_result.GetTagsManager()
    if content_action == HC.CONTENT_UPDATE_ADD:
        current_tags = t_m.GetCurrent(tag_service_key, ClientTags.TAG_DISPLAY_STORAGE)
        for tag in tags:
            self.assertIn(tag, current_tags)
    elif content_action == HC.CONTENT_UPDATE_DELETE:
        current_tags = t_m.GetCurrent(tag_service_key, ClientTags.TAG_DISPLAY_STORAGE)
        deleted_tags = t_m.GetDeleted(tag_service_key, ClientTags.TAG_DISPLAY_STORAGE)
        for tag in tags:
            self.assertNotIn(tag, current_tags)
            self.assertIn(tag, deleted_tags)
    elif content_action == HC.CONTENT_UPDATE_CLEAR_DELETE_RECORD:
        deleted_tags = t_m.GetDeleted(tag_service_key, ClientTags.TAG_DISPLAY_STORAGE)
        for tag in tags:
            self.assertNotIn(tag, deleted_tags)
    elif content_action == HC.CONTENT_UPDATE_PEND:
        pending_tags = t_m.GetPending(tag_service_key, ClientTags.TAG_DISPLAY_STORAGE)
        for tag in tags:
            self.assertIn(tag, pending_tags)
    elif content_action == HC.CONTENT_UPDATE_PETITION:
        petitioned_tags = t_m.GetPetitioned(tag_service_key, ClientTags.TAG_DISPLAY_STORAGE)
        for tag in tags:
            self.assertIn(tag, petitioned_tags)","for e_target in expected_data:
    tags = e_target[1]
    hash = e_target[0]
    media_result = hashes_to_media_results[hash]
    t_m = media_result.GetTagsManager()
    if content_action == HC.CONTENT_UPDATE_ADD:
        current_tags = t_m.GetCurrent(tag_service_key, ClientTags.TAG_DISPLAY_STORAGE)
        for tag in tags:
            self.assertIn(tag, current_tags)
    elif content_action == HC.CONTENT_UPDATE_DELETE:
        current_tags = t_m.GetCurrent(tag_service_key, ClientTags.TAG_DISPLAY_STORAGE)
        deleted_tags = t_m.GetDeleted(tag_service_key, ClientTags.TAG_DISPLAY_STORAGE)
        for tag in tags:
            self.assertNotIn(tag, current_tags)
            self.assertIn(tag, deleted_tags)
    elif content_action == HC.CONTENT_UPDATE_CLEAR_DELETE_RECORD:
        deleted_tags = t_m.GetDeleted(tag_service_key, ClientTags.TAG_DISPLAY_STORAGE)
        for tag in tags:
            self.assertNotIn(tag, deleted_tags)
    elif content_action == HC.CONTENT_UPDATE_PEND:
        pending_tags = t_m.GetPending(tag_service_key, ClientTags.TAG_DISPLAY_STORAGE)
        for tag in tags:
            self.assertIn(tag, pending_tags)
    elif content_action == HC.CONTENT_UPDATE_PETITION:
        petitioned_tags = t_m.GetPetitioned(tag_service_key, ClientTags.TAG_DISPLAY_STORAGE)
        for tag in tags:
            self.assertIn(tag, petitioned_tags)"
phillip,https://github.com/vladfi1/phillip/tree/master/phillip/embed.py,StructEmbedding,extract$162,"for (field, op) in self.embedding:
    t = tf.slice(embedded, begin + [offset], size + [op.size])
    struct[field] = op.extract(t)
    offset += op.size","for e_target in self.embedding:
    op = e_target[1]
    field = e_target[0]
    t = tf.slice(embedded, begin + [offset], size + [op.size])
    struct[field] = op.extract(t)
    offset += op.size"
open_model_zoo,https://github.com/openvinotoolkit/open_model_zoo/tree/master/tools/accuracy_checker/tests/test_metric_evaluator.py,TestMetric,test_threshold_is_10_by_config$241,"for (_, evaluation_result) in dispatcher.iterate_metrics([annotations], [predictions]):
    assert evaluation_result.name == 'accuracy'
    assert evaluation_result.evaluated_value == 0.0
    assert evaluation_result.reference_value is None
    assert evaluation_result.abs_threshold == 10","for e_target in dispatcher.iterate_metrics([annotations], [predictions]):
    evaluation_result = e_target[1]
    _ = e_target[0]
    assert evaluation_result.name == 'accuracy'
    assert evaluation_result.evaluated_value == 0.0
    assert evaluation_result.reference_value is None
    assert evaluation_result.abs_threshold == 10"
nova,https://github.com/openstack/nova/tree/master/nova/tests/functional/test_servers_provider_tree.py,ProviderTreeTests,update_provider_tree$211,"for (rc, amt) in allocs[self.host_uuid]['resources'].items():
    if rc == 'VCPU':
        rp_uuid = uuids.numa1
    elif rc == 'MEMORY_MB':
        rp_uuid = uuids.numa2
    elif rc == 'DISK_GB':
        rp_uuid = uuids.ssp
    else:
        self.fail('Unexpected resource on compute node: %s=%d' % (rc, amt))
    new_allocs[rp_uuid] = {'resources': {rc: amt}}","for e_target in allocs[self.host_uuid]['resources'].items():
    amt = e_target[1]
    rc = e_target[0]
    if rc == 'VCPU':
        rp_uuid = uuids.numa1
    elif rc == 'MEMORY_MB':
        rp_uuid = uuids.numa2
    elif rc == 'DISK_GB':
        rp_uuid = uuids.ssp
    else:
        self.fail('Unexpected resource on compute node: %s=%d' % (rc, amt))
    new_allocs[rp_uuid] = {'resources': {rc: amt}}"
evalml,https://github.com/alteryx/evalml/tree/master/evalml/tests/automl_tests/test_iterative_algorithm.py,,test_iterative_algorithm_sampling_params$901,"for (score, pipeline) in zip(scores, next_batch):
    algo.add_result(score, pipeline, {'id': algo.pipeline_number})","for e_target in zip(scores, next_batch):
    pipeline = e_target[1]
    score = e_target[0]
    algo.add_result(score, pipeline, {'id': algo.pipeline_number})"
nlp-recipes,https://github.com/microsoft/nlp-recipes/tree/master/utils_nlp/models/bert/sequence_classification_distributed.py,BERTSequenceClassifier,predict$289,"for (i, data) in enumerate(tqdm(test_loader, desc='Iteration')):
    x_batch = data['token_ids']
    x_batch = x_batch.cuda()
    mask_batch = data['input_mask']
    mask_batch = mask_batch.cuda()
    y_batch = data['labels']
    token_type_ids_batch = None
    if 'token_type_ids' in data and data['token_type_ids'] is not None:
        token_type_ids_batch = data['token_type_ids']
        token_type_ids_batch = token_type_ids_batch.cuda()
    with torch.no_grad():
        p_batch = self.model(input_ids=x_batch, token_type_ids=token_type_ids_batch, attention_mask=mask_batch, labels=None)
    preds.append(p_batch.cpu())
    test_labels.append(y_batch)","for e_target in enumerate(tqdm(test_loader, desc='Iteration')):
    data = e_target[1]
    i = e_target[0]
    x_batch = data['token_ids']
    x_batch = x_batch.cuda()
    mask_batch = data['input_mask']
    mask_batch = mask_batch.cuda()
    y_batch = data['labels']
    token_type_ids_batch = None
    if 'token_type_ids' in data and data['token_type_ids'] is not None:
        token_type_ids_batch = data['token_type_ids']
        token_type_ids_batch = token_type_ids_batch.cuda()
    with torch.no_grad():
        p_batch = self.model(input_ids=x_batch, token_type_ids=token_type_ids_batch, attention_mask=mask_batch, labels=None)
    preds.append(p_batch.cpu())
    test_labels.append(y_batch)"
Efficient-Segmentation-Networks,https://github.com/xiaoyufenfei/Efficient-Segmentation-Networks/tree/master/utils/losses/lovasz_losses.py,,iou_binary$34,"for (pred, label) in zip(preds, labels):
    intersection = ((label == 1) & (pred == 1)).sum()
    union = ((label == 1) | (pred == 1) & (label != ignore)).sum()
    if not union:
        iou = EMPTY
    else:
        iou = float(intersection) / float(union)
    ious.append(iou)","for e_target in zip(preds, labels):
    label = e_target[1]
    pred = e_target[0]
    intersection = ((label == 1) & (pred == 1)).sum()
    union = ((label == 1) | (pred == 1) & (label != ignore)).sum()
    if not union:
        iou = EMPTY
    else:
        iou = float(intersection) / float(union)
    ious.append(iou)"
powerline,https://github.com/powerline/powerline/tree/master/tools/colors_find.py,,find_color$29,"for (color, clab) in colors:
    dist = delta_e_cie2000(ulab, clab)
    if dist < cur_distance:
        cur_distance = dist
        cur_color = (color, clab)","for e_target in colors:
    clab = e_target[1]
    color = e_target[0]
    dist = delta_e_cie2000(ulab, clab)
    if dist < cur_distance:
        cur_distance = dist
        cur_color = (color, clab)"
CLUENER2020,https://github.com/CLUEbenchmark/CLUENER2020/tree/master//score.py,,get_f1_score_label$10,"for (pre, gold) in zip(pre_lines, gold_lines):
    pre = pre['label'].get(label, {}).keys()
    gold = gold['label'].get(label, {}).keys()
    for i in pre:
        if i in gold:
            TP += 1
        else:
            FP += 1
    for i in gold:
        if i not in pre:
            FN += 1","for e_target in zip(pre_lines, gold_lines):
    gold = e_target[1]
    pre = e_target[0]
    pre = pre['label'].get(label, {}).keys()
    gold = gold['label'].get(label, {}).keys()
    for i in pre:
        if i in gold:
            TP += 1
        else:
            FP += 1
    for i in gold:
        if i not in pre:
            FN += 1"
pymdown-extensions,https://github.com/facelessuser/pymdown-extensions/tree/master/pymdownx/util.py,PatternSequenceProcessor,handleMatch$277,"for (index, item) in enumerate(self.PATTERNS):
    m1 = item.pattern.match(data, m.start(0))
    if m1:
        start = m1.start(0)
        end = m1.end(0)
        el = self.build_element(m1, item.builder, item.tags, index)
        break","for e_target in enumerate(self.PATTERNS):
    item = e_target[1]
    index = e_target[0]
    m1 = item.pattern.match(data, m.start(0))
    if m1:
        start = m1.start(0)
        end = m1.end(0)
        el = self.build_element(m1, item.builder, item.tags, index)
        break"
pyglet,https://github.com/pyglet/pyglet/tree/master/pyglet/graphics/vertexdomain.py,VertexList,migrate$353,"for (key, old_attribute) in self.domain.attribute_names.items():
    old = old_attribute.get_region(old_attribute.buffer, self.start, self.count)
    new_attribute = domain.attribute_names[key]
    new = new_attribute.get_region(new_attribute.buffer, new_start, self.count)
    new.array[:] = old.array[:]
    new.invalidate()","for e_target in self.domain.attribute_names.items():
    old_attribute = e_target[1]
    key = e_target[0]
    old = old_attribute.get_region(old_attribute.buffer, self.start, self.count)
    new_attribute = domain.attribute_names[key]
    new = new_attribute.get_region(new_attribute.buffer, new_start, self.count)
    new.array[:] = old.array[:]
    new.invalidate()"
bagua,https://github.com/BaguaSys/bagua/tree/master/bagua/torch_api/data_parallel/bagua_distributed.py,BaguaDistributedDataParallel,_bagua_broadcast_optimizer_state$220,"for (param_name, inner_state) in sorted(param_state.items(), key=lambda item: item[0]):
    repeat_param_count[param_name] += 1
    key = '%s_%d' % (str(param_name), repeat_param_count[param_name])
    if isinstance(inner_state, torch.Tensor):
        params.append((key, inner_state))
    else:
        scalars[key] = inner_state
        call_back_param[key] = _state_param_callback(param_id, param_name)","for e_target in sorted(param_state.items(), key=lambda item: item[0]):
    inner_state = e_target[1]
    param_name = e_target[0]
    repeat_param_count[param_name] += 1
    key = '%s_%d' % (str(param_name), repeat_param_count[param_name])
    if isinstance(inner_state, torch.Tensor):
        params.append((key, inner_state))
    else:
        scalars[key] = inner_state
        call_back_param[key] = _state_param_callback(param_id, param_name)"
FedML,https://github.com/FedML-AI/FedML/tree/master/fedml_api/model/cv/resnet56_gkt/resnet_client.py,,resnet8_56$230,"for (k, v) in state_dict.items():
    name = k.replace('module.', '')
    new_state_dict[name] = v","for e_target in state_dict.items():
    v = e_target[1]
    k = e_target[0]
    name = k.replace('module.', '')
    new_state_dict[name] = v"
wagtail,https://github.com/wagtail/wagtail/tree/master/wagtail/admin/views/pages/workflow.py,BaseWorkflowFormView,dispatch$22,"for (name, verbose_name, modal) in actions:
    if name == self.action_name:
        action_available = True
        if modal:
            self.action_modal = True
            self.action_verbose_name = verbose_name","for e_target in actions:
    modal = e_target[2]
    verbose_name = e_target[1]
    name = e_target[0]
    if name == self.action_name:
        action_available = True
        if modal:
            self.action_modal = True
            self.action_verbose_name = verbose_name"
dragonfly,https://github.com/dragonfly/dragonfly/tree/master/dragonfly/opt/multiobjective_gp_bandit.py,MultiObjectiveGPBandit,_compare_two_sets_of_obj_values$155,"for (obj1, obj2) in zip(obj_vals_1, obj_vals_2):
    if obj1 > obj2:
        ret[0] += 1
    elif obj1 == obj2:
        ret[1] += 1
    else:
        ret[2] += 1","for e_target in zip(obj_vals_1, obj_vals_2):
    obj2 = e_target[1]
    obj1 = e_target[0]
    if obj1 > obj2:
        ret[0] += 1
    elif obj1 == obj2:
        ret[1] += 1
    else:
        ret[2] += 1"
nova,https://github.com/openstack/nova/tree/master/nova/tests/unit/compute/test_resource_tracker.py,TestInstanceClaim,assertEqualNUMAHostTopology$1914,"for (exp_cell, got_cell) in zip(expected.cells, got.cells):
    for attr in attrs:
        if getattr(exp_cell, attr) != getattr(got_cell, attr):
            raise AssertionError(""Topologies don't match. Expected: %(expected)s, but got: %(got)s"" % {'expected': expected, 'got': got})","for e_target in zip(expected.cells, got.cells):
    got_cell = e_target[1]
    exp_cell = e_target[0]
    for attr in attrs:
        if getattr(exp_cell, attr) != getattr(got_cell, attr):
            raise AssertionError(""Topologies don't match. Expected: %(expected)s, but got: %(got)s"" % {'expected': expected, 'got': got})"
pgmpy,https://github.com/pgmpy/pgmpy/tree/master/pgmpy/utils/mathext.py,,sample_discrete$91,"for (index, size) in enumerate(counts):
    samples[(weights == unique_weights[index]).all(axis=1)] = np.random.choice(values, size=size, p=_adjusted_weights(unique_weights[index]))","for e_target in enumerate(counts):
    size = e_target[1]
    index = e_target[0]
    samples[(weights == unique_weights[index]).all(axis=1)] = np.random.choice(values, size=size, p=_adjusted_weights(unique_weights[index]))"
ludwig,https://github.com/ludwig-ai/ludwig/tree/master/tests/integration_tests/test_visualization.py,,test_visualization_compare_classifiers_from_prob_csv_output_saved$259,"for (command, viz_pattern) in zip(commands, vis_patterns):
    result = subprocess.run(command)
    figure_cnt = glob.glob(viz_pattern)
    assert 0 == result.returncode
    assert 1 == len(figure_cnt)","for e_target in zip(commands, vis_patterns):
    viz_pattern = e_target[1]
    command = e_target[0]
    result = subprocess.run(command)
    figure_cnt = glob.glob(viz_pattern)
    assert 0 == result.returncode
    assert 1 == len(figure_cnt)"
mypaint,https://github.com/mypaint/mypaint/tree/master/lib/layer/tree.py,RootLayerStack,_get_backdrop_render_spec_for_layer$387,"for (p, layer) in self.walk():
    if path_startswith(p, path):
        seen_srclayer = True
    elif seen_srclayer or isinstance(layer, group.LayerStack):
        backdrop_layers.add(layer)","for e_target in self.walk():
    layer = e_target[1]
    p = e_target[0]
    if path_startswith(p, path):
        seen_srclayer = True
    elif seen_srclayer or isinstance(layer, group.LayerStack):
        backdrop_layers.add(layer)"
animation_nodes,https://github.com/JacquesLucke/animation_nodes/tree/master/animation_nodes/nodes/object/object_instancer.py,ObjectInstancerNode,getOutputObjects$161,"for (i, objectGroup) in enumerate(self.linkedObjects):
    object = objectGroup.object
    if object is None:
        self.linkedObjects.remove(i)
    else:
        objects.append(object)","for e_target in enumerate(self.linkedObjects):
    objectGroup = e_target[1]
    i = e_target[0]
    object = objectGroup.object
    if object is None:
        self.linkedObjects.remove(i)
    else:
        objects.append(object)"
HigherHRNet-Human-Pose-Estimation,https://github.com/HRNet/HigherHRNet-Human-Pose-Estimation/tree/master/lib/models/pose_higher_hrnet.py,PoseHigherResolutionNet,init_weights$522,"for (name, _) in m.named_parameters():
    if name in ['bias']:
        nn.init.constant_(m.bias, 0)","for e_target in m.named_parameters():
    _ = e_target[1]
    name = e_target[0]
    if name in ['bias']:
        nn.init.constant_(m.bias, 0)"
mwparserfromhell,https://github.com/earwig/mwparserfromhell/tree/master/src/mwparserfromhell/smart_list/smart_list.py,SmartList,__delitem__$85,"for (child, (start, stop, _step)) in self._children.values():
    if start > key.start:
        self._children[id(child)][1][0] -= diff
    if stop is not None and stop >= key.stop:
        self._children[id(child)][1][1] -= diff","for e_target in self._children.values():
    _step = e_target[1][2]
    stop = e_target[1][1]
    start = e_target[1][0]
    child = e_target[0]
    if start > key.start:
        self._children[id(child)][1][0] -= diff
    if stop is not None and stop >= key.stop:
        self._children[id(child)][1][1] -= diff"
PaddleDetection,https://github.com/PaddlePaddle/PaddleDetection/tree/master/deploy/pptracking/python/mot/mtmct/utils.py,,get_match$381,"for (i, l) in enumerate(cluster_labels):
    if l in list(cluster_dict.keys()):
        cluster_dict[l].append(i)
    else:
        cluster_dict[l] = [i]","for e_target in enumerate(cluster_labels):
    l = e_target[1]
    i = e_target[0]
    if l in list(cluster_dict.keys()):
        cluster_dict[l].append(i)
    else:
        cluster_dict[l] = [i]"
checkmk,https://github.com/tribe29/checkmk/tree/master/cmk/gui/wato/mkeventd.py,ModeEventConsoleMIBs,_show_mib_table$3058,"for (filename, mib) in sorted(self._load_snmp_mibs(path).items()):
    table.row()
    if is_custom_dir:
        table.cell(html.render_input('_toggle_group', type_='button', class_='checkgroup', onclick='cmk.selection.toggle_all_rows();', value='X'))
        html.checkbox('_c_mib_%s' % filename, deflt=False)
    table.cell(_('Actions'), css='buttons')
    if is_custom_dir:
        delete_url = make_confirm_link(url=make_action_link([('mode', 'mkeventd_mibs'), ('_delete', filename)]), message=_('Do you really want to delete the MIB file <b>%s</b>?') % filename)
        html.icon_button(delete_url, _('Delete this MIB'), 'delete')
    table.cell(_('Filename'), filename)
    table.cell(_('MIB'), mib.get('name', ''))
    table.cell(_('Organization'), mib.get('organization', ''))
    table.cell(_('Size'), cmk.utils.render.fmt_bytes(mib.get('size', 0)), css='number')","for e_target in sorted(self._load_snmp_mibs(path).items()):
    mib = e_target[1]
    filename = e_target[0]
    table.row()
    if is_custom_dir:
        table.cell(html.render_input('_toggle_group', type_='button', class_='checkgroup', onclick='cmk.selection.toggle_all_rows();', value='X'))
        html.checkbox('_c_mib_%s' % filename, deflt=False)
    table.cell(_('Actions'), css='buttons')
    if is_custom_dir:
        delete_url = make_confirm_link(url=make_action_link([('mode', 'mkeventd_mibs'), ('_delete', filename)]), message=_('Do you really want to delete the MIB file <b>%s</b>?') % filename)
        html.icon_button(delete_url, _('Delete this MIB'), 'delete')
    table.cell(_('Filename'), filename)
    table.cell(_('MIB'), mib.get('name', ''))
    table.cell(_('Organization'), mib.get('organization', ''))
    table.cell(_('Size'), cmk.utils.render.fmt_bytes(mib.get('size', 0)), css='number')"
django-silk,https://github.com/jazzband/django-silk/tree/master/project/tests/test_view_requests.py,TestGetObjects,assertSorted$66,"for (idx, r) in enumerate(objects):
    try:
        nxt = objects[idx + 1]
        self.assertGreaterEqual(getattr(r, sort_field), getattr(nxt, sort_field))
    except IndexError:
        pass","for e_target in enumerate(objects):
    r = e_target[1]
    idx = e_target[0]
    try:
        nxt = objects[idx + 1]
        self.assertGreaterEqual(getattr(r, sort_field), getattr(nxt, sort_field))
    except IndexError:
        pass"
KiKit,https://github.com/yaqwsx/KiKit/tree/master/kikit/plugin/panelize.py,,presetDifferential$15,"for (key, value) in section.items():
    if key not in sourceSection or str(sourceSection[key]).lower() != str(value).lower():
        updateKeys[key] = value","for e_target in section.items():
    value = e_target[1]
    key = e_target[0]
    if key not in sourceSection or str(sourceSection[key]).lower() != str(value).lower():
        updateKeys[key] = value"
FeatherNets_Face-Anti-spoofing-Attack-Detection-Challenge-CVPR2019,https://github.com/SoftwareGift/FeatherNets_Face-Anti-spoofing-Attack-Detection-Challenge-CVPR2019/tree/master//roc.py,,cal_metric$11,"for (i, (key, value)) in enumerate(FPRs.items()):
    index = np.argwhere(scale == value)
    score = y[index]
    TPRs[key] = float(np.squeeze(score))","for e_target in enumerate(FPRs.items()):
    value = e_target[1][1]
    key = e_target[1][0]
    i = e_target[0]
    index = np.argwhere(scale == value)
    score = y[index]
    TPRs[key] = float(np.squeeze(score))"
sympy,https://github.com/sympy/sympy/tree/master/sympy/core/mul.py,Mul,_gather$462,"for (di, li) in d.items():
    d[di] = Add(*li)","for e_target in d.items():
    li = e_target[1]
    di = e_target[0]
    d[di] = Add(*li)"
scattertext,https://github.com/JasonKessler/scattertext/tree/master/scattertext/CorpusFromFeatureDict.py,CorpusFromFeatureDict,_add_to_x_factory$80,"for (feat, count) in row[self._feature_col].items():
    feat_idx = self._term_idx_store.getidx(feat)
    self._X_factory[row.name, feat_idx] = count","for e_target in row[self._feature_col].items():
    count = e_target[1]
    feat = e_target[0]
    feat_idx = self._term_idx_store.getidx(feat)
    self._X_factory[row.name, feat_idx] = count"
networkx,https://github.com/networkx/networkx/tree/master/networkx/algorithms/traversal/depth_first_search.py,,dfs_successors$192,"for (s, t) in dfs_edges(G, source=source, depth_limit=depth_limit):
    d[s].append(t)","for e_target in dfs_edges(G, source=source, depth_limit=depth_limit):
    t = e_target[1]
    s = e_target[0]
    d[s].append(t)"
veusz,https://github.com/veusz/veusz/tree/master/veusz/dialogs/reloaddata.py,ReloadData,reloadData$102,"for (lname, link) in linked:
    lines.append('')
    lines.append(_('Linked to %s') % lname)
    for var in sorted(datasets):
        ds = self.document.data[var]
        if ds.linked is link:
            lines.append(' %s: %s' % (var, ds.description()))","for e_target in linked:
    link = e_target[1]
    lname = e_target[0]
    lines.append('')
    lines.append(_('Linked to %s') % lname)
    for var in sorted(datasets):
        ds = self.document.data[var]
        if ds.linked is link:
            lines.append(' %s: %s' % (var, ds.description()))"
mysql-connector-python,https://github.com/mysql/mysql-connector-python/tree/master/lib/mysql/connector/connection_cext.py,CMySQLConnection,prepare_for_mysql$647,"for (key, value) in params.items():
    result[key] = self.converter.quote(self.converter.escape(self.converter.to_mysql(value)))","for e_target in params.items():
    value = e_target[1]
    key = e_target[0]
    result[key] = self.converter.quote(self.converter.escape(self.converter.to_mysql(value)))"
hub,https://github.com/pytorch/hub/tree/master/tensorflow_hub/tools/make_image_classifier/make_image_classifier_test.py,MakeImageClassifierTest,_write_cmy_dataset$57,"for (class_name, rgb) in self.CMY_NAMES_AND_RGB_VALUES:
    class_subdir = os.path.join(path, class_name)
    os.mkdir(class_subdir)
    for i in range(self.IMAGES_PER_CLASS):
        _write_filled_jpeg_file(os.path.join(class_subdir, 'img_%s_%03d.jpeg' % (class_name, i)), rgb, self.IMAGE_SIZE)","for e_target in self.CMY_NAMES_AND_RGB_VALUES:
    rgb = e_target[1]
    class_name = e_target[0]
    class_subdir = os.path.join(path, class_name)
    os.mkdir(class_subdir)
    for i in range(self.IMAGES_PER_CLASS):
        _write_filled_jpeg_file(os.path.join(class_subdir, 'img_%s_%03d.jpeg' % (class_name, i)), rgb, self.IMAGE_SIZE)"
micropython-lib,https://github.com/micropython/micropython-lib/tree/master/python-stdlib/email.internal/email/_policybase.py,_PolicyBase,clone$61,"for (attr, value) in self.__dict__.items():
    object.__setattr__(newpolicy, attr, value)","for e_target in self.__dict__.items():
    value = e_target[1]
    attr = e_target[0]
    object.__setattr__(newpolicy, attr, value)"
borb,https://github.com/jorisschellekens/borb/tree/master//main.py,,_extract_files$90,"for (_, content) in doc.get_embedded_files().items():
    i += 1
    with open(output_dir / ('image_%d.jpg' % i), 'wb') as file_handle:
        file_handle.write(content)","for e_target in doc.get_embedded_files().items():
    content = e_target[1]
    _ = e_target[0]
    i += 1
    with open(output_dir / ('image_%d.jpg' % i), 'wb') as file_handle:
        file_handle.write(content)"
checkmk,https://github.com/tribe29/checkmk/tree/master/tests/unit/test_pipfile.py,,get_unused_dependencies$199,"for (packagename, import_names) in pipfile_libs.items():
    if set(import_names).isdisjoint(imported_libs):
        yield packagename","for e_target in pipfile_libs.items():
    import_names = e_target[1]
    packagename = e_target[0]
    if set(import_names).isdisjoint(imported_libs):
        yield packagename"
jc,https://github.com/kellyjonbrazil/jc/tree/master/jc/parsers/finger.py,,parse$157,"for (index, line) in enumerate(second_half):
    dt = re.search(pattern, line)
    if dt:
        if dt.group(1) and dt.group(2):
            raw_output[index]['login_time'] = dt.group(1).strip() + ' ' + dt.group(2).strip()
        if dt.group(3):
            raw_output[index]['details'] = dt.group(3).strip()","for e_target in enumerate(second_half):
    line = e_target[1]
    index = e_target[0]
    dt = re.search(pattern, line)
    if dt:
        if dt.group(1) and dt.group(2):
            raw_output[index]['login_time'] = dt.group(1).strip() + ' ' + dt.group(2).strip()
        if dt.group(3):
            raw_output[index]['details'] = dt.group(3).strip()"
pororo,https://github.com/kakaobrain/pororo/tree/master/pororo/models/collocate.py,Collocate,parse_results$70,"for (pos, cols) in entry.items():
    _entry[pos] = dict()
    for (col_pos, collocates) in cols.items():
        collocate2cnt = Counter(collocates)
        _collocates = []
        for (collocate, cnt) in collocate2cnt.most_common(len(collocate2cnt)):
            if cnt > min_cnt:
                _collocates.append((collocate, cnt))
        _entry[pos][col_pos] = _collocates","for e_target in entry.items():
    cols = e_target[1]
    pos = e_target[0]
    _entry[pos] = dict()
    for (col_pos, collocates) in cols.items():
        collocate2cnt = Counter(collocates)
        _collocates = []
        for (collocate, cnt) in collocate2cnt.most_common(len(collocate2cnt)):
            if cnt > min_cnt:
                _collocates.append((collocate, cnt))
        _entry[pos][col_pos] = _collocates"
plantcv,https://github.com/danforthcenter/plantcv/tree/master/plantcv/plantcv/visualize/obj_sizes.py,,obj_sizes$12,"for (c, value) in enumerate(area_vals):
    text = '{:.0f}'.format(value)
    w = label_coord_x[c]
    h = label_coord_y[c]
    cv2.putText(img=plotting_img, text=text, org=(w, h), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=params.text_size, color=(150, 150, 150), thickness=params.text_thickness)","for e_target in enumerate(area_vals):
    value = e_target[1]
    c = e_target[0]
    text = '{:.0f}'.format(value)
    w = label_coord_x[c]
    h = label_coord_y[c]
    cv2.putText(img=plotting_img, text=text, org=(w, h), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=params.text_size, color=(150, 150, 150), thickness=params.text_thickness)"
model-analysis,https://github.com/tensorflow/model-analysis/tree/master/tensorflow_model_analysis/evaluators/metrics_plots_and_validations_evaluator_test.py,MetricsPlotsAndValidationsEvaluatorTest,check_metrics$658,"for (slice_key, value) in got:
    slices[slice_key] = value","for e_target in got:
    value = e_target[1]
    slice_key = e_target[0]
    slices[slice_key] = value"
albert_zh,https://github.com/brightmart/albert_zh/tree/master//create_pretraining_data.py,,create_masked_lm_predictions$498,"for (i, token) in enumerate(tokens):
    if token == '[CLS]' or token == '[SEP]':
        continue
    if FLAGS.do_whole_word_mask and len(cand_indexes) >= 1 and token.startswith('##'):
        cand_indexes[-1].append(i)
    else:
        cand_indexes.append([i])","for e_target in enumerate(tokens):
    token = e_target[1]
    i = e_target[0]
    if token == '[CLS]' or token == '[SEP]':
        continue
    if FLAGS.do_whole_word_mask and len(cand_indexes) >= 1 and token.startswith('##'):
        cand_indexes[-1].append(i)
    else:
        cand_indexes.append([i])"
picard,https://github.com/metabrainz/picard/tree/master/picard/ui/logview.py,VerbosityMenu,__init__$139,"for (level, feat) in log.levels_features.items():
    action = QtWidgets.QAction(_(feat.name), self)
    action.setCheckable(True)
    action.triggered.connect(partial(self.verbosity_changed.emit, level))
    self.action_group.addAction(action)
    self.addAction(action)
    self.actions[level] = action","for e_target in log.levels_features.items():
    feat = e_target[1]
    level = e_target[0]
    action = QtWidgets.QAction(_(feat.name), self)
    action.setCheckable(True)
    action.triggered.connect(partial(self.verbosity_changed.emit, level))
    self.action_group.addAction(action)
    self.addAction(action)
    self.actions[level] = action"
intelmq,https://github.com/certtools/intelmq/tree/master/intelmq/bin/intelmqctl.py,IntelMQController,clear_queue$730,"for (key, value) in self._pipeline_configuration().items():
    if 'source_queue' in value:
        queues.add(value['source_queue'])
        if pipeline.has_internal_queues:
            queues.add(value['source_queue'] + '-internal')
    if 'destination_queues' in value:
        queues.update(value['destination_queues'])","for e_target in self._pipeline_configuration().items():
    value = e_target[1]
    key = e_target[0]
    if 'source_queue' in value:
        queues.add(value['source_queue'])
        if pipeline.has_internal_queues:
            queues.add(value['source_queue'] + '-internal')
    if 'destination_queues' in value:
        queues.update(value['destination_queues'])"
galaxy,https://github.com/ansible/galaxy/tree/master/lib/galaxy/model/custom_types.py,,sizeof$313,"for (typ, handler) in all_handlers.items():
    if isinstance(o, typ):
        s += sum(map(sizeof, handler(o)))
        break","for e_target in all_handlers.items():
    handler = e_target[1]
    typ = e_target[0]
    if isinstance(o, typ):
        s += sum(map(sizeof, handler(o)))
        break"
deepvoice3_pytorch,https://github.com/r9y9/deepvoice3_pytorch/tree/master/vctk_preprocess/extract_feats.py,,save_numpy_features$868,"for (n, ttup) in enumerate(text_tups):
    if ttup[0] in file_list:
        new_text_tups.append(ttup)","for e_target in enumerate(text_tups):
    ttup = e_target[1]
    n = e_target[0]
    if ttup[0] in file_list:
        new_text_tups.append(ttup)"
pydantic,https://github.com/samuelcolvin/pydantic/tree/master/pydantic/main.py,,create_model$888,"for (f_name, f_def) in field_definitions.items():
    if not is_valid_field(f_name):
        warnings.warn(f'fields may not start with an underscore, ignoring ""{f_name}""', RuntimeWarning)
    if isinstance(f_def, tuple):
        try:
            (f_annotation, f_value) = f_def
        except ValueError as e:
            raise ConfigError('field definitions should either be a tuple of (<type>, <default>) or just a default value, unfortunately this means tuples as default values are not allowed') from e
    else:
        (f_annotation, f_value) = (None, f_def)
    if f_annotation:
        annotations[f_name] = f_annotation
    fields[f_name] = f_value","for e_target in field_definitions.items():
    f_def = e_target[1]
    f_name = e_target[0]
    if not is_valid_field(f_name):
        warnings.warn(f'fields may not start with an underscore, ignoring ""{f_name}""', RuntimeWarning)
    if isinstance(f_def, tuple):
        try:
            (f_annotation, f_value) = f_def
        except ValueError as e:
            raise ConfigError('field definitions should either be a tuple of (<type>, <default>) or just a default value, unfortunately this means tuples as default values are not allowed') from e
    else:
        (f_annotation, f_value) = (None, f_def)
    if f_annotation:
        annotations[f_name] = f_annotation
    fields[f_name] = f_value"
espnet,https://github.com/espnet/espnet/tree/master/test/espnet2/bin/test_asr_inference_k2.py,,test_k2Speech2Text$95,"for (text, token, token_int, score) in results:
    assert isinstance(text, str)
    assert isinstance(token[0], str)
    assert isinstance(token_int[0], int)
    assert isinstance(score, float)","for e_target in results:
    score = e_target[3]
    token_int = e_target[2]
    token = e_target[1]
    text = e_target[0]
    assert isinstance(text, str)
    assert isinstance(token[0], str)
    assert isinstance(token_int[0], int)
    assert isinstance(score, float)"
mindmeld,https://github.com/cisco/mindmeld/tree/master/tests/test_resource_loader.py,,_run_tests_on_pql$48,"for (q1, q2) in zip(_reversed, iterator):
    assert q1 == q2","for e_target in zip(_reversed, iterator):
    q2 = e_target[1]
    q1 = e_target[0]
    assert q1 == q2"
DAIN,https://github.com/baowenbo/DAIN/tree/master/MegaDepth/util/visualizer.py,Visualizer,display_current_results$27,"for (label, image_numpy) in visuals.items():
    img_path = os.path.join(self.img_dir, 'epoch%.3d_%s.png' % (epoch, label))
    util.save_image(image_numpy, img_path)","for e_target in visuals.items():
    image_numpy = e_target[1]
    label = e_target[0]
    img_path = os.path.join(self.img_dir, 'epoch%.3d_%s.png' % (epoch, label))
    util.save_image(image_numpy, img_path)"
sympy,https://github.com/sympy/sympy/tree/master/sympy/simplify/hyperexpand.py,,try_lerchphi$1742,"for (n, b) in enumerate([S.One] + list(deriv.keys())):
    trans[b] = n","for e_target in enumerate([S.One] + list(deriv.keys())):
    b = e_target[1]
    n = e_target[0]
    trans[b] = n"
pytorch_geometric,https://github.com/pyg-team/pytorch_geometric/tree/master/examples/jit/gat.py,,test$49,"for (_, mask) in data('train_mask', 'val_mask', 'test_mask'):
    pred = logits[mask].max(1)[1]
    acc = pred.eq(data.y[mask]).sum().item() / mask.sum().item()
    accs.append(acc)","for e_target in data('train_mask', 'val_mask', 'test_mask'):
    mask = e_target[1]
    _ = e_target[0]
    pred = logits[mask].max(1)[1]
    acc = pred.eq(data.y[mask]).sum().item() / mask.sum().item()
    accs.append(acc)"
ansible-modules-core,https://github.com/ansible/ansible-modules-core/tree/master/utilities/logic/async_wrapper.py,,_filter_non_json_lines$76,"for (reverse_end_offset, line) in enumerate(reversed(lines)):
    if line.strip().endswith(endchar):
        break
else:
    raise ValueError('No end of json char found')","for e_target in enumerate(reversed(lines)):
    line = e_target[1]
    reverse_end_offset = e_target[0]
    if line.strip().endswith(endchar):
        break
else:
    raise ValueError('No end of json char found')"
imgaug,https://github.com/aleju/imgaug/tree/master/imgaug/augmenters/contrast.py,AllChannelsHistogramEqualization,_augment_batch_$1380,"for (i, image) in enumerate(images):
    if image.size == 0:
        continue
    image_warped = [cv2.equalizeHist(_normalize_cv2_input_arr_(image[..., c])) for c in sm.xrange(image.shape[2])]
    image_warped = np.array(image_warped, dtype=image_warped[0].dtype)
    image_warped = image_warped.transpose((1, 2, 0))
    batch.images[i] = image_warped","for e_target in enumerate(images):
    image = e_target[1]
    i = e_target[0]
    if image.size == 0:
        continue
    image_warped = [cv2.equalizeHist(_normalize_cv2_input_arr_(image[..., c])) for c in sm.xrange(image.shape[2])]
    image_warped = np.array(image_warped, dtype=image_warped[0].dtype)
    image_warped = image_warped.transpose((1, 2, 0))
    batch.images[i] = image_warped"
node-gyp,https://github.com/nodejs/node-gyp/tree/master/gyp/pylib/gyp/input.py,,ExpandWildcardDependencies$1528,"for (target, target_dict) in targets.items():
    target_build_file = gyp.common.BuildFile(target)
    for dependency_key in dependency_sections:
        dependencies = target_dict.get(dependency_key, [])
        index = 0
        while index < len(dependencies):
            (dependency_build_file, dependency_target, dependency_toolset) = gyp.common.ParseQualifiedTarget(dependencies[index])
            if dependency_target != '*' and dependency_toolset != '*':
                index = index + 1
                continue
            if dependency_build_file == target_build_file:
                raise GypError('Found wildcard in ' + dependency_key + ' of ' + target + ' referring to same build file')
            del dependencies[index]
            index = index - 1
            dependency_target_dicts = data[dependency_build_file]['targets']
            for dependency_target_dict in dependency_target_dicts:
                if int(dependency_target_dict.get('suppress_wildcard', False)):
                    continue
                dependency_target_name = dependency_target_dict['target_name']
                if dependency_target != '*' and dependency_target != dependency_target_name:
                    continue
                dependency_target_toolset = dependency_target_dict['toolset']
                if dependency_toolset != '*' and dependency_toolset != dependency_target_toolset:
                    continue
                dependency = gyp.common.QualifiedTarget(dependency_build_file, dependency_target_name, dependency_target_toolset)
                index = index + 1
                dependencies.insert(index, dependency)
            index = index + 1","for e_target in targets.items():
    target_dict = e_target[1]
    target = e_target[0]
    target_build_file = gyp.common.BuildFile(target)
    for dependency_key in dependency_sections:
        dependencies = target_dict.get(dependency_key, [])
        index = 0
        while index < len(dependencies):
            (dependency_build_file, dependency_target, dependency_toolset) = gyp.common.ParseQualifiedTarget(dependencies[index])
            if dependency_target != '*' and dependency_toolset != '*':
                index = index + 1
                continue
            if dependency_build_file == target_build_file:
                raise GypError('Found wildcard in ' + dependency_key + ' of ' + target + ' referring to same build file')
            del dependencies[index]
            index = index - 1
            dependency_target_dicts = data[dependency_build_file]['targets']
            for dependency_target_dict in dependency_target_dicts:
                if int(dependency_target_dict.get('suppress_wildcard', False)):
                    continue
                dependency_target_name = dependency_target_dict['target_name']
                if dependency_target != '*' and dependency_target != dependency_target_name:
                    continue
                dependency_target_toolset = dependency_target_dict['toolset']
                if dependency_toolset != '*' and dependency_toolset != dependency_target_toolset:
                    continue
                dependency = gyp.common.QualifiedTarget(dependency_build_file, dependency_target_name, dependency_target_toolset)
                index = index + 1
                dependencies.insert(index, dependency)
            index = index + 1"
data-driven-web-apps-with-flask,https://github.com/talkpython/data-driven-web-apps-with-flask/tree/master/app/ch12-forms/starter/pypi_org/bin/load_data.py,,load_package$225,"for (email, name) in maintainers_lookup.items():
    user = user_lookup.get(email)
    if not user:
        continue
    m = Maintainer()
    m.package_id = p.id
    m.user_id = user.id
    maintainers.append(m)","for e_target in maintainers_lookup.items():
    name = e_target[1]
    email = e_target[0]
    user = user_lookup.get(email)
    if not user:
        continue
    m = Maintainer()
    m.package_id = p.id
    m.user_id = user.id
    maintainers.append(m)"
DeepPavlov,https://github.com/deepmipt/DeepPavlov/tree/master/deeppavlov/models/ranking/sequential_matching_network.py,SMNNetwork,_init_graph$87,"for (utterance_embeddings, utterance_len) in zip(all_utterance_embeddings, all_utterance_len):
    matrix1 = tf.matmul(utterance_embeddings, response_embeddings)
    (utterance_GRU_embeddings, _) = tf.nn.dynamic_rnn(sentence_GRU, utterance_embeddings, sequence_length=utterance_len, dtype=tf.float32, scope='sentence_GRU')
    matrix2 = tf.einsum('aij,jk->aik', utterance_GRU_embeddings, A_matrix)
    matrix2 = tf.matmul(matrix2, response_GRU_embeddings)
    matrix = tf.stack([matrix1, matrix2], axis=3, name='matrix_stack')
    conv_layer = tf.layers.conv2d(matrix, filters=8, kernel_size=(3, 3), padding='VALID', kernel_initializer=tf.contrib.keras.initializers.he_normal(), activation=tf.nn.relu, reuse=reuse, name='conv')
    pooling_layer = tf.layers.max_pooling2d(conv_layer, (3, 3), strides=(3, 3), padding='VALID', name='max_pooling')
    matching_vector = tf.layers.dense(tf.contrib.layers.flatten(pooling_layer), 50, kernel_initializer=tf.contrib.layers.xavier_initializer(), activation=tf.tanh, reuse=reuse, name='matching_v')
    if not reuse:
        reuse = True
    matching_vectors.append(matching_vector)","for e_target in zip(all_utterance_embeddings, all_utterance_len):
    utterance_len = e_target[1]
    utterance_embeddings = e_target[0]
    matrix1 = tf.matmul(utterance_embeddings, response_embeddings)
    (utterance_GRU_embeddings, _) = tf.nn.dynamic_rnn(sentence_GRU, utterance_embeddings, sequence_length=utterance_len, dtype=tf.float32, scope='sentence_GRU')
    matrix2 = tf.einsum('aij,jk->aik', utterance_GRU_embeddings, A_matrix)
    matrix2 = tf.matmul(matrix2, response_GRU_embeddings)
    matrix = tf.stack([matrix1, matrix2], axis=3, name='matrix_stack')
    conv_layer = tf.layers.conv2d(matrix, filters=8, kernel_size=(3, 3), padding='VALID', kernel_initializer=tf.contrib.keras.initializers.he_normal(), activation=tf.nn.relu, reuse=reuse, name='conv')
    pooling_layer = tf.layers.max_pooling2d(conv_layer, (3, 3), strides=(3, 3), padding='VALID', name='max_pooling')
    matching_vector = tf.layers.dense(tf.contrib.layers.flatten(pooling_layer), 50, kernel_initializer=tf.contrib.layers.xavier_initializer(), activation=tf.tanh, reuse=reuse, name='matching_v')
    if not reuse:
        reuse = True
    matching_vectors.append(matching_vector)"
speechbrain,https://github.com/speechbrain/speechbrain/tree/master/speechbrain/utils/metric_stats.py,,sequence_evaluation$175,"for (p, t) in zip(predict, target):
    score = metric(p, t)
    scores.append(score)","for e_target in zip(predict, target):
    t = e_target[1]
    p = e_target[0]
    score = metric(p, t)
    scores.append(score)"
chainer-chemistry,https://github.com/chainer/chainer-chemistry/tree/master/examples/molnet_wle/predict_molnet_wle.py,,main$69,"for (k, v) in rocauc_result.items():
    eval_result[k] = float(v)","for e_target in rocauc_result.items():
    v = e_target[1]
    k = e_target[0]
    eval_result[k] = float(v)"
open-paperless,https://github.com/zhoubear/open-paperless/tree/master/mayan/apps/common/generics.py,SingleObjectEditView,get_object$467,"for (key, value) in self.get_instance_extra_data().items():
    setattr(obj, key, value)","for e_target in self.get_instance_extra_data().items():
    value = e_target[1]
    key = e_target[0]
    setattr(obj, key, value)"
trankit,https://github.com/nlp-uoregon/trankit/tree/master/trankit/adapter_transformers/trainer.py,Trainer,train$364,"for (step, inputs) in enumerate(epoch_iterator):
    if steps_trained_in_current_epoch > 0:
        steps_trained_in_current_epoch -= 1
        continue
    tr_loss += self._training_step(model, inputs, optimizer)
    if (step + 1) % self.args.gradient_accumulation_steps == 0 or (len(epoch_iterator) <= self.args.gradient_accumulation_steps and step + 1 == len(epoch_iterator)):
        if hasattr(self.model.config, 'adapter_fusion') and self.model.config.adapter_fusion['regularization']:
            fusion_reg_loss = get_fusion_regularization_loss(self.model)
            fusion_reg_loss.backward()
        if self.args.fp16:
            torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), self.args.max_grad_norm)
        else:
            torch.nn.utils.clip_grad_norm_(model.parameters(), self.args.max_grad_norm)
        if is_tpu_available():
            xm.optimizer_step(optimizer)
        else:
            optimizer.step()
        scheduler.step()
        model.zero_grad()
        self.global_step += 1
        self.epoch = epoch + (step + 1) / len(epoch_iterator)
        if self.args.logging_steps > 0 and self.global_step % self.args.logging_steps == 0 or (self.global_step == 1 and self.args.logging_first_step):
            logs: Dict[str, float] = {}
            logs['loss'] = (tr_loss - logging_loss) / self.args.logging_steps
            logs['learning_rate'] = scheduler.get_last_lr()[0] if version.parse(torch.__version__) >= version.parse('1.4') else scheduler.get_lr()[0]
            logging_loss = tr_loss
            self._log(logs)
            if self.args.evaluate_during_training:
                self.evaluate()
        if self.args.save_steps > 0 and self.global_step % self.args.save_steps == 0:
            if hasattr(model, 'module'):
                assert model.module is self.model
            else:
                assert model is self.model
            output_dir = os.path.join(self.args.output_dir, f'{PREFIX_CHECKPOINT_DIR}-{self.global_step}')
            self.save_model(output_dir)
            if self.is_world_master():
                self._rotate_checkpoints()
            if is_tpu_available():
                xm.rendezvous('saving_optimizer_states')
                xm.save(optimizer.state_dict(), os.path.join(output_dir, 'optimizer.pt'))
                xm.save(scheduler.state_dict(), os.path.join(output_dir, 'scheduler.pt'))
            elif self.is_world_master():
                torch.save(optimizer.state_dict(), os.path.join(output_dir, 'optimizer.pt'))
                torch.save(scheduler.state_dict(), os.path.join(output_dir, 'scheduler.pt'))
    if self.args.max_steps > 0 and self.global_step > self.args.max_steps:
        epoch_iterator.close()
        break","for e_target in enumerate(epoch_iterator):
    inputs = e_target[1]
    step = e_target[0]
    if steps_trained_in_current_epoch > 0:
        steps_trained_in_current_epoch -= 1
        continue
    tr_loss += self._training_step(model, inputs, optimizer)
    if (step + 1) % self.args.gradient_accumulation_steps == 0 or (len(epoch_iterator) <= self.args.gradient_accumulation_steps and step + 1 == len(epoch_iterator)):
        if hasattr(self.model.config, 'adapter_fusion') and self.model.config.adapter_fusion['regularization']:
            fusion_reg_loss = get_fusion_regularization_loss(self.model)
            fusion_reg_loss.backward()
        if self.args.fp16:
            torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), self.args.max_grad_norm)
        else:
            torch.nn.utils.clip_grad_norm_(model.parameters(), self.args.max_grad_norm)
        if is_tpu_available():
            xm.optimizer_step(optimizer)
        else:
            optimizer.step()
        scheduler.step()
        model.zero_grad()
        self.global_step += 1
        self.epoch = epoch + (step + 1) / len(epoch_iterator)
        if self.args.logging_steps > 0 and self.global_step % self.args.logging_steps == 0 or (self.global_step == 1 and self.args.logging_first_step):
            logs: Dict[str, float] = {}
            logs['loss'] = (tr_loss - logging_loss) / self.args.logging_steps
            logs['learning_rate'] = scheduler.get_last_lr()[0] if version.parse(torch.__version__) >= version.parse('1.4') else scheduler.get_lr()[0]
            logging_loss = tr_loss
            self._log(logs)
            if self.args.evaluate_during_training:
                self.evaluate()
        if self.args.save_steps > 0 and self.global_step % self.args.save_steps == 0:
            if hasattr(model, 'module'):
                assert model.module is self.model
            else:
                assert model is self.model
            output_dir = os.path.join(self.args.output_dir, f'{PREFIX_CHECKPOINT_DIR}-{self.global_step}')
            self.save_model(output_dir)
            if self.is_world_master():
                self._rotate_checkpoints()
            if is_tpu_available():
                xm.rendezvous('saving_optimizer_states')
                xm.save(optimizer.state_dict(), os.path.join(output_dir, 'optimizer.pt'))
                xm.save(scheduler.state_dict(), os.path.join(output_dir, 'scheduler.pt'))
            elif self.is_world_master():
                torch.save(optimizer.state_dict(), os.path.join(output_dir, 'optimizer.pt'))
                torch.save(scheduler.state_dict(), os.path.join(output_dir, 'scheduler.pt'))
    if self.args.max_steps > 0 and self.global_step > self.args.max_steps:
        epoch_iterator.close()
        break"
moto,https://github.com/spulec/moto/tree/master/moto/ds/validations.py,,validate_args$10,"for (fieldname, value) in validators:
    msg = validation_map[fieldname](value)
    if msg:
        err_msgs.append((fieldname, value, msg))","for e_target in validators:
    value = e_target[1]
    fieldname = e_target[0]
    msg = validation_map[fieldname](value)
    if msg:
        err_msgs.append((fieldname, value, msg))"
fonttools,https://github.com/fonttools/fonttools/tree/master/Lib/fontTools/ttLib/tables/TupleVariation.py,,decompileTupleVariation_$710,"for (p, x, y) in zip(points, deltas_x, deltas_y):
    if 0 <= p < pointCount:
        deltas[p] = (x, y)","for e_target in zip(points, deltas_x, deltas_y):
    y = e_target[2]
    x = e_target[1]
    p = e_target[0]
    if 0 <= p < pointCount:
        deltas[p] = (x, y)"
TensorNetwork,https://github.com/google/TensorNetwork/tree/master/tensornetwork/tests/testing_utils.py,,assert_allclose$104,"for (c1, c2) in zip(exp.flat_charges, act.flat_charges):
    if not charge_equal(c1, c2):
        raise ValueError('expected charges differ from actual charges')","for e_target in zip(exp.flat_charges, act.flat_charges):
    c2 = e_target[1]
    c1 = e_target[0]
    if not charge_equal(c1, c2):
        raise ValueError('expected charges differ from actual charges')"
salt,https://github.com/saltstack/salt/tree/master/salt/states/bigip.py,,manage_monitor$2812,"for (key, value) in kwargs.items():
    params[key] = value","for e_target in kwargs.items():
    value = e_target[1]
    key = e_target[0]
    params[key] = value"
animation_nodes,https://github.com/JacquesLucke/animation_nodes/tree/master/animation_nodes/data_structures/struct.py,ANStruct,copyValues$5,"for ((dataType, name), value) in self.items():
    s[dataType, name] = getCopyFunction(dataType)(value)","for e_target in self.items():
    value = e_target[1]
    name = e_target[0][1]
    dataType = e_target[0][0]
    s[dataType, name] = getCopyFunction(dataType)(value)"
DetectoRS,https://github.com/joe-siyuan-qiao/DetectoRS/tree/master/mmdet/models/backbones/resnext.py,ResNeXt,__init__$206,"for (i, num_blocks) in enumerate(self.stage_blocks):
    stride = self.strides[i]
    dilation = self.dilations[i]
    dcn = self.dcn if self.stage_with_dcn[i] else None
    gcb = self.gcb if self.stage_with_gcb[i] else None
    sac = self.sac if self.stage_with_sac[i] else None
    rfp = self.rfp if self.stage_with_rfp[i] else None
    planes = 64 * 2 ** i
    res_layer = make_res_layer(self.block, self.inplanes, planes, num_blocks, stride=stride, dilation=dilation, groups=self.groups, base_width=self.base_width, style=self.style, with_cp=self.with_cp, conv_cfg=self.conv_cfg, norm_cfg=self.norm_cfg, dcn=dcn, gcb=gcb, sac=sac, rfp=rfp)
    self.inplanes = planes * self.block.expansion
    layer_name = 'layer{}'.format(i + 1)
    self.add_module(layer_name, res_layer)
    self.res_layers.append(layer_name)","for e_target in enumerate(self.stage_blocks):
    num_blocks = e_target[1]
    i = e_target[0]
    stride = self.strides[i]
    dilation = self.dilations[i]
    dcn = self.dcn if self.stage_with_dcn[i] else None
    gcb = self.gcb if self.stage_with_gcb[i] else None
    sac = self.sac if self.stage_with_sac[i] else None
    rfp = self.rfp if self.stage_with_rfp[i] else None
    planes = 64 * 2 ** i
    res_layer = make_res_layer(self.block, self.inplanes, planes, num_blocks, stride=stride, dilation=dilation, groups=self.groups, base_width=self.base_width, style=self.style, with_cp=self.with_cp, conv_cfg=self.conv_cfg, norm_cfg=self.norm_cfg, dcn=dcn, gcb=gcb, sac=sac, rfp=rfp)
    self.inplanes = planes * self.block.expansion
    layer_name = 'layer{}'.format(i + 1)
    self.add_module(layer_name, res_layer)
    self.res_layers.append(layer_name)"
angr,https://github.com/angr/angr/tree/master/angr/analyses/cfg/cfg_base.py,CFGBase,_get_finished_functions$1193,"for (func_addr, all_jobs) in self._jobs_to_analyze_per_function.items():
    if not all_jobs:
        finished_func_addrs.append(func_addr)","for e_target in self._jobs_to_analyze_per_function.items():
    all_jobs = e_target[1]
    func_addr = e_target[0]
    if not all_jobs:
        finished_func_addrs.append(func_addr)"
ansible-modules-core,https://github.com/ansible/ansible-modules-core/tree/master/packaging/os/apt_repository.py,SourcesList,save$292,"for (filename, sources) in list(self.files.items()):
    if sources:
        (d, fn) = os.path.split(filename)
        (fd, tmp_path) = tempfile.mkstemp(prefix='.%s-' % fn, dir=d)
        f = os.fdopen(fd, 'w')
        for (n, valid, enabled, source, comment) in sources:
            chunks = []
            if not enabled:
                chunks.append('# ')
            chunks.append(source)
            if comment:
                chunks.append(' # ')
                chunks.append(comment)
            chunks.append('\n')
            line = ''.join(chunks)
            try:
                f.write(line)
            except IOError:
                err = get_exception()
                self.module.fail_json(msg='Failed to write to file %s: %s' % (tmp_path, unicode(err)))
        self.module.atomic_move(tmp_path, filename)
        if filename in self.new_repos:
            this_mode = self.module.params.get('mode', DEFAULT_SOURCES_PERM)
            self.module.set_mode_if_different(filename, this_mode, False)
    else:
        del self.files[filename]
        if os.path.exists(filename):
            os.remove(filename)","for e_target in list(self.files.items()):
    sources = e_target[1]
    filename = e_target[0]
    if sources:
        (d, fn) = os.path.split(filename)
        (fd, tmp_path) = tempfile.mkstemp(prefix='.%s-' % fn, dir=d)
        f = os.fdopen(fd, 'w')
        for (n, valid, enabled, source, comment) in sources:
            chunks = []
            if not enabled:
                chunks.append('# ')
            chunks.append(source)
            if comment:
                chunks.append(' # ')
                chunks.append(comment)
            chunks.append('\n')
            line = ''.join(chunks)
            try:
                f.write(line)
            except IOError:
                err = get_exception()
                self.module.fail_json(msg='Failed to write to file %s: %s' % (tmp_path, unicode(err)))
        self.module.atomic_move(tmp_path, filename)
        if filename in self.new_repos:
            this_mode = self.module.params.get('mode', DEFAULT_SOURCES_PERM)
            self.module.set_mode_if_different(filename, this_mode, False)
    else:
        del self.files[filename]
        if os.path.exists(filename):
            os.remove(filename)"
mypy,https://github.com/python/mypy/tree/master/mypy/semanal_classprop.py,,calculate_class_vars$128,"for (name, sym) in info.names.items():
    node = sym.node
    if isinstance(node, Var) and node.info and node.is_inferred and (not node.is_classvar):
        for base in info.mro[1:]:
            member = base.names.get(name)
            if member is not None and isinstance(member.node, Var) and member.node.is_classvar:
                node.is_classvar = True","for e_target in info.names.items():
    sym = e_target[1]
    name = e_target[0]
    node = sym.node
    if isinstance(node, Var) and node.info and node.is_inferred and (not node.is_classvar):
        for base in info.mro[1:]:
            member = base.names.get(name)
            if member is not None and isinstance(member.node, Var) and member.node.is_classvar:
                node.is_classvar = True"
aldjemy,https://github.com/aldjemy/aldjemy/tree/master/test_project_postgres/sample/tests.py,TestArrayField,test_sa_sql_expression_language_fetching$60,"for (t_data, c_object) in zip(test_data, created_objects):
    (t_data_id, t_data_board) = t_data
    assert t_data_id == c_object.id
    assert t_data_board == c_object.board","for e_target in zip(test_data, created_objects):
    c_object = e_target[1]
    t_data = e_target[0]
    (t_data_id, t_data_board) = t_data
    assert t_data_id == c_object.id
    assert t_data_board == c_object.board"
sympy,https://github.com/sympy/sympy/tree/master/sympy/simplify/hyperexpand.py,G_Function,compute_buckets$630,"for (dic, lis) in zip(dicts, (self.an, self.ap, self.bm, self.bq)):
    for x in lis:
        dic[_mod1(x)].append(x)","for e_target in zip(dicts, (self.an, self.ap, self.bm, self.bq)):
    lis = e_target[1]
    dic = e_target[0]
    for x in lis:
        dic[_mod1(x)].append(x)"
sympy,https://github.com/sympy/sympy/tree/master/sympy/ntheory/qs.py,,_initialize_first_polynomial$101,"for (idx, val) in enumerate(q):
    q_l = factor_base[val].prime
    gamma = factor_base[val].tmem_p * mod_inverse(a // q_l, q_l) % q_l
    if gamma > q_l / 2:
        gamma = q_l - gamma
    B.append(a // q_l * gamma)","for e_target in enumerate(q):
    val = e_target[1]
    idx = e_target[0]
    q_l = factor_base[val].prime
    gamma = factor_base[val].tmem_p * mod_inverse(a // q_l, q_l) % q_l
    if gamma > q_l / 2:
        gamma = q_l - gamma
    B.append(a // q_l * gamma)"
checkmk,https://github.com/tribe29/checkmk/tree/master/cmk/gui/plugins/visuals/filters.py,FilterHostgroupProblems,display$728,"for (host_var, host_text) in self._options('host'):
    namevar = 'hostgroups_having_hosts_%s' % host_var
    html.checkbox(namevar, bool(value.get(namevar, True)), label=host_text)","for e_target in self._options('host'):
    host_text = e_target[1]
    host_var = e_target[0]
    namevar = 'hostgroups_having_hosts_%s' % host_var
    html.checkbox(namevar, bool(value.get(namevar, True)), label=host_text)"
salt,https://github.com/saltstack/salt/tree/master/tests/integration/ssh/test_state.py,SSHStateTest,_check_dict_ret$24,"for (key, value) in ret.items():
    self.assertIsInstance(value, dict)
    if equal:
        self.assertEqual(value[val], exp_ret)
    else:
        self.assertNotEqual(value[val], exp_ret)","for e_target in ret.items():
    value = e_target[1]
    key = e_target[0]
    self.assertIsInstance(value, dict)
    if equal:
        self.assertEqual(value[val], exp_ret)
    else:
        self.assertNotEqual(value[val], exp_ret)"
deluge,https://github.com/deluge-torrent/deluge/tree/master/deluge/ui/console/modes/torrentlist/torrentactions.py,,action_remove$35,"for (t_id, e_msg) in errors:
    error_msgs += 'Error removing torrent %s : %s\n' % (t_id, e_msg)","for e_target in errors:
    e_msg = e_target[1]
    t_id = e_target[0]
    error_msgs += 'Error removing torrent %s : %s\n' % (t_id, e_msg)"
checkmk,https://github.com/tribe29/checkmk/tree/master/cmk/gui/utils/labels.py,LabelsCache,_livestatus_get_labels$128,"for (key, value) in service_label.items():
    service_labels.setdefault(site_id, {}).update({key: value})","for e_target in service_label.items():
    value = e_target[1]
    key = e_target[0]
    service_labels.setdefault(site_id, {}).update({key: value})"
rpaframework,https://github.com/robocorp/rpaframework/tree/master/packages/pdf/src/RPA/PDF/keywords/document.py,DocumentKeywords,template_html_to_pdf$131,"for (key, value) in variables.items():
    html = html.replace('{{' + key + '}}', str(value))","for e_target in variables.items():
    value = e_target[1]
    key = e_target[0]
    html = html.replace('{{' + key + '}}', str(value))"
TFace,https://github.com/Tencent/TFace/tree/master/torchkit/backbone/model_efficientnet.py,EfficientNet,_init_params$398,"for (name, module) in self.named_modules():
    if isinstance(module, nn.Conv2d):
        init.kaiming_uniform_(module.weight)
        if module.bias is not None:
            init.constant_(module.bias, 0)","for e_target in self.named_modules():
    module = e_target[1]
    name = e_target[0]
    if isinstance(module, nn.Conv2d):
        init.kaiming_uniform_(module.weight)
        if module.bias is not None:
            init.constant_(module.bias, 0)"
felicette,https://github.com/plant99/felicette/tree/master/felicette/utils/color.py,,color$26,"for (window, ij) in windows:
    arr = color_worker(rasters, window, ij, args)
    dest.write(arr, window=window)","for e_target in windows:
    ij = e_target[1]
    window = e_target[0]
    arr = color_worker(rasters, window, ij, args)
    dest.write(arr, window=window)"
kivy,https://github.com/kivy/kivy/tree/master/kivy/animation.py,Animation,_initialize$300,"for (key, value) in self._animated_properties.items():
    original_value = getattr(widget, key)
    if isinstance(original_value, (tuple, list)):
        original_value = original_value[:]
    elif isinstance(original_value, dict):
        original_value = original_value.copy()
    p[key] = (original_value, value)","for e_target in self._animated_properties.items():
    value = e_target[1]
    key = e_target[0]
    original_value = getattr(widget, key)
    if isinstance(original_value, (tuple, list)):
        original_value = original_value[:]
    elif isinstance(original_value, dict):
        original_value = original_value.copy()
    p[key] = (original_value, value)"
pymc,https://github.com/pymc-devs/pymc/tree/master/pymc/model_graph.py,ModelGraph,make_graph$189,"for (key, values) in self.make_compute_graph().items():
    for value in values:
        graph.edge(value.replace(':', '&'), key.replace(':', '&'))","for e_target in self.make_compute_graph().items():
    values = e_target[1]
    key = e_target[0]
    for value in values:
        graph.edge(value.replace(':', '&'), key.replace(':', '&'))"
LanZouCloud-CMD,https://github.com/zaxtyson/LanZouCloud-CMD/tree/master/lanzou/api/core.py,LanZouCloud,get_rec_file_list$200,"for (fid, ftype, name, time) in sorted(files, key=lambda x: x[2]):
    if not name.endswith(ftype):
        name = name + '.' + ftype
    if name in file_name_list:
        counter += 1
        name = f'{name}({counter})'
    else:
        counter = 1
        file_name_list.append(name)
    file_list.append(RecFile(name, int(fid), ftype, size='', time=time))","for e_target in sorted(files, key=lambda x: x[2]):
    time = e_target[3]
    name = e_target[2]
    ftype = e_target[1]
    fid = e_target[0]
    if not name.endswith(ftype):
        name = name + '.' + ftype
    if name in file_name_list:
        counter += 1
        name = f'{name}({counter})'
    else:
        counter = 1
        file_name_list.append(name)
    file_list.append(RecFile(name, int(fid), ftype, size='', time=time))"
nova,https://github.com/openstack/nova/tree/master/nova/tests/functional/test_nova_manage.py,TestDBArchiveDeletedRowsMultiCell,test_archive_deleted_rows$2239,"for (cell_name, server_id) in server_ids.items():
    with context.target_cell(admin_context, self.cell_mappings[cell_name]) as cctxt:
        objects.Instance.get_by_uuid(cctxt, server_id)","for e_target in server_ids.items():
    server_id = e_target[1]
    cell_name = e_target[0]
    with context.target_cell(admin_context, self.cell_mappings[cell_name]) as cctxt:
        objects.Instance.get_by_uuid(cctxt, server_id)"
galaxy,https://github.com/ansible/galaxy/tree/master/lib/galaxy/webapps/galaxy/controllers/dataset.py,DatasetInterface,get_edit$196,"for (action, roles) in trans.app.security_agent.get_permissions(data.dataset).items():
    in_roles[action.action] = [trans.security.encode_id(role.id) for role in roles]","for e_target in trans.app.security_agent.get_permissions(data.dataset).items():
    roles = e_target[1]
    action = e_target[0]
    in_roles[action.action] = [trans.security.encode_id(role.id) for role in roles]"
elastalert,https://github.com/Yelp/elastalert/tree/master/tests/rules_test.py,,test_eventwindow$187,"for (exp, actual) in zip(timestamps[1:], window.data):
    assert actual[0]['@timestamp'] == exp","for e_target in zip(timestamps[1:], window.data):
    actual = e_target[1]
    exp = e_target[0]
    assert actual[0]['@timestamp'] == exp"
open_model_zoo,https://github.com/openvinotoolkit/open_model_zoo/tree/master/tools/accuracy_checker/openvino/tools/accuracy_checker/adapters/mask_rcnn_with_text.py,MaskRCNNWithTextAdapter,mask_to_result_old$166,"for (cls, raw_mask) in zip(det_labels, det_masks):
    per_obj_raw_masks.append(raw_mask[cls, ...])","for e_target in zip(det_labels, det_masks):
    raw_mask = e_target[1]
    cls = e_target[0]
    per_obj_raw_masks.append(raw_mask[cls, ...])"
v2rayL,https://github.com/jiangxufeng/v2rayL/tree/master/v2rayL-GUI/sub2conf_api.py,Sub2Conf,conf2b64$434,"for (k, v) in self.conf[region].items():
    tmp[k] = v","for e_target in self.conf[region].items():
    v = e_target[1]
    k = e_target[0]
    tmp[k] = v"
sparseml,https://github.com/neuralmagic/sparseml/tree/master/src/sparseml/optim/sensitivity.py,PruningLossSensitivityAnalysis,results_model$326,"for (key, val) in res.averages.items():
    if key not in measurements:
        measurements[key] = 0.0
    measurements[key] += val","for e_target in res.averages.items():
    val = e_target[1]
    key = e_target[0]
    if key not in measurements:
        measurements[key] = 0.0
    measurements[key] += val"
FSL-Mate,https://github.com/tata1661/FSL-Mate/tree/master/PaddleFSL/examples/FewCLUE/pet/predict.py,,write_bustm$218,"for (idx, example) in enumerate(test_ds):
    test_example['id'] = example['id']
    test_example['label'] = pred_labels[idx]
    str_test_example = json.dumps(test_example) + '\n'
    f.write(str_test_example)","for e_target in enumerate(test_ds):
    example = e_target[1]
    idx = e_target[0]
    test_example['id'] = example['id']
    test_example['label'] = pred_labels[idx]
    str_test_example = json.dumps(test_example) + '\n'
    f.write(str_test_example)"
py-evm,https://github.com/ethereum/py-evm/tree/master/eth/db/journal.py,Journal,discard$140,"for (old_key, old_value) in rollback_data.items():
    if old_value is REVERT_TO_WRAPPED:
        self._current_values.pop(old_key, None)
    elif old_value is DELETE_WRAPPED:
        self._current_values[old_key] = old_value
    elif type(old_value) is bytes:
        self._current_values[old_key] = old_value
    else:
        raise ValidationError(f'Unexpected value, must be bytes: {old_value!r}')","for e_target in rollback_data.items():
    old_value = e_target[1]
    old_key = e_target[0]
    if old_value is REVERT_TO_WRAPPED:
        self._current_values.pop(old_key, None)
    elif old_value is DELETE_WRAPPED:
        self._current_values[old_key] = old_value
    elif type(old_value) is bytes:
        self._current_values[old_key] = old_value
    else:
        raise ValidationError(f'Unexpected value, must be bytes: {old_value!r}')"
ansible-modules-extras,https://github.com/ansible/ansible-modules-extras/tree/master/cloud/amazon/efs_facts.py,,group_list_of_dict$326,"for (key, value) in item.items():
    result[key] += value if isinstance(value, list) else [value]","for e_target in item.items():
    value = e_target[1]
    key = e_target[0]
    result[key] += value if isinstance(value, list) else [value]"
checkmk,https://github.com/tribe29/checkmk/tree/master/cmk/utils/packaging.py,,_validate_package_files_part$536,"for (other_pacname, other_package_info) in packages.items():
    for other_rel_path in other_package_info['files'].get(part, []):
        if other_rel_path == rel_path and other_pacname != pacname:
            raise PackageException('File %s does already belong to package %s' % (path, other_pacname))","for e_target in packages.items():
    other_package_info = e_target[1]
    other_pacname = e_target[0]
    for other_rel_path in other_package_info['files'].get(part, []):
        if other_rel_path == rel_path and other_pacname != pacname:
            raise PackageException('File %s does already belong to package %s' % (path, other_pacname))"
SSL4MIS,https://github.com/HiLab-git/SSL4MIS/tree/master/code/train_fully_supervised_3D.py,,train$55,"for (i_batch, sampled_batch) in enumerate(trainloader):
    (volume_batch, label_batch) = (sampled_batch['image'], sampled_batch['label'])
    (volume_batch, label_batch) = (volume_batch.cuda(), label_batch.cuda())
    outputs = model(volume_batch)
    outputs_soft = torch.softmax(outputs, dim=1)
    loss_ce = ce_loss(outputs, label_batch)
    loss_dice = dice_loss(outputs_soft, label_batch.unsqueeze(1))
    loss = 0.5 * (loss_dice + loss_ce)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    lr_ = base_lr * (1.0 - iter_num / max_iterations) ** 0.9
    for param_group in optimizer.param_groups:
        param_group['lr'] = lr_
    iter_num = iter_num + 1
    writer.add_scalar('info/lr', lr_, iter_num)
    writer.add_scalar('info/total_loss', loss, iter_num)
    writer.add_scalar('info/loss_ce', loss_ce, iter_num)
    writer.add_scalar('info/loss_dice', loss_dice, iter_num)
    logging.info('iteration %d : loss : %f, loss_ce: %f, loss_dice: %f' % (iter_num, loss.item(), loss_ce.item(), loss_dice.item()))
    writer.add_scalar('loss/loss', loss, iter_num)
    if iter_num % 20 == 0:
        image = volume_batch[0, 0:1, :, :, 20:61:10].permute(3, 0, 1, 2).repeat(1, 3, 1, 1)
        grid_image = make_grid(image, 5, normalize=True)
        writer.add_image('train/Image', grid_image, iter_num)
        image = outputs_soft[0, 1:2, :, :, 20:61:10].permute(3, 0, 1, 2).repeat(1, 3, 1, 1)
        grid_image = make_grid(image, 5, normalize=False)
        writer.add_image('train/Predicted_label', grid_image, iter_num)
        image = label_batch[0, :, :, 20:61:10].unsqueeze(0).permute(3, 0, 1, 2).repeat(1, 3, 1, 1)
        grid_image = make_grid(image, 5, normalize=False)
        writer.add_image('train/Groundtruth_label', grid_image, iter_num)
    if iter_num > 0 and iter_num % 200 == 0:
        model.eval()
        avg_metric = test_all_case(model, args.root_path, test_list='val.txt', num_classes=2, patch_size=args.patch_size, stride_xy=64, stride_z=64)
        if avg_metric[:, 0].mean() > best_performance:
            best_performance = avg_metric[:, 0].mean()
            save_mode_path = os.path.join(snapshot_path, 'iter_{}_dice_{}.pth'.format(iter_num, round(best_performance, 4)))
            save_best = os.path.join(snapshot_path, '{}_best_model.pth'.format(args.model))
            torch.save(model.state_dict(), save_mode_path)
            torch.save(model.state_dict(), save_best)
        writer.add_scalar('info/val_dice_score', avg_metric[0, 0], iter_num)
        writer.add_scalar('info/val_hd95', avg_metric[0, 1], iter_num)
        logging.info('iteration %d : dice_score : %f hd95 : %f' % (iter_num, avg_metric[0, 0].mean(), avg_metric[0, 1].mean()))
        model.train()
    if iter_num % 3000 == 0:
        save_mode_path = os.path.join(snapshot_path, 'iter_' + str(iter_num) + '.pth')
        torch.save(model.state_dict(), save_mode_path)
        logging.info('save model to {}'.format(save_mode_path))
    if iter_num >= max_iterations:
        break","for e_target in enumerate(trainloader):
    sampled_batch = e_target[1]
    i_batch = e_target[0]
    (volume_batch, label_batch) = (sampled_batch['image'], sampled_batch['label'])
    (volume_batch, label_batch) = (volume_batch.cuda(), label_batch.cuda())
    outputs = model(volume_batch)
    outputs_soft = torch.softmax(outputs, dim=1)
    loss_ce = ce_loss(outputs, label_batch)
    loss_dice = dice_loss(outputs_soft, label_batch.unsqueeze(1))
    loss = 0.5 * (loss_dice + loss_ce)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    lr_ = base_lr * (1.0 - iter_num / max_iterations) ** 0.9
    for param_group in optimizer.param_groups:
        param_group['lr'] = lr_
    iter_num = iter_num + 1
    writer.add_scalar('info/lr', lr_, iter_num)
    writer.add_scalar('info/total_loss', loss, iter_num)
    writer.add_scalar('info/loss_ce', loss_ce, iter_num)
    writer.add_scalar('info/loss_dice', loss_dice, iter_num)
    logging.info('iteration %d : loss : %f, loss_ce: %f, loss_dice: %f' % (iter_num, loss.item(), loss_ce.item(), loss_dice.item()))
    writer.add_scalar('loss/loss', loss, iter_num)
    if iter_num % 20 == 0:
        image = volume_batch[0, 0:1, :, :, 20:61:10].permute(3, 0, 1, 2).repeat(1, 3, 1, 1)
        grid_image = make_grid(image, 5, normalize=True)
        writer.add_image('train/Image', grid_image, iter_num)
        image = outputs_soft[0, 1:2, :, :, 20:61:10].permute(3, 0, 1, 2).repeat(1, 3, 1, 1)
        grid_image = make_grid(image, 5, normalize=False)
        writer.add_image('train/Predicted_label', grid_image, iter_num)
        image = label_batch[0, :, :, 20:61:10].unsqueeze(0).permute(3, 0, 1, 2).repeat(1, 3, 1, 1)
        grid_image = make_grid(image, 5, normalize=False)
        writer.add_image('train/Groundtruth_label', grid_image, iter_num)
    if iter_num > 0 and iter_num % 200 == 0:
        model.eval()
        avg_metric = test_all_case(model, args.root_path, test_list='val.txt', num_classes=2, patch_size=args.patch_size, stride_xy=64, stride_z=64)
        if avg_metric[:, 0].mean() > best_performance:
            best_performance = avg_metric[:, 0].mean()
            save_mode_path = os.path.join(snapshot_path, 'iter_{}_dice_{}.pth'.format(iter_num, round(best_performance, 4)))
            save_best = os.path.join(snapshot_path, '{}_best_model.pth'.format(args.model))
            torch.save(model.state_dict(), save_mode_path)
            torch.save(model.state_dict(), save_best)
        writer.add_scalar('info/val_dice_score', avg_metric[0, 0], iter_num)
        writer.add_scalar('info/val_hd95', avg_metric[0, 1], iter_num)
        logging.info('iteration %d : dice_score : %f hd95 : %f' % (iter_num, avg_metric[0, 0].mean(), avg_metric[0, 1].mean()))
        model.train()
    if iter_num % 3000 == 0:
        save_mode_path = os.path.join(snapshot_path, 'iter_' + str(iter_num) + '.pth')
        torch.save(model.state_dict(), save_mode_path)
        logging.info('save model to {}'.format(save_mode_path))
    if iter_num >= max_iterations:
        break"
DetectoRS,https://github.com/joe-siyuan-qiao/DetectoRS/tree/master/mmdet/datasets/coco.py,CocoDataset,evaluate$290,"for (i, num) in enumerate(proposal_nums):
    eval_results['AR@{}'.format(num)] = ar[i]
    log_msg.append('\nAR@{}\t{:.4f}'.format(num, ar[i]))","for e_target in enumerate(proposal_nums):
    num = e_target[1]
    i = e_target[0]
    eval_results['AR@{}'.format(num)] = ar[i]
    log_msg.append('\nAR@{}\t{:.4f}'.format(num, ar[i]))"
galaxy,https://github.com/ansible/galaxy/tree/master/lib/galaxy/tools/parameters/meta.py,,expand_workflow_inputs$33,"for (key, value) in sorted(step.items()):
    if is_batch(value):
        nval = len(value['values'])
        if 'product' in value and value['product'] is True:
            product.append(value['values'])
            product_keys.append(ParamKey(step_id, key))
        else:
            if linked_n is None:
                linked_n = nval
            elif linked_n != nval or nval == 0:
                raise exceptions.RequestParameterInvalidException('Failed to match linked batch selections. Please select equal number of data files.')
            linked.append(value['values'])
            linked_keys.append(ParamKey(step_id, key))","for e_target in sorted(step.items()):
    value = e_target[1]
    key = e_target[0]
    if is_batch(value):
        nval = len(value['values'])
        if 'product' in value and value['product'] is True:
            product.append(value['values'])
            product_keys.append(ParamKey(step_id, key))
        else:
            if linked_n is None:
                linked_n = nval
            elif linked_n != nval or nval == 0:
                raise exceptions.RequestParameterInvalidException('Failed to match linked batch selections. Please select equal number of data files.')
            linked.append(value['values'])
            linked_keys.append(ParamKey(step_id, key))"
scrapy,https://github.com/scrapy/scrapy/tree/master/tests/test_responsetypes.py,ResponseTypesTest,test_from_headers$63,"for (source, cls) in mappings:
    source = Headers(source)
    retcls = responsetypes.from_headers(source)
    assert retcls is cls, f'{source} ==> {retcls} != {cls}'","for e_target in mappings:
    cls = e_target[1]
    source = e_target[0]
    source = Headers(source)
    retcls = responsetypes.from_headers(source)
    assert retcls is cls, f'{source} ==> {retcls} != {cls}'"
detect-secrets,https://github.com/Yelp/detect-secrets/tree/master/detect_secrets/core/scan.py,,scan_diff$165,"for (filename, lines) in _get_lines_from_diff(diff):
    yield from _process_line_based_plugins(lines, filename=filename)","for e_target in _get_lines_from_diff(diff):
    lines = e_target[1]
    filename = e_target[0]
    yield from _process_line_based_plugins(lines, filename=filename)"
VideoSuperResolution,https://github.com/LoSealL/VideoSuperResolution/tree/master/Tools/FastMetrics.py,,gen$44,"for (x, y) in zip(d1, d2):
    if x.is_file() and y.is_file():
        yield (x, y)
    else:
        print(f' [!] Found {x} v.s. {y} mismatch.')","for e_target in zip(d1, d2):
    y = e_target[1]
    x = e_target[0]
    if x.is_file() and y.is_file():
        yield (x, y)
    else:
        print(f' [!] Found {x} v.s. {y} mismatch.')"
python,https://github.com/zhanghe06/python/tree/master/kubernetes/client/api/core_v1_api.py,CoreV1Api,create_namespaced_binding_with_http_info$6494,"for (key, val) in six.iteritems(local_var_params['kwargs']):
    if key not in all_params:
        raise ApiTypeError(""Got an unexpected keyword argument '%s' to method create_namespaced_binding"" % key)
    local_var_params[key] = val","for e_target in six.iteritems(local_var_params['kwargs']):
    val = e_target[1]
    key = e_target[0]
    if key not in all_params:
        raise ApiTypeError(""Got an unexpected keyword argument '%s' to method create_namespaced_binding"" % key)
    local_var_params[key] = val"
deepvoice3_pytorch,https://github.com/r9y9/deepvoice3_pytorch/tree/master/vctk_preprocess/extract_feats.py,,replace_conflines$41,"for (n, l) in enumerate(conf):
    if l[:len(match)] == match:
        replace = n
        break","for e_target in enumerate(conf):
    l = e_target[1]
    n = e_target[0]
    if l[:len(match)] == match:
        replace = n
        break"
celery,https://github.com/celery/celery/tree/master/t/unit/tasks/test_tasks.py,test_tasks,assert_next_task_data_equal$855,"for (arg_name, arg_value) in kwargs.items():
    assert task_kwargs.get(arg_name) == arg_value","for e_target in kwargs.items():
    arg_value = e_target[1]
    arg_name = e_target[0]
    assert task_kwargs.get(arg_name) == arg_value"
jukebox,https://github.com/openai/jukebox/tree/master/apex/apex/optimizers/fp16_optimizer.py,FP16_Optimizer,load_state_dict$234,"for (current, saved) in zip(self.fp32_groups_flat, state_dict['fp32_groups_flat']):
    current.data.copy_(saved.data)","for e_target in zip(self.fp32_groups_flat, state_dict['fp32_groups_flat']):
    saved = e_target[1]
    current = e_target[0]
    current.data.copy_(saved.data)"
python,https://github.com/zhanghe06/python/tree/master/kubernetes/client/api/storage_v1beta1_api.py,StorageV1beta1Api,create_storage_class_with_http_info$463,"for (key, val) in six.iteritems(local_var_params['kwargs']):
    if key not in all_params:
        raise ApiTypeError(""Got an unexpected keyword argument '%s' to method create_storage_class"" % key)
    local_var_params[key] = val","for e_target in six.iteritems(local_var_params['kwargs']):
    val = e_target[1]
    key = e_target[0]
    if key not in all_params:
        raise ApiTypeError(""Got an unexpected keyword argument '%s' to method create_storage_class"" % key)
    local_var_params[key] = val"
mars,https://github.com/mars-project/mars/tree/master/mars/learn/contrib/pytorch/tests/test_dataset.py,,test_distributed_sampler$267,"for (_, (batch_data, batch_labels)) in enumerate(train_loader):
    assert len(batch_data[0]) == 32
    assert len(batch_labels[0]) == 10","for e_target in enumerate(train_loader):
    batch_labels = e_target[1][1]
    batch_data = e_target[1][0]
    _ = e_target[0]
    assert len(batch_data[0]) == 32
    assert len(batch_labels[0]) == 10"
metaflow,https://github.com/Netflix/metaflow/tree/master/metaflow/datatools/s3.py,S3,get_many$631,"for (s3prefix, s3url, fname) in res:
    if return_info:
        if fname:
            with open(os.path.join(self._tmpdir, '%s_meta' % fname), 'r') as f:
                info = json.load(f)
            yield (self._s3root, s3url, os.path.join(self._tmpdir, fname), None, info['content_type'], info['metadata'], None, info['last_modified'])
        else:
            yield (self._s3root, s3prefix, None)
    elif fname:
        yield (self._s3root, s3url, os.path.join(self._tmpdir, fname))
    else:
        yield (self._s3root, s3prefix, None)","for e_target in res:
    fname = e_target[2]
    s3url = e_target[1]
    s3prefix = e_target[0]
    if return_info:
        if fname:
            with open(os.path.join(self._tmpdir, '%s_meta' % fname), 'r') as f:
                info = json.load(f)
            yield (self._s3root, s3url, os.path.join(self._tmpdir, fname), None, info['content_type'], info['metadata'], None, info['last_modified'])
        else:
            yield (self._s3root, s3prefix, None)
    elif fname:
        yield (self._s3root, s3url, os.path.join(self._tmpdir, fname))
    else:
        yield (self._s3root, s3prefix, None)"
LibCST,https://github.com/Instagram/LibCST/tree/master/libcst/codemod/visitors/_add_imports.py,AddImportsVisitor,visit_Module$168,"for (module, aliases) in gatherer.alias_mapping.items():
    for (obj, alias) in aliases:
        if module in self.alias_mapping and (obj, alias) in self.alias_mapping[module]:
            self.alias_mapping[module].remove((obj, alias))
            if len(self.alias_mapping[module]) == 0:
                del self.alias_mapping[module]","for e_target in gatherer.alias_mapping.items():
    aliases = e_target[1]
    module = e_target[0]
    for (obj, alias) in aliases:
        if module in self.alias_mapping and (obj, alias) in self.alias_mapping[module]:
            self.alias_mapping[module].remove((obj, alias))
            if len(self.alias_mapping[module]) == 0:
                del self.alias_mapping[module]"
botocore,https://github.com/boto/botocore/tree/master/tests/functional/test_paginator_config.py,,_pagination_configs$134,"for (op_name, single_config) in page_config['pagination'].items():
    yield (op_name, single_config, service_model)","for e_target in page_config['pagination'].items():
    single_config = e_target[1]
    op_name = e_target[0]
    yield (op_name, single_config, service_model)"
ansible-modules-core,https://github.com/ansible/ansible-modules-core/tree/master/system/cron.py,CronTab,find_env$444,"for (index, l) in enumerate(self.lines):
    if re.match('^%s=' % name, l):
        return [index, l]","for e_target in enumerate(self.lines):
    l = e_target[1]
    index = e_target[0]
    if re.match('^%s=' % name, l):
        return [index, l]"
hummingbird,https://github.com/microsoft/hummingbird/tree/master/benchmarks/pipelines/openml_pipelines.py,,if_main_my$197,"for (name, _, column_indices) in transf.transformers:
    if isinstance(column_indices, list):
        if len(column_indices) > 0 and type(column_indices[0]) == bool:
            found_unsupported_column_transformer = True","for e_target in transf.transformers:
    column_indices = e_target[2]
    _ = e_target[1]
    name = e_target[0]
    if isinstance(column_indices, list):
        if len(column_indices) > 0 and type(column_indices[0]) == bool:
            found_unsupported_column_transformer = True"
open_model_zoo,https://github.com/openvinotoolkit/open_model_zoo/tree/master/demos/face_recognition_demo/python/face_identifier.py,FaceIdentifier,_align_rois$132,"for (image, image_landmarks) in zip(face_images, face_landmarks):
    scale = np.array((image.shape[1], image.shape[0]))
    desired_landmarks = np.array(self.REFERENCE_LANDMARKS, dtype=np.float64) * scale
    landmarks = image_landmarks * scale
    transform = FaceIdentifier.get_transform(desired_landmarks, landmarks)
    cv2.warpAffine(image, transform, tuple(scale), image, flags=cv2.WARP_INVERSE_MAP)","for e_target in zip(face_images, face_landmarks):
    image_landmarks = e_target[1]
    image = e_target[0]
    scale = np.array((image.shape[1], image.shape[0]))
    desired_landmarks = np.array(self.REFERENCE_LANDMARKS, dtype=np.float64) * scale
    landmarks = image_landmarks * scale
    transform = FaceIdentifier.get_transform(desired_landmarks, landmarks)
    cv2.warpAffine(image, transform, tuple(scale), image, flags=cv2.WARP_INVERSE_MAP)"
sktime,https://github.com/alan-turing-institute/sktime/tree/master/sktime/contrib/distance_based/_proximity_forest.py,,pick_rand_param_perm_from_dict$632,"for (param_name, param_values) in param_pool.items():
    if isinstance(param_values, list):
        param_value = param_values[random_state.randint(len(param_values))]
    elif hasattr(param_values, 'rvs'):
        param_value = param_values.rvs(random_state=random_state)
    else:
        raise Exception('unknown type of parameter pool')
    param_perm[param_name] = param_value","for e_target in param_pool.items():
    param_values = e_target[1]
    param_name = e_target[0]
    if isinstance(param_values, list):
        param_value = param_values[random_state.randint(len(param_values))]
    elif hasattr(param_values, 'rvs'):
        param_value = param_values.rvs(random_state=random_state)
    else:
        raise Exception('unknown type of parameter pool')
    param_perm[param_name] = param_value"
ERNIE,https://github.com/thunlp/ERNIE/tree/master/propeller/paddle/train/hooks.py,EvalHook,after_train$317,"for (n, m) in zip(self.names, self.metrics):
    val = m.eval()
    self._result[n] = val","for e_target in zip(self.names, self.metrics):
    m = e_target[1]
    n = e_target[0]
    val = m.eval()
    self._result[n] = val"
OpenFermion,https://github.com/quantumlib/OpenFermion/tree/master/src/openfermion/measurements/equality_constraint_projection.py,,operator_to_vector$120,"for (term, coefficient) in operator.terms.items():
    term_index = linearize_term(term, n_orbitals)
    vectorized_operator[term_index] = coefficient","for e_target in operator.terms.items():
    coefficient = e_target[1]
    term = e_target[0]
    term_index = linearize_term(term, n_orbitals)
    vectorized_operator[term_index] = coefficient"
pycoin,https://github.com/richardkiss/pycoin/tree/master/pycoin/cmds/tx.py,,dump_signatures_hex$625,"for (_, tx_in) in enumerate(tx.txs_in):
    sigs.extend(network.who_signed.extract_signatures(tx, _))","for e_target in enumerate(tx.txs_in):
    tx_in = e_target[1]
    _ = e_target[0]
    sigs.extend(network.who_signed.extract_signatures(tx, _))"
KPConv,https://github.com/HuguesTHOMAS/KPConv/tree/master/utils/tester.py,ModelTester,test_cloud_segmentation_on_val$817,"for (i, file_path) in enumerate(files):
    if dataset.all_splits[i] == dataset.validation_split:
        points = dataset.load_evaluation_points(file_path)
        preds = dataset.label_values[np.argmax(proj_probs[i_test], axis=1)].astype(np.int32)
        pots = dataset.potentials['validation'][i_test][dataset.validation_proj[i_test]]
        cloud_name = file_path.split('/')[-1]
        test_name = join(test_path, 'val_predictions', cloud_name)
        write_ply(test_name, [points, preds, pots, dataset.validation_labels[i_test]], ['x', 'y', 'z', 'preds', 'pots', 'gt'])
        test_name2 = join(test_path, 'val_probs', cloud_name)
        prob_names = ['_'.join(dataset.label_to_names[label].split()) for label in dataset.label_values]
        write_ply(test_name2, [points, proj_probs[i_test]], ['x', 'y', 'z'] + prob_names)
        i_test += 1","for e_target in enumerate(files):
    file_path = e_target[1]
    i = e_target[0]
    if dataset.all_splits[i] == dataset.validation_split:
        points = dataset.load_evaluation_points(file_path)
        preds = dataset.label_values[np.argmax(proj_probs[i_test], axis=1)].astype(np.int32)
        pots = dataset.potentials['validation'][i_test][dataset.validation_proj[i_test]]
        cloud_name = file_path.split('/')[-1]
        test_name = join(test_path, 'val_predictions', cloud_name)
        write_ply(test_name, [points, preds, pots, dataset.validation_labels[i_test]], ['x', 'y', 'z', 'preds', 'pots', 'gt'])
        test_name2 = join(test_path, 'val_probs', cloud_name)
        prob_names = ['_'.join(dataset.label_to_names[label].split()) for label in dataset.label_values]
        write_ply(test_name2, [points, proj_probs[i_test]], ['x', 'y', 'z'] + prob_names)
        i_test += 1"
texar-pytorch,https://github.com/asyml/texar-pytorch/tree/master/texar/torch/data/tokenizers/bert_tokenizer.py,BERTTokenizer,save_vocab$182,"for (token, token_index) in sorted(self.vocab.items(), key=lambda kv: kv[1]):
    if index != token_index:
        print('Saving vocabulary to {}: vocabulary indices are not consecutive. Please check that the vocabulary is not corrupted!'.format(save_dir))
        index = token_index
    writer.write(token + u'\n')
    index += 1","for e_target in sorted(self.vocab.items(), key=lambda kv: kv[1]):
    token_index = e_target[1]
    token = e_target[0]
    if index != token_index:
        print('Saving vocabulary to {}: vocabulary indices are not consecutive. Please check that the vocabulary is not corrupted!'.format(save_dir))
        index = token_index
    writer.write(token + u'\n')
    index += 1"
OpenFermion,https://github.com/quantumlib/OpenFermion/tree/master/src/openfermion/third_party/representability/constraints/spin_orbital_2pos_constraints.py,,tpdm_to_phdm_mapping$231,"for (i, j, k, l) in product(range(dim), repeat=4):
    if i * dim + j <= k * dim + l:
        db_element = g2d2map(i, j, k, l, factor=0.5)
        db_element_2 = g2d2map(k, l, i, j, factor=0.5)
        dbe_list.append(db_element.join_elements(db_element_2))","for e_target in product(range(dim), repeat=4):
    l = e_target[3]
    k = e_target[2]
    j = e_target[1]
    i = e_target[0]
    if i * dim + j <= k * dim + l:
        db_element = g2d2map(i, j, k, l, factor=0.5)
        db_element_2 = g2d2map(k, l, i, j, factor=0.5)
        dbe_list.append(db_element.join_elements(db_element_2))"
sympy,https://github.com/sympy/sympy/tree/master/sympy/printing/codeprinter.py,requires,__call__$22,"for (k, v) in self._req.items():
    getattr(self_, k).update(v)","for e_target in self._req.items():
    v = e_target[1]
    k = e_target[0]
    getattr(self_, k).update(v)"
httpx,https://github.com/encode/httpx/tree/master/httpx/_transports/wsgi.py,WSGITransport,handle_request$80,"for (header_key, header_value) in request.headers.raw:
    key = header_key.decode('ascii').upper().replace('-', '_')
    if key not in ('CONTENT_TYPE', 'CONTENT_LENGTH'):
        key = 'HTTP_' + key
    environ[key] = header_value.decode('ascii')","for e_target in request.headers.raw:
    header_value = e_target[1]
    header_key = e_target[0]
    key = header_key.decode('ascii').upper().replace('-', '_')
    if key not in ('CONTENT_TYPE', 'CONTENT_LENGTH'):
        key = 'HTTP_' + key
    environ[key] = header_value.decode('ascii')"
mypy,https://github.com/python/mypy/tree/master/mypy/semanal.py,SemanticAnalyzer,current_symbol_table$4896,"for (i, is_comprehension) in enumerate(reversed(self.is_comprehension_stack)):
    if not is_comprehension:
        if i == len(self.locals) - 1:
            names = self.globals
        else:
            names_candidate = self.locals[-1 - i]
            assert names_candidate is not None, 'Escaping comprehension from invalid scope'
            names = names_candidate
        break
else:
    assert False, 'Should have at least one non-comprehension scope'","for e_target in enumerate(reversed(self.is_comprehension_stack)):
    is_comprehension = e_target[1]
    i = e_target[0]
    if not is_comprehension:
        if i == len(self.locals) - 1:
            names = self.globals
        else:
            names_candidate = self.locals[-1 - i]
            assert names_candidate is not None, 'Escaping comprehension from invalid scope'
            names = names_candidate
        break
else:
    assert False, 'Should have at least one non-comprehension scope'"
jax,https://github.com/google/jax/tree/master/tests/masking_test.py,MaskingTest,rnn_reference$409,"for (xs, target) in zip(seqs, targets):
    h = jnp.zeros(n)
    for x in xs:
        h = jnp.tanh(jnp.dot(W, h) + jnp.dot(W, x))
    predicted = h
    total_loss = total_loss + jnp.sum((predicted - target) ** 2)","for e_target in zip(seqs, targets):
    target = e_target[1]
    xs = e_target[0]
    h = jnp.zeros(n)
    for x in xs:
        h = jnp.tanh(jnp.dot(W, h) + jnp.dot(W, x))
    predicted = h
    total_loss = total_loss + jnp.sum((predicted - target) ** 2)"
decision-transformer,https://github.com/kzl/decision-transformer/tree/master/gym/decision_transformer/training/trainer.py,Trainer,train_iteration$21,"for (k, v) in outputs.items():
    logs[f'evaluation/{k}'] = v","for e_target in outputs.items():
    v = e_target[1]
    k = e_target[0]
    logs[f'evaluation/{k}'] = v"
checkmk,https://github.com/tribe29/checkmk/tree/master/cmk/base/config.py,PackedConfigGenerator,filter_clusters$719,"for (cluster_entry, cluster_nodes) in clusters_orig.items():
    clustername = cluster_entry.split('|', 1)[0]
    if clustername in active_clusters:
        clusters_red[cluster_entry] = cluster_nodes","for e_target in clusters_orig.items():
    cluster_nodes = e_target[1]
    cluster_entry = e_target[0]
    clustername = cluster_entry.split('|', 1)[0]
    if clustername in active_clusters:
        clusters_red[cluster_entry] = cluster_nodes"
meshio,https://github.com/nschloe/meshio/tree/master/src/meshio/ansys/_ansys.py,,write$391,"for (cell_type, values) in mesh.cells:
    key = binary_dtypes[values.dtype] if binary else '12'
    last_index = first_index + len(values) - 1
    try:
        ansys_cell_type = meshio_to_ansys_type[cell_type]
    except KeyError:
        legal_keys = ', '.join(meshio_to_ansys_type.keys())
        raise KeyError(f""Illegal ANSYS cell type '{cell_type}'. (legal: {legal_keys})"")
    fh.write(f'({key} (1 {first_index:x} {last_index:x} 1 {ansys_cell_type})(\n'.encode())
    if binary:
        (values + first_node_index).tofile(fh)
        fh.write(b'\n)')
        fh.write(f'End of Binary Section {key})\n'.encode())
    else:
        np.savetxt(fh, values + first_node_index, fmt='%x')
        fh.write(b'))\n')
    first_index = last_index + 1","for e_target in mesh.cells:
    values = e_target[1]
    cell_type = e_target[0]
    key = binary_dtypes[values.dtype] if binary else '12'
    last_index = first_index + len(values) - 1
    try:
        ansys_cell_type = meshio_to_ansys_type[cell_type]
    except KeyError:
        legal_keys = ', '.join(meshio_to_ansys_type.keys())
        raise KeyError(f""Illegal ANSYS cell type '{cell_type}'. (legal: {legal_keys})"")
    fh.write(f'({key} (1 {first_index:x} {last_index:x} 1 {ansys_cell_type})(\n'.encode())
    if binary:
        (values + first_node_index).tofile(fh)
        fh.write(b'\n)')
        fh.write(f'End of Binary Section {key})\n'.encode())
    else:
        np.savetxt(fh, values + first_node_index, fmt='%x')
        fh.write(b'))\n')
    first_index = last_index + 1"
model-analysis,https://github.com/tensorflow/model-analysis/tree/master/tensorflow_model_analysis/evaluators/metrics_plots_and_validations_evaluator_test.py,MetricsPlotsAndValidationsEvaluatorTest,testEvaluateWithSlicing$588,"for (slice_key, value) in got:
    slices[slice_key] = value","for e_target in got:
    value = e_target[1]
    slice_key = e_target[0]
    slices[slice_key] = value"
rpaframework,https://github.com/robocorp/rpaframework/tree/master/packages/google/src/RPA/Cloud/Google/keywords/storage.py,StorageKeywords,upload_storage_files$199,"for (target_name, filename) in files.items():
    blob = bucket.blob(target_name)
    blob.upload_from_filename(filename)","for e_target in files.items():
    filename = e_target[1]
    target_name = e_target[0]
    blob = bucket.blob(target_name)
    blob.upload_from_filename(filename)"
DeepPavlov,https://github.com/deepmipt/DeepPavlov/tree/master/deeppavlov/models/preprocessors/squad_preprocessor.py,SquadVocabEmbedder,__call__$236,"for (i, question) in enumerate(questions):
    for (j, token) in enumerate(question):
        for (k, char) in enumerate(token):
            q_idxs[i, j, k] = self._get_idx(char)","for e_target in enumerate(questions):
    question = e_target[1]
    i = e_target[0]
    for (j, token) in enumerate(question):
        for (k, char) in enumerate(token):
            q_idxs[i, j, k] = self._get_idx(char)"
model-analysis,https://github.com/tensorflow/model-analysis/tree/master/tensorflow_model_analysis/eval_saved_model/graph_ref.py,,load_metrics$290,"for (k, v) in metrics.items():
    node = tf.compat.v1.saved_model.utils.get_tensor_from_tensor_info(v, graph)
    if k.endswith('/' + constants.METRIC_VALUE_SUFFIX):
        key = k[:-len(constants.METRIC_VALUE_SUFFIX) - 1]
        metrics_map[key][encoding.VALUE_OP_SUFFIX] = node
    elif k.endswith('/' + constants.METRIC_UPDATE_SUFFIX):
        key = k[:-len(constants.METRIC_UPDATE_SUFFIX) - 1]
        metrics_map[key][encoding.UPDATE_OP_SUFFIX] = node
    else:
        raise ValueError('unrecognised suffix for metric. key was: %s' % k)","for e_target in metrics.items():
    v = e_target[1]
    k = e_target[0]
    node = tf.compat.v1.saved_model.utils.get_tensor_from_tensor_info(v, graph)
    if k.endswith('/' + constants.METRIC_VALUE_SUFFIX):
        key = k[:-len(constants.METRIC_VALUE_SUFFIX) - 1]
        metrics_map[key][encoding.VALUE_OP_SUFFIX] = node
    elif k.endswith('/' + constants.METRIC_UPDATE_SUFFIX):
        key = k[:-len(constants.METRIC_UPDATE_SUFFIX) - 1]
        metrics_map[key][encoding.UPDATE_OP_SUFFIX] = node
    else:
        raise ValueError('unrecognised suffix for metric. key was: %s' % k)"
sympy,https://github.com/sympy/sympy/tree/master/sympy/polys/factortools.py,,dup_gf_factor$1284,"for (i, (f, k)) in enumerate(factors):
    factors[i] = (dup_convert(f, K.dom, K), k)","for e_target in enumerate(factors):
    k = e_target[1][1]
    f = e_target[1][0]
    i = e_target[0]
    factors[i] = (dup_convert(f, K.dom, K), k)"
texar,https://github.com/asyml/texar/tree/master/examples/seq2seq_exposure_bias/scheduled_sampling_main.py,,main$121,"for (key, value) in test_score.items():
    print_stdout_and_file('{}: {}'.format(key, value), file=scores_file)","for e_target in test_score.items():
    value = e_target[1]
    key = e_target[0]
    print_stdout_and_file('{}: {}'.format(key, value), file=scores_file)"
data-science-competition,https://github.com/DLLXW/data-science-competition/tree/master/else/婢垛晠鈹堥弶--AI+z閺呴缚鍏樼拹銊/code/run_ner.py,,read_dataset$71,"for (line_id, line) in enumerate(f):
    if line_id == 0:
        for (i, column_name) in enumerate(line.strip().split('\t')):
            columns[column_name] = i
        continue
    line = line.strip().split('\t')
    labels = line[columns['label']]
    tgt = [args.l2i[l] for l in labels.split(' ')]
    text_a = line[columns['text_a']]
    src = args.tokenizer.convert_tokens_to_ids(args.tokenizer.tokenize(text_a))
    seg = [1] * len(src)
    if len(src) > args.seq_length:
        src = src[:args.seq_length]
        tgt = tgt[:args.seq_length]
        seg = seg[:args.seq_length]
    while len(src) < args.seq_length:
        src.append(0)
        tgt.append(args.labels_num - 1)
        seg.append(0)
    dataset.append([src, tgt, seg])","for e_target in enumerate(f):
    line = e_target[1]
    line_id = e_target[0]
    if line_id == 0:
        for (i, column_name) in enumerate(line.strip().split('\t')):
            columns[column_name] = i
        continue
    line = line.strip().split('\t')
    labels = line[columns['label']]
    tgt = [args.l2i[l] for l in labels.split(' ')]
    text_a = line[columns['text_a']]
    src = args.tokenizer.convert_tokens_to_ids(args.tokenizer.tokenize(text_a))
    seg = [1] * len(src)
    if len(src) > args.seq_length:
        src = src[:args.seq_length]
        tgt = tgt[:args.seq_length]
        seg = seg[:args.seq_length]
    while len(src) < args.seq_length:
        src.append(0)
        tgt.append(args.labels_num - 1)
        seg.append(0)
    dataset.append([src, tgt, seg])"
satellite,https://github.com/Blockstream/satellite/tree/master/blocksatcli/util.py,,ask_multiple_choice$105,"for (i_elem, elem) in enumerate(vec):
    elem_str = to_str(elem)
    print('[%2u] %s' % (i_elem, elem_str))","for e_target in enumerate(vec):
    elem = e_target[1]
    i_elem = e_target[0]
    elem_str = to_str(elem)
    print('[%2u] %s' % (i_elem, elem_str))"
SimSwap,https://github.com/neuralchen/SimSwap/tree/master/util/html.py,HTML,add_images$32,"for (im, txt, link) in zip(ims, txts, links):
    with td(style='word-wrap: break-word;', halign='center', valign='top'):
        with p():
            with a(href=os.path.join('images', link)):
                img(style='width:%dpx' % width, src=os.path.join('images', im))
            br()
            p(txt)","for e_target in zip(ims, txts, links):
    link = e_target[2]
    txt = e_target[1]
    im = e_target[0]
    with td(style='word-wrap: break-word;', halign='center', valign='top'):
        with p():
            with a(href=os.path.join('images', link)):
                img(style='width:%dpx' % width, src=os.path.join('images', im))
            br()
            p(txt)"
cartography,https://github.com/lyft/cartography/tree/master/cartography/intel/aws/iam.py,,load_roles$278,"for (principal_type, principal_value) in principal_entries:
    neo4j_session.run(ingest_policy_statement, SpnArn=principal_value, SpnType=principal_type, RoleArn=role['Arn'], aws_update_tag=aws_update_tag)","for e_target in principal_entries:
    principal_value = e_target[1]
    principal_type = e_target[0]
    neo4j_session.run(ingest_policy_statement, SpnArn=principal_value, SpnType=principal_type, RoleArn=role['Arn'], aws_update_tag=aws_update_tag)"
mindmeld,https://github.com/cisco/mindmeld/tree/master/mindmeld/models/features/query_features.py,,get_exact_span_conflict_features$135,"for (key, value) in features.items():
    feature_sequence[i][feat_prefix + key] = value","for e_target in features.items():
    value = e_target[1]
    key = e_target[0]
    feature_sequence[i][feat_prefix + key] = value"
keras-rl,https://github.com/keras-rl/keras-rl/tree/master/tests/rl/agents/test_dqn.py,,test_naf_layer_full$105,"for (l, l_T, l_flat) in zip(L, LT, L_flat):
    l[np.tril_indices(nb_actions)] = l_flat
    l[np.diag_indices(nb_actions)] = np.exp(l[np.diag_indices(nb_actions)])
    l_T[:, :] = l.T","for e_target in zip(L, LT, L_flat):
    l_flat = e_target[2]
    l_T = e_target[1]
    l = e_target[0]
    l[np.tril_indices(nb_actions)] = l_flat
    l[np.diag_indices(nb_actions)] = np.exp(l[np.diag_indices(nb_actions)])
    l_T[:, :] = l.T"
modin,https://github.com/modin-project/modin/tree/master/modin/experimental/xgboost/xgboost_ray.py,,_split_data_across_actors$288,"for (rank, (_, actor)) in enumerate(actors):
    set_func(actor, *X_parts_by_actors[rank][0] + y_parts_by_actors[rank][0])","for e_target in enumerate(actors):
    actor = e_target[1][1]
    _ = e_target[1][0]
    rank = e_target[0]
    set_func(actor, *X_parts_by_actors[rank][0] + y_parts_by_actors[rank][0])"
fedlearner,https://github.com/bytedance/fedlearner/tree/master/test/trainer/test_nn_trainer.py,TestNNTraining,test_remote_cluster$401,"for (i, addr) in enumerate(leader_cluster_spec['clusterSpec']['PS']):
    psl = _Task(name='RunLeaderPS_%d' % i, target=run_ps, args=(addr,), weight=1, force_quit=True, kwargs={'env': child_env}, daemon=True)
    self.sche.submit(psl)","for e_target in enumerate(leader_cluster_spec['clusterSpec']['PS']):
    addr = e_target[1]
    i = e_target[0]
    psl = _Task(name='RunLeaderPS_%d' % i, target=run_ps, args=(addr,), weight=1, force_quit=True, kwargs={'env': child_env}, daemon=True)
    self.sche.submit(psl)"
dash,https://github.com/plotly/dash/tree/master/components/dash-table/dash_table_base/Format.py,Format,__init__$73,"for (kw, val) in kwargs.items():
    if kw not in valid_methods:
        raise TypeError('{0} is not a format method. Expected one of'.format(kw), str(list(valid_methods)))
    getattr(self, kw)(val)","for e_target in kwargs.items():
    val = e_target[1]
    kw = e_target[0]
    if kw not in valid_methods:
        raise TypeError('{0} is not a format method. Expected one of'.format(kw), str(list(valid_methods)))
    getattr(self, kw)(val)"
hydrus,https://github.com/hydrusnetwork/hydrus/tree/master/hydrus/client/db/ClientDBRepositories.py,ClientDBRepositories,GetRepositoryUpdateHashesICanProcess$361,"for (hash_ids, hashes_and_content_types) in [(definition_hash_ids, definition_hashes_and_content_types), (content_hash_ids, content_hashes_and_content_types)]:
    hashes_and_content_types.extend(((hash_ids_to_hashes[hash_id], hash_ids_to_content_types_to_process[hash_id]) for hash_id in hash_ids))","for e_target in [(definition_hash_ids, definition_hashes_and_content_types), (content_hash_ids, content_hashes_and_content_types)]:
    hashes_and_content_types = e_target[1]
    hash_ids = e_target[0]
    hashes_and_content_types.extend(((hash_ids_to_hashes[hash_id], hash_ids_to_content_types_to_process[hash_id]) for hash_id in hash_ids))"
core,https://github.com/home-assistant/core/tree/master/tests/helpers/test_script.py,,check_action$3300,"for (message, flag) in watch_messages:
    if script_obj.last_action and message in script_obj.last_action:
        flag.set()","for e_target in watch_messages:
    flag = e_target[1]
    message = e_target[0]
    if script_obj.last_action and message in script_obj.last_action:
        flag.set()"
mitmproxy,https://github.com/mitmproxy/mitmproxy/tree/master/mitmproxy/optmanager.py,OptManager,update_known$202,"for (k, v) in known.items():
    self._options[k].set(v)","for e_target in known.items():
    v = e_target[1]
    k = e_target[0]
    self._options[k].set(v)"
organize,https://github.com/tfeldmann/organize/tree/master/tests/filters/test_regex.py,,test_regex_return$27,"for (path, valid, result) in TESTDATA:
    if valid:
        dct = regex.run(path=path)
        assert dct == {'regex': {'the_number': result}}","for e_target in TESTDATA:
    result = e_target[2]
    valid = e_target[1]
    path = e_target[0]
    if valid:
        dct = regex.run(path=path)
        assert dct == {'regex': {'the_number': result}}"
core,https://github.com/home-assistant/core/tree/master/homeassistant/components/homekit/type_remotes.py,RemoteInputSelectAccessory,__init__$81,"for (index, source) in enumerate(self.sources):
    serv_input = self.add_preload_service(SERV_INPUT_SOURCE, [CHAR_IDENTIFIER, CHAR_NAME])
    serv_tv.add_linked_service(serv_input)
    serv_input.configure_char(CHAR_CONFIGURED_NAME, value=source[:MAX_NAME_LENGTH])
    serv_input.configure_char(CHAR_NAME, value=source[:MAX_NAME_LENGTH])
    serv_input.configure_char(CHAR_IDENTIFIER, value=index)
    serv_input.configure_char(CHAR_IS_CONFIGURED, value=True)
    input_type = 3 if 'hdmi' in source.lower() else 0
    serv_input.configure_char(CHAR_INPUT_SOURCE_TYPE, value=input_type)
    serv_input.configure_char(CHAR_CURRENT_VISIBILITY_STATE, value=False)
    _LOGGER.debug('%s: Added source %s', self.entity_id, source)","for e_target in enumerate(self.sources):
    source = e_target[1]
    index = e_target[0]
    serv_input = self.add_preload_service(SERV_INPUT_SOURCE, [CHAR_IDENTIFIER, CHAR_NAME])
    serv_tv.add_linked_service(serv_input)
    serv_input.configure_char(CHAR_CONFIGURED_NAME, value=source[:MAX_NAME_LENGTH])
    serv_input.configure_char(CHAR_NAME, value=source[:MAX_NAME_LENGTH])
    serv_input.configure_char(CHAR_IDENTIFIER, value=index)
    serv_input.configure_char(CHAR_IS_CONFIGURED, value=True)
    input_type = 3 if 'hdmi' in source.lower() else 0
    serv_input.configure_char(CHAR_INPUT_SOURCE_TYPE, value=input_type)
    serv_input.configure_char(CHAR_CURRENT_VISIBILITY_STATE, value=False)
    _LOGGER.debug('%s: Added source %s', self.entity_id, source)"
decision-transformer,https://github.com/kzl/decision-transformer/tree/master/gym/decision_transformer/models/trajectory_gpt2.py,,load_tf_weights_in_gpt2$63,"for (name, shape) in init_vars:
    logger.info('Loading TF weight {} with shape {}'.format(name, shape))
    array = tf.train.load_variable(tf_path, name)
    names.append(name)
    arrays.append(array.squeeze())","for e_target in init_vars:
    shape = e_target[1]
    name = e_target[0]
    logger.info('Loading TF weight {} with shape {}'.format(name, shape))
    array = tf.train.load_variable(tf_path, name)
    names.append(name)
    arrays.append(array.squeeze())"
yt-dlp,https://github.com/yt-dlp/yt-dlp/tree/master/yt_dlp/extractor/lifenews.py,LifeNewsIE,_real_extract$93,"for (num, video_url) in enumerate(video_urls, 1):
    entries.append(make_video_entry(video_id, video_url, num))","for e_target in enumerate(video_urls, 1):
    video_url = e_target[1]
    num = e_target[0]
    entries.append(make_video_entry(video_id, video_url, num))"
networkx,https://github.com/networkx/networkx/tree/master/networkx/algorithms/flow/boykovkolmogorov.py,,boykov_kolmogorov_impl$161,"for (v, attr, d) in sorted(nbrs, key=itemgetter(2)):
    if attr['capacity'] - attr['flow'] > 0:
        if v not in active:
            active.append(v)
    if tree[v] == u:
        tree[v] = None
        orphans.appendleft(v)","for e_target in sorted(nbrs, key=itemgetter(2)):
    d = e_target[2]
    attr = e_target[1]
    v = e_target[0]
    if attr['capacity'] - attr['flow'] > 0:
        if v not in active:
            active.append(v)
    if tree[v] == u:
        tree[v] = None
        orphans.appendleft(v)"
djongo,https://github.com/nesdis/djongo/tree/master/tests/django_tests/tests/v22/tests/admin_widgets/test_autocomplete_widget.py,AutocompleteMixinTests,test_media$113,"for (lang, select_lang) in languages:
    with self.subTest(lang=lang):
        if select_lang:
            expected_files = base_files[:2] + ('admin/js/vendor/select2/i18n/%s.js' % select_lang,) + base_files[2:]
        else:
            expected_files = base_files
        with translation.override(lang):
            self.assertEqual(AutocompleteSelect(rel, admin.site).media._js, list(expected_files))","for e_target in languages:
    select_lang = e_target[1]
    lang = e_target[0]
    with self.subTest(lang=lang):
        if select_lang:
            expected_files = base_files[:2] + ('admin/js/vendor/select2/i18n/%s.js' % select_lang,) + base_files[2:]
        else:
            expected_files = base_files
        with translation.override(lang):
            self.assertEqual(AutocompleteSelect(rel, admin.site).media._js, list(expected_files))"
OpenFermion,https://github.com/quantumlib/OpenFermion/tree/master/src/openfermion/third_party/representability/_namedtensor_test.py,,test_tensor_iterator$113,"for (val, idx) in test_tensor.all_iterator():
    assert val == a[tuple(idx)]
    counter += 1","for e_target in test_tensor.all_iterator():
    idx = e_target[1]
    val = e_target[0]
    assert val == a[tuple(idx)]
    counter += 1"
waitress,https://github.com/Pylons/waitress/tree/master/src/waitress/server.py,TcpWSGIServer,set_socket_options$370,"for (level, optname, value) in self.adj.socket_options:
    conn.setsockopt(level, optname, value)","for e_target in self.adj.socket_options:
    value = e_target[2]
    optname = e_target[1]
    level = e_target[0]
    conn.setsockopt(level, optname, value)"
meta-dataset,https://github.com/google-research/meta-dataset/tree/master/meta_dataset/data/dump_and_read_episodes_test.py,DumpAndReadEpisodesTest,test_read_as_meta_dataset$108,"for (example_string, (img2, label2)) in zip(train_episode, train_ds):
    example = decoder.read_example_and_parse_image(example_string)
    (img1, label1) = (example['image'], example['label'])
    self.assertAllEqual(img1, img2)
    self.assertAllEqual(label1, label2)","for e_target in zip(train_episode, train_ds):
    label2 = e_target[1][1]
    img2 = e_target[1][0]
    example_string = e_target[0]
    example = decoder.read_example_and_parse_image(example_string)
    (img1, label1) = (example['image'], example['label'])
    self.assertAllEqual(img1, img2)
    self.assertAllEqual(label1, label2)"
aws-serverless-ecommerce-platform,https://github.com/aws-samples/aws-serverless-ecommerce-platform/tree/master/shared/lint/rules/custom_rules.py,LambdaInsightsLayer,match$221,"for (key, resource) in functions.items():
    if self._layer_pattern not in resource.get('Properties', {}).get('Layers', []):
        matches.append(RuleMatch(['Resources', key], self._message.format(key)))","for e_target in functions.items():
    resource = e_target[1]
    key = e_target[0]
    if self._layer_pattern not in resource.get('Properties', {}).get('Layers', []):
        matches.append(RuleMatch(['Resources', key], self._message.format(key)))"
Deep-Reinforcement-Learning-Hands-On,https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On/tree/master/Chapter08/train_model.py,,if_main_my$41,"for (key, val) in res.items():
    writer.add_scalar(key + '_test', val, step_idx)","for e_target in res.items():
    val = e_target[1]
    key = e_target[0]
    writer.add_scalar(key + '_test', val, step_idx)"
pytorch_geometric,https://github.com/pyg-team/pytorch_geometric/tree/master/torch_geometric/data/hetero_data.py,HeteroData,__copy__$164,"for (key, store) in self._edge_store_dict.items():
    out._edge_store_dict[key] = copy.copy(store)
    out._edge_store_dict[key]._parent = out","for e_target in self._edge_store_dict.items():
    store = e_target[1]
    key = e_target[0]
    out._edge_store_dict[key] = copy.copy(store)
    out._edge_store_dict[key]._parent = out"
CrypTen,https://github.com/facebookresearch/CrypTen/tree/master/test/test_mpc.py,TestMPC,test_broadcast_matmul$1425,"for (batch1, batch2) in itertools.combinations(batch_dims, 2):
    size1 = (*batch1, *size)
    size2 = (*batch2, *size)
    tensor1 = self._get_random_test_tensor(size=size1, is_float=True)
    tensor2 = self._get_random_test_tensor(size=size2, is_float=True)
    tensor2 = tensor2.transpose(-2, -1)
    encrypted1 = MPCTensor(tensor1)
    encrypted2 = tensor_type(tensor2)
    reference = tensor1.matmul(tensor2)
    encrypted_out = encrypted1.matmul(encrypted2)
    private = isinstance(encrypted2, MPCTensor)
    self._check(encrypted_out, reference, '%s matmul broadcast failed' % ('private' if private else 'public'))
    tensor2 = self._get_random_test_tensor(size=size2, is_float=False)
    tensor2 = tensor2.float().transpose(-2, -1)
    reference = tensor1.matmul(tensor2)
    encrypted_out = encrypted1.matmul(tensor2)
    self._check(encrypted_out, reference, 'matmul broadcast failed with public integer tensor')","for e_target in itertools.combinations(batch_dims, 2):
    batch2 = e_target[1]
    batch1 = e_target[0]
    size1 = (*batch1, *size)
    size2 = (*batch2, *size)
    tensor1 = self._get_random_test_tensor(size=size1, is_float=True)
    tensor2 = self._get_random_test_tensor(size=size2, is_float=True)
    tensor2 = tensor2.transpose(-2, -1)
    encrypted1 = MPCTensor(tensor1)
    encrypted2 = tensor_type(tensor2)
    reference = tensor1.matmul(tensor2)
    encrypted_out = encrypted1.matmul(encrypted2)
    private = isinstance(encrypted2, MPCTensor)
    self._check(encrypted_out, reference, '%s matmul broadcast failed' % ('private' if private else 'public'))
    tensor2 = self._get_random_test_tensor(size=size2, is_float=False)
    tensor2 = tensor2.float().transpose(-2, -1)
    reference = tensor1.matmul(tensor2)
    encrypted_out = encrypted1.matmul(tensor2)
    self._check(encrypted_out, reference, 'matmul broadcast failed with public integer tensor')"
scipy,https://github.com/scipy/scipy/tree/master/scipy/sparse/tests/test_base.py,_TestCommon,test_setdiag_comprehensive$802,"for (m, n) in shapes:
    ks = np.arange(-m + 1, n - 1)
    for k in ks:
        msg = repr((dtype, m, n, k))
        a = np.zeros((m, n), dtype=dtype)
        b = self.spmatrix((m, n), dtype=dtype)
        check_setdiag(a, b, k)
        for k2 in np.random.choice(ks, size=min(len(ks), 5)):
            check_setdiag(a, b, k2)","for e_target in shapes:
    n = e_target[1]
    m = e_target[0]
    ks = np.arange(-m + 1, n - 1)
    for k in ks:
        msg = repr((dtype, m, n, k))
        a = np.zeros((m, n), dtype=dtype)
        b = self.spmatrix((m, n), dtype=dtype)
        check_setdiag(a, b, k)
        for k2 in np.random.choice(ks, size=min(len(ks), 5)):
            check_setdiag(a, b, k2)"
catalyst,https://github.com/scrtlabs/catalyst/tree/master/catalyst/dl/scripts/_misc.py,,parse_config_args$11,"for (key, value) in args._get_kwargs():
    if value is not None:
        if key in ['logdir', 'baselogdir'] and value == '':
            continue
        config['args'][key] = value","for e_target in args._get_kwargs():
    value = e_target[1]
    key = e_target[0]
    if value is not None:
        if key in ['logdir', 'baselogdir'] and value == '':
            continue
        config['args'][key] = value"
MONAI,https://github.com/Project-MONAI/MONAI/tree/master/tests/test_handler_stats.py,TestHandlerStats,test_loss_file$121,"for (idx, line) in enumerate(output_str.split('\n')):
    if grep.match(line):
        if idx in [1, 2, 3, 6, 7, 8]:
            self.assertTrue(has_key_word.match(line))","for e_target in enumerate(output_str.split('\n')):
    line = e_target[1]
    idx = e_target[0]
    if grep.match(line):
        if idx in [1, 2, 3, 6, 7, 8]:
            self.assertTrue(has_key_word.match(line))"
eo-learn,https://github.com/sentinel-hub/eo-learn/tree/master/core/eolearn/core/core_tasks.py,RenameFeatureTask,execute$230,"for (feature_type, feature_name, new_feature_name) in self.feature_gen(eopatch):
    eopatch[feature_type][new_feature_name] = eopatch[feature_type][feature_name]
    del eopatch[feature_type][feature_name]","for e_target in self.feature_gen(eopatch):
    new_feature_name = e_target[2]
    feature_name = e_target[1]
    feature_type = e_target[0]
    eopatch[feature_type][new_feature_name] = eopatch[feature_type][feature_name]
    del eopatch[feature_type][feature_name]"
scipy,https://github.com/scipy/scipy/tree/master/scipy/sparse/_lil.py,lil_matrix,reshape$369,"for (i, row) in enumerate(self.rows):
    for (col, j) in enumerate(row):
        (new_r, new_c) = np.unravel_index(i * ncols + j, shape)
        new[new_r, new_c] = self[i, j]","for e_target in enumerate(self.rows):
    row = e_target[1]
    i = e_target[0]
    for (col, j) in enumerate(row):
        (new_r, new_c) = np.unravel_index(i * ncols + j, shape)
        new[new_r, new_c] = self[i, j]"
kornia,https://github.com/kornia/kornia/tree/master/kornia/augmentation/container/patch.py,PatchSequential,generate_parameters$262,"for (i, nchild) in enumerate(islice(cycle(self.named_children()), batch_shape[0])):
    if isinstance(nchild[1], (_AugmentationBase, MixAugmentationBase, SequentialBase)):
        yield (ParamItem(nchild[0], nchild[1].forward_parameters(torch.Size(batch_shape[1:]))), i)
    else:
        yield (ParamItem(nchild[0], None), i)","for e_target in enumerate(islice(cycle(self.named_children()), batch_shape[0])):
    nchild = e_target[1]
    i = e_target[0]
    if isinstance(nchild[1], (_AugmentationBase, MixAugmentationBase, SequentialBase)):
        yield (ParamItem(nchild[0], nchild[1].forward_parameters(torch.Size(batch_shape[1:]))), i)
    else:
        yield (ParamItem(nchild[0], None), i)"
ansible,https://github.com/ansible/ansible/tree/master/test/lib/ansible_test/_internal/commands/coverage/analyze/targets/missing.py,,find_missing$94,"for (from_path, from_points) in from_data.items():
    if only_exists and (not os.path.isfile(to_bytes(from_path))):
        continue
    to_points = to_data.get(from_path, {})
    for (from_point, from_target_indexes) in from_points.items():
        to_target_indexes = to_points.get(from_point, set())
        remaining_targets = set((from_index[i] for i in from_target_indexes)) - set((to_index[i] for i in to_target_indexes))
        if remaining_targets:
            target_index = target_data.setdefault(from_path, {}).setdefault(from_point, set())
            target_index.update((get_target_index(name, target_indexes) for name in remaining_targets))","for e_target in from_data.items():
    from_points = e_target[1]
    from_path = e_target[0]
    if only_exists and (not os.path.isfile(to_bytes(from_path))):
        continue
    to_points = to_data.get(from_path, {})
    for (from_point, from_target_indexes) in from_points.items():
        to_target_indexes = to_points.get(from_point, set())
        remaining_targets = set((from_index[i] for i in from_target_indexes)) - set((to_index[i] for i in to_target_indexes))
        if remaining_targets:
            target_index = target_data.setdefault(from_path, {}).setdefault(from_point, set())
            target_index.update((get_target_index(name, target_indexes) for name in remaining_targets))"
djongo,https://github.com/nesdis/djongo/tree/master/tests/django_tests/tests/v21/tests/gis_tests/geoapp/tests.py,GeoLookupTest,test_null_geometries_excluded_in_lookups$409,"for (lookup, geom) in queries:
    with self.subTest(lookup=lookup):
        self.assertNotIn(null, State.objects.filter(**{'poly__%s' % lookup: geom}))","for e_target in queries:
    geom = e_target[1]
    lookup = e_target[0]
    with self.subTest(lookup=lookup):
        self.assertNotIn(null, State.objects.filter(**{'poly__%s' % lookup: geom}))"
Deep-Learning,https://github.com/Jack-Cherish/Deep-Learning/tree/master/Discuz/train.py,Discuz,vec2text$191,"for (i, c) in enumerate(char_pos):
    char_at_pos = i
    char_idx = c % self.char_set_len
    if char_idx < 10:
        char_code = char_idx + ord('0')
    elif char_idx < 36:
        char_code = char_idx - 10 + ord('A')
    elif char_idx < 62:
        char_code = char_idx - 36 + ord('a')
    elif char_idx == 62:
        char_code = ord('_')
    else:
        raise ValueError('error')
    text.append(chr(char_code))","for e_target in enumerate(char_pos):
    c = e_target[1]
    i = e_target[0]
    char_at_pos = i
    char_idx = c % self.char_set_len
    if char_idx < 10:
        char_code = char_idx + ord('0')
    elif char_idx < 36:
        char_code = char_idx - 10 + ord('A')
    elif char_idx < 62:
        char_code = char_idx - 36 + ord('a')
    elif char_idx == 62:
        char_code = ord('_')
    else:
        raise ValueError('error')
    text.append(chr(char_code))"
open_model_zoo,https://github.com/openvinotoolkit/open_model_zoo/tree/master/demos/common/python/openvino/model_zoo/model_api/models/hpe_associative_embedding.py,AssociativeEmbeddingDecoder,adjust$248,"for (k, joint) in enumerate(person):
    heatmap = heatmaps[batch_idx, k]
    px = int(joint[0])
    py = int(joint[1])
    if 1 < px < W - 1 and 1 < py < H - 1:
        diff = np.array([heatmap[py, px + 1] - heatmap[py, px - 1], heatmap[py + 1, px] - heatmap[py - 1, px]])
        joint[:2] += np.sign(diff) * 0.25","for e_target in enumerate(person):
    joint = e_target[1]
    k = e_target[0]
    heatmap = heatmaps[batch_idx, k]
    px = int(joint[0])
    py = int(joint[1])
    if 1 < px < W - 1 and 1 < py < H - 1:
        diff = np.array([heatmap[py, px + 1] - heatmap[py, px - 1], heatmap[py + 1, px] - heatmap[py - 1, px]])
        joint[:2] += np.sign(diff) * 0.25"
neural_sp,https://github.com/hirofumi0810/neural_sp/tree/master/neural_sp/models/seq2seq/speech2text.py,Speech2Text,__init__$45,"for (n, p) in self.named_parameters():
    if 'output' in n or 'output_bn' in n or 'linear' in n:
        p.requires_grad = True
    else:
        p.requires_grad = False","for e_target in self.named_parameters():
    p = e_target[1]
    n = e_target[0]
    if 'output' in n or 'output_bn' in n or 'linear' in n:
        p.requires_grad = True
    else:
        p.requires_grad = False"
salt,https://github.com/saltstack/salt/tree/master/salt/states/pkg.py,,_fulfills_version_string$186,"for (operator, version_string) in version_conditions:
    if allow_updates and len(version_conditions) == 1 and (operator == '=='):
        operator = '>='
    fullfills_all = fullfills_all and _fulfills_version_spec([installed_version], operator, version_string, ignore_epoch=ignore_epoch)","for e_target in version_conditions:
    version_string = e_target[1]
    operator = e_target[0]
    if allow_updates and len(version_conditions) == 1 and (operator == '=='):
        operator = '>='
    fullfills_all = fullfills_all and _fulfills_version_spec([installed_version], operator, version_string, ignore_epoch=ignore_epoch)"
mmdetection-mini,https://github.com/hhaAndroid/mmdetection-mini/tree/master/mmdet/models/backbones/ssd_vgg.py,SSDVGG,forward$104,"for (i, layer) in enumerate(self.extra):
    x = F.relu(layer(x), inplace=True)
    if i % 2 == 1:
        outs.append(x)","for e_target in enumerate(self.extra):
    layer = e_target[1]
    i = e_target[0]
    x = F.relu(layer(x), inplace=True)
    if i % 2 == 1:
        outs.append(x)"
PyTorch-YOLOv3,https://github.com/eriklindernoren/PyTorch-YOLOv3/tree/master/pytorchyolo/utils/datasets.py,ListDataset,collate_fn$121,"for (i, boxes) in enumerate(bb_targets):
    boxes[:, 0] = i","for e_target in enumerate(bb_targets):
    boxes = e_target[1]
    i = e_target[0]
    boxes[:, 0] = i"
sparseml,https://github.com/neuralmagic/sparseml/tree/master/src/sparseml/onnx/utils/graph_editor.py,,prune_model_one_shot$400,"for (node, sparsity) in zip(nodes, sparsity):
    (weight, bias) = get_node_params(model, node)
    pruned_weight_val = prune_unstructured(weight.val, sparsity)
    update_model_param(model, weight.name, pruned_weight_val)","for e_target in zip(nodes, sparsity):
    sparsity = e_target[1]
    node = e_target[0]
    (weight, bias) = get_node_params(model, node)
    pruned_weight_val = prune_unstructured(weight.val, sparsity)
    update_model_param(model, weight.name, pruned_weight_val)"
checkmk,https://github.com/tribe29/checkmk/tree/master/cmk/gui/plugins/views/layouts.py,LayoutTable,render$556,"for (index, row) in rows_with_ids:
    if group_cells:
        this_group = group_value(row, group_cells)
        if this_group != last_group:
            if column != 1:
                for _i in range(column - 1, num_columns):
                    html.td('', class_='gap')
                    html.td('', class_='fillup', colspan=num_cells)
                html.close_tr()
                column = 1
            group_open = True
            visible_row_number = 0
            header_is_empty = True
            for cell in group_cells:
                (_tdclass, content) = cell.render(row)
                if content:
                    header_is_empty = False
                    break
            if not header_is_empty:
                html.open_tr(class_='groupheader')
                html.open_td(class_='groupheader', colspan=num_cells * (num_columns + 2) + (num_columns - 1))
                html.open_table(class_='groupheader', cellspacing='0', cellpadding='0', border='0')
                html.open_tr()
                painted = False
                for cell in group_cells:
                    if painted:
                        html.td(',&nbsp;')
                    painted = cell.paint(row)
                html.close_tr()
                html.close_table()
                html.close_td()
                html.close_tr()
                odd = 'odd'
            if view.get('column_headers') != 'off':
                self._show_header_line(cells, num_columns, show_checkboxes)
            last_group = this_group
    if column >= num_columns + 1:
        html.close_tr()
        column = 1
    if column == 1:
        if view.get('column_headers') == 'repeat':
            if visible_row_number > 0 and visible_row_number % repeat_heading_every == 0:
                self._show_header_line(cells, num_columns, show_checkboxes)
        visible_row_number += 1
        if num_columns == 1:
            if not row.get('service_description'):
                state = row.get('host_state', 0)
                if state > 0:
                    state += 1
            else:
                state = row.get('service_state', 0)
        else:
            state = 0
        if index in groups:
            (group_spec, num_grouped_rows) = groups[index]
            group_hidden = grouped_row_title(index, group_spec, num_grouped_rows, odd, num_cells)
            odd = 'even' if odd == 'odd' else 'odd'
        css_classes = []
        hide = ''
        if num_grouped_rows > 0:
            num_grouped_rows -= 1
            if group_hidden:
                hide = 'display:none'
        if group_hidden is not None and num_grouped_rows == 0:
            css_classes.append('group_end')
            group_hidden = None
        odd = 'even' if odd == 'odd' else 'odd'
        if num_columns > 1:
            css_classes.append('multicolumn')
        css_classes += ['%s%d' % (odd, state)]
        html.open_tr(class_=['data'] + css_classes, style=hide)
    else:
        html.open_td(class_='gap')
        html.close_td()
    if show_checkboxes:
        render_checkbox_td(view, row, num_cells)
    for cell in cells:
        cell.paint(row)
    column += 1","for e_target in rows_with_ids:
    row = e_target[1]
    index = e_target[0]
    if group_cells:
        this_group = group_value(row, group_cells)
        if this_group != last_group:
            if column != 1:
                for _i in range(column - 1, num_columns):
                    html.td('', class_='gap')
                    html.td('', class_='fillup', colspan=num_cells)
                html.close_tr()
                column = 1
            group_open = True
            visible_row_number = 0
            header_is_empty = True
            for cell in group_cells:
                (_tdclass, content) = cell.render(row)
                if content:
                    header_is_empty = False
                    break
            if not header_is_empty:
                html.open_tr(class_='groupheader')
                html.open_td(class_='groupheader', colspan=num_cells * (num_columns + 2) + (num_columns - 1))
                html.open_table(class_='groupheader', cellspacing='0', cellpadding='0', border='0')
                html.open_tr()
                painted = False
                for cell in group_cells:
                    if painted:
                        html.td(',&nbsp;')
                    painted = cell.paint(row)
                html.close_tr()
                html.close_table()
                html.close_td()
                html.close_tr()
                odd = 'odd'
            if view.get('column_headers') != 'off':
                self._show_header_line(cells, num_columns, show_checkboxes)
            last_group = this_group
    if column >= num_columns + 1:
        html.close_tr()
        column = 1
    if column == 1:
        if view.get('column_headers') == 'repeat':
            if visible_row_number > 0 and visible_row_number % repeat_heading_every == 0:
                self._show_header_line(cells, num_columns, show_checkboxes)
        visible_row_number += 1
        if num_columns == 1:
            if not row.get('service_description'):
                state = row.get('host_state', 0)
                if state > 0:
                    state += 1
            else:
                state = row.get('service_state', 0)
        else:
            state = 0
        if index in groups:
            (group_spec, num_grouped_rows) = groups[index]
            group_hidden = grouped_row_title(index, group_spec, num_grouped_rows, odd, num_cells)
            odd = 'even' if odd == 'odd' else 'odd'
        css_classes = []
        hide = ''
        if num_grouped_rows > 0:
            num_grouped_rows -= 1
            if group_hidden:
                hide = 'display:none'
        if group_hidden is not None and num_grouped_rows == 0:
            css_classes.append('group_end')
            group_hidden = None
        odd = 'even' if odd == 'odd' else 'odd'
        if num_columns > 1:
            css_classes.append('multicolumn')
        css_classes += ['%s%d' % (odd, state)]
        html.open_tr(class_=['data'] + css_classes, style=hide)
    else:
        html.open_td(class_='gap')
        html.close_td()
    if show_checkboxes:
        render_checkbox_td(view, row, num_cells)
    for cell in cells:
        cell.paint(row)
    column += 1"
scipy,https://github.com/scipy/scipy/tree/master/scipy/sparse/linalg/tests/test_norm.py,TestNorm,test_vector_norm$43,"for (m, a) in ((self.b, 0), (self.b.T, 1)):
    for axis in (a, (a,), a - 2, (a - 2,)):
        assert_allclose(spnorm(m, 1, axis=axis), [7, 6, 7])
        assert_allclose(spnorm(m, np.inf, axis=axis), [4, 3, 4])
        assert_allclose(spnorm(m, axis=axis), v)
        assert_allclose(spnorm(m, ord=2, axis=axis), v)
        assert_allclose(spnorm(m, ord=None, axis=axis), v)","for e_target in ((self.b, 0), (self.b.T, 1)):
    a = e_target[1]
    m = e_target[0]
    for axis in (a, (a,), a - 2, (a - 2,)):
        assert_allclose(spnorm(m, 1, axis=axis), [7, 6, 7])
        assert_allclose(spnorm(m, np.inf, axis=axis), [4, 3, 4])
        assert_allclose(spnorm(m, axis=axis), v)
        assert_allclose(spnorm(m, ord=2, axis=axis), v)
        assert_allclose(spnorm(m, ord=None, axis=axis), v)"
couler,https://github.com/couler-proj/couler/tree/master/couler/core/proto_repr.py,,add_deps_to_step$191,"for (i, step) in enumerate(proto_wf.steps):
    if step.steps[0].name == step_name:
        step_id = i
        break","for e_target in enumerate(proto_wf.steps):
    step = e_target[1]
    i = e_target[0]
    if step.steps[0].name == step_name:
        step_id = i
        break"
nltk,https://github.com/nltk/nltk/tree/master/nltk/sentiment/vader.py,SentimentIntensityAnalyzer,_but_check$453,"for (sidx, sentiment) in enumerate(sentiments):
    if sidx < bi:
        sentiments[sidx] = sentiment * 0.5
    elif sidx > bi:
        sentiments[sidx] = sentiment * 1.5","for e_target in enumerate(sentiments):
    sentiment = e_target[1]
    sidx = e_target[0]
    if sidx < bi:
        sentiments[sidx] = sentiment * 0.5
    elif sidx > bi:
        sentiments[sidx] = sentiment * 1.5"
espnet,https://github.com/espnet/espnet/tree/master/espnet/nets/pytorch_backend/e2e_st_transformer.py,E2E,calculate_all_ctc_probs$563,"for (name, m) in self.named_modules():
    if isinstance(m, CTC) and m.probs is not None:
        ret = m.probs.cpu().numpy()","for e_target in self.named_modules():
    m = e_target[1]
    name = e_target[0]
    if isinstance(m, CTC) and m.probs is not None:
        ret = m.probs.cpu().numpy()"
pytorch-toolbelt,https://github.com/BloodAxe/pytorch-toolbelt/tree/master/pytorch_toolbelt/modules/backbone/wider_resnet.py,WiderResNetA2,__init__$171,"for (mod_id, num) in enumerate(structure):
    blocks = []
    for block_id in range(num):
        if not dilation:
            dil = 1
            stride = 2 if block_id == 0 and 2 <= mod_id <= 4 else 1
        else:
            if mod_id == 3:
                dil = 2
            elif mod_id > 3:
                dil = 4
            else:
                dil = 1
            stride = 2 if block_id == 0 and mod_id == 2 else 1
        if mod_id == 4:
            drop = partial(nn.Dropout2d, p=0.3)
        elif mod_id == 5:
            drop = partial(nn.Dropout2d, p=0.5)
        else:
            drop = None
        blocks.append(('block%d' % (block_id + 1), IdentityResidualBlock(in_channels, channels[mod_id], norm_act=norm_act, stride=stride, dilation=dil, dropout=drop)))
        in_channels = channels[mod_id][-1]
    if mod_id < 2:
        self.add_module('pool%d' % (mod_id + 2), nn.MaxPool2d(3, stride=2, padding=1))
    self.add_module('mod%d' % (mod_id + 2), nn.Sequential(OrderedDict(blocks)))","for e_target in enumerate(structure):
    num = e_target[1]
    mod_id = e_target[0]
    blocks = []
    for block_id in range(num):
        if not dilation:
            dil = 1
            stride = 2 if block_id == 0 and 2 <= mod_id <= 4 else 1
        else:
            if mod_id == 3:
                dil = 2
            elif mod_id > 3:
                dil = 4
            else:
                dil = 1
            stride = 2 if block_id == 0 and mod_id == 2 else 1
        if mod_id == 4:
            drop = partial(nn.Dropout2d, p=0.3)
        elif mod_id == 5:
            drop = partial(nn.Dropout2d, p=0.5)
        else:
            drop = None
        blocks.append(('block%d' % (block_id + 1), IdentityResidualBlock(in_channels, channels[mod_id], norm_act=norm_act, stride=stride, dilation=dil, dropout=drop)))
        in_channels = channels[mod_id][-1]
    if mod_id < 2:
        self.add_module('pool%d' % (mod_id + 2), nn.MaxPool2d(3, stride=2, padding=1))
    self.add_module('mod%d' % (mod_id + 2), nn.Sequential(OrderedDict(blocks)))"
python-for-android,https://github.com/kivy/python-for-android/tree/master/tests/recipes/test_python3.py,TestPython3Recipe,test_create_python_bundle$163,"for (n, (sp_call, kw)) in enumerate(mock_subprocess.call_args_list):
    self.assertEqual(sp_call[0][-1], expected_sp_paths[n])","for e_target in enumerate(mock_subprocess.call_args_list):
    kw = e_target[1][1]
    sp_call = e_target[1][0]
    n = e_target[0]
    self.assertEqual(sp_call[0][-1], expected_sp_paths[n])"
docassemble,https://github.com/jhpyle/docassemble/tree/master/docassemble_base/docassemble/base/parse.py,Question,__init__$1817,"for (key, value) in orig_data.items():
    data[key.lower()] = value","for e_target in orig_data.items():
    value = e_target[1]
    key = e_target[0]
    data[key.lower()] = value"
open-reid,https://github.com/Cysu/open-reid/tree/master/reid/trainers.py,BaseTrainer,train$18,"for (i, inputs) in enumerate(data_loader):
    data_time.update(time.time() - end)
    (inputs, targets) = self._parse_data(inputs)
    (loss, prec1) = self._forward(inputs, targets)
    losses.update(loss.data[0], targets.size(0))
    precisions.update(prec1, targets.size(0))
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    batch_time.update(time.time() - end)
    end = time.time()
    if (i + 1) % print_freq == 0:
        print('Epoch: [{}][{}/{}]\tTime {:.3f} ({:.3f})\tData {:.3f} ({:.3f})\tLoss {:.3f} ({:.3f})\tPrec {:.2%} ({:.2%})\t'.format(epoch, i + 1, len(data_loader), batch_time.val, batch_time.avg, data_time.val, data_time.avg, losses.val, losses.avg, precisions.val, precisions.avg))","for e_target in enumerate(data_loader):
    inputs = e_target[1]
    i = e_target[0]
    data_time.update(time.time() - end)
    (inputs, targets) = self._parse_data(inputs)
    (loss, prec1) = self._forward(inputs, targets)
    losses.update(loss.data[0], targets.size(0))
    precisions.update(prec1, targets.size(0))
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    batch_time.update(time.time() - end)
    end = time.time()
    if (i + 1) % print_freq == 0:
        print('Epoch: [{}][{}/{}]\tTime {:.3f} ({:.3f})\tData {:.3f} ({:.3f})\tLoss {:.3f} ({:.3f})\tPrec {:.2%} ({:.2%})\t'.format(epoch, i + 1, len(data_loader), batch_time.val, batch_time.avg, data_time.val, data_time.avg, losses.val, losses.avg, precisions.val, precisions.avg))"
django,https://github.com/django/django/tree/master/tests/gis_tests/geoapp/test_functions.py,GISFunctionsTests,test_scale$452,"for (c1, c2) in zip(r1.coords, r2.coords):
    self.assertAlmostEqual(c1[0] * xfac, c2[0], tol)
    self.assertAlmostEqual(c1[1] * yfac, c2[1], tol)","for e_target in zip(r1.coords, r2.coords):
    c2 = e_target[1]
    c1 = e_target[0]
    self.assertAlmostEqual(c1[0] * xfac, c2[0], tol)
    self.assertAlmostEqual(c1[1] * yfac, c2[1], tol)"
ottertune,https://github.com/cmu-db/ottertune/tree/master/server/analysis/cluster.py,KMeans,fit$98,"for (j, (row, label)) in enumerate(zip(member_rows, member_labels)):
    dists[j] = cdist(np.expand_dims(row, axis=0), centroid, 'euclidean').squeeze()
    dist_labels.append(label)","for e_target in enumerate(zip(member_rows, member_labels)):
    label = e_target[1][1]
    row = e_target[1][0]
    j = e_target[0]
    dists[j] = cdist(np.expand_dims(row, axis=0), centroid, 'euclidean').squeeze()
    dist_labels.append(label)"
thonny,https://github.com/thonny/thonny/tree/master/thonny/plugins/ast_view.py,AstView,_update$49,"for (field_key, field_value) in fields:
    _format(field_key, field_value, node_id)","for e_target in fields:
    field_value = e_target[1]
    field_key = e_target[0]
    _format(field_key, field_value, node_id)"
fonttools,https://github.com/fonttools/fonttools/tree/master/Lib/fontTools/misc/psOperators.py,PSOperators,ps_forall$489,"for (key, value) in obj.value.items():
    self.push(ps_name(key))
    self.push(value)
    self.call_procedure(proc)","for e_target in obj.value.items():
    value = e_target[1]
    key = e_target[0]
    self.push(ps_name(key))
    self.push(value)
    self.call_procedure(proc)"
cloud-inquisitor,https://github.com/RiotGames/cloud-inquisitor/tree/master/backend/cloud_inquisitor/plugins/types/resources.py,BaseResource,search$214,"for (prop_name, value) in properties.items():
    alias = aliased(ResourceProperty)
    qry = qry.join(alias, Resource.resource_id == alias.resource_id)
    if type(value) == list:
        where_clause = []
        for item in value:
            where_clause.append(alias.value == item)
        qry = qry.filter(and_(alias.name == prop_name, or_(*where_clause)).self_group())
    else:
        qry = qry.filter(and_(alias.name == prop_name, alias.value == value).self_group())","for e_target in properties.items():
    value = e_target[1]
    prop_name = e_target[0]
    alias = aliased(ResourceProperty)
    qry = qry.join(alias, Resource.resource_id == alias.resource_id)
    if type(value) == list:
        where_clause = []
        for item in value:
            where_clause.append(alias.value == item)
        qry = qry.filter(and_(alias.name == prop_name, or_(*where_clause)).self_group())
    else:
        qry = qry.filter(and_(alias.name == prop_name, alias.value == value).self_group())"
HyperGAN,https://github.com/HyperGAN/HyperGAN/tree/master/hypergan/layers/operation.py,Operation,forward$68,"for (layer, layer_name) in zip(self.layers, self.layer_names):
    if layer_name == 'self':
        layer_output = input
    elif isinstance(layer, hg.Layer):
        layer_output = layer(input, context)
    elif layer_name.split(' ')[0] == 'layer':
        layer_output = context[layer_name.split(' ')[1]]
    elif layer_name in context:
        layer_output = context[layer_name]
    else:
        layer_output = layer(input)
    if output is None:
        output = layer_output
    elif self.operation == '+':
        output = output + layer_output
    elif self.operation == '*':
        output = output * layer_output
    elif self.operation == 'cat':
        output = torch.cat([output, layer_output], 1)
    else:
        raise ValidationException('Unknown operation: ' + self.operation)","for e_target in zip(self.layers, self.layer_names):
    layer_name = e_target[1]
    layer = e_target[0]
    if layer_name == 'self':
        layer_output = input
    elif isinstance(layer, hg.Layer):
        layer_output = layer(input, context)
    elif layer_name.split(' ')[0] == 'layer':
        layer_output = context[layer_name.split(' ')[1]]
    elif layer_name in context:
        layer_output = context[layer_name]
    else:
        layer_output = layer(input)
    if output is None:
        output = layer_output
    elif self.operation == '+':
        output = output + layer_output
    elif self.operation == '*':
        output = output * layer_output
    elif self.operation == 'cat':
        output = torch.cat([output, layer_output], 1)
    else:
        raise ValidationException('Unknown operation: ' + self.operation)"
capa,https://github.com/mandiant/capa/tree/master/capa/main.py,,compute_layout$616,"for (rule_name, matches) in capabilities.items():
    rule = rules[rule_name]
    if rule.meta.get('scope') == capa.rules.BASIC_BLOCK_SCOPE:
        for (addr, match) in matches:
            assert addr in functions_by_bb
            matched_bbs.add(addr)","for e_target in capabilities.items():
    matches = e_target[1]
    rule_name = e_target[0]
    rule = rules[rule_name]
    if rule.meta.get('scope') == capa.rules.BASIC_BLOCK_SCOPE:
        for (addr, match) in matches:
            assert addr in functions_by_bb
            matched_bbs.add(addr)"
graphite-api,https://github.com/brutasse/graphite-api/tree/master/graphite_api/functions.py,,divideSeriesLists$664,"for (dividendSeries, divisorSeries) in zip(dividendSeriesList, divisorSeriesList):
    name = 'divideSeries(%s,%s)' % (dividendSeries.name, divisorSeries.name)
    bothSeries = (dividendSeries, divisorSeries)
    step = reduce(lcm, [s.step for s in bothSeries])
    for s in bothSeries:
        s.consolidate(step // s.step)
    start = min([s.start for s in bothSeries])
    end = max([s.end for s in bothSeries])
    end -= (end - start) % step
    values = (safeDiv(v1, v2) for (v1, v2) in zip(*bothSeries))
    quotientSeries = TimeSeries(name, start, end, step, values)
    results.append(quotientSeries)","for e_target in zip(dividendSeriesList, divisorSeriesList):
    divisorSeries = e_target[1]
    dividendSeries = e_target[0]
    name = 'divideSeries(%s,%s)' % (dividendSeries.name, divisorSeries.name)
    bothSeries = (dividendSeries, divisorSeries)
    step = reduce(lcm, [s.step for s in bothSeries])
    for s in bothSeries:
        s.consolidate(step // s.step)
    start = min([s.start for s in bothSeries])
    end = max([s.end for s in bothSeries])
    end -= (end - start) % step
    values = (safeDiv(v1, v2) for (v1, v2) in zip(*bothSeries))
    quotientSeries = TimeSeries(name, start, end, step, values)
    results.append(quotientSeries)"
nilearn,https://github.com/nilearn/nilearn/tree/master/nilearn/plotting/displays.py,TiledSlicer,_init_axes$1468,"for (index, direction) in enumerate(self._cut_displayed):
    fh = self.frame_axes.get_figure()
    axes_coords = self._find_initial_axes_coord(index)
    ax = fh.add_axes(axes_coords, aspect='equal')
    if LooseVersion(matplotlib.__version__) >= LooseVersion('1.6'):
        ax.set_facecolor(facecolor)
    else:
        ax.set_axis_bgcolor(facecolor)
    ax.axis('off')
    coord = self.cut_coords[sorted(self._cut_displayed).index(direction)]
    display_ax = self._axes_class(ax, direction, coord, **kwargs)
    self.axes[direction] = display_ax
    ax.set_axes_locator(self._locator)","for e_target in enumerate(self._cut_displayed):
    direction = e_target[1]
    index = e_target[0]
    fh = self.frame_axes.get_figure()
    axes_coords = self._find_initial_axes_coord(index)
    ax = fh.add_axes(axes_coords, aspect='equal')
    if LooseVersion(matplotlib.__version__) >= LooseVersion('1.6'):
        ax.set_facecolor(facecolor)
    else:
        ax.set_axis_bgcolor(facecolor)
    ax.axis('off')
    coord = self.cut_coords[sorted(self._cut_displayed).index(direction)]
    display_ax = self._axes_class(ax, direction, coord, **kwargs)
    self.axes[direction] = display_ax
    ax.set_axes_locator(self._locator)"
not-youtube-dl,https://github.com/scastillo/not-youtube-dl/tree/master/youtube_dl/extractor/soundcloud.py,SoundcloudIE,add_format$349,"for (k, v) in mobj.groupdict().items():
    if not f.get(k):
        f[k] = v","for e_target in mobj.groupdict().items():
    v = e_target[1]
    k = e_target[0]
    if not f.get(k):
        f[k] = v"
elasticdl,https://github.com/sql-machine-learning/elasticdl/tree/master/elasticdl/python/tests/elasticdl_job_service_test.py,ElasticdlJobServiceTest,_get_args$46,"for (key, value) in self.arguments.items():
    args.append('--' + key)
    args.append(value)","for e_target in self.arguments.items():
    value = e_target[1]
    key = e_target[0]
    args.append('--' + key)
    args.append(value)"
speedtest-cli,https://github.com/sivel/speedtest-cli/tree/master//speedtest.py,Speedtest,download$1516,"for (i, url) in enumerate(urls):
    requests.append(build_request(url, bump=i, secure=self._secure))","for e_target in enumerate(urls):
    url = e_target[1]
    i = e_target[0]
    requests.append(build_request(url, bump=i, secure=self._secure))"
poetry,https://github.com/sheepzh/poetry/tree/master/src/poetry/console/commands/config.py,ConfigCommand,_get_setting$326,"for (key, value) in contents.items():
    if setting and key != setting.split('.')[0]:
        continue
    if isinstance(value, dict) or (key == 'repositories' and k is None):
        if k is None:
            k = ''
        k += re.sub('^config\\.', '', key + '.')
        if setting and len(setting) > 1:
            setting = '.'.join(setting.split('.')[1:])
        values += self._get_setting(value, k=k, setting=setting, default=default)
        k = orig_k
        continue
    if isinstance(value, list):
        value = ', '.join((json.dumps(val) if isinstance(val, list) else val for val in value))
        value = f'[{value}]'
    value = json.dumps(value)
    values.append(((k or '') + key, value))","for e_target in contents.items():
    value = e_target[1]
    key = e_target[0]
    if setting and key != setting.split('.')[0]:
        continue
    if isinstance(value, dict) or (key == 'repositories' and k is None):
        if k is None:
            k = ''
        k += re.sub('^config\\.', '', key + '.')
        if setting and len(setting) > 1:
            setting = '.'.join(setting.split('.')[1:])
        values += self._get_setting(value, k=k, setting=setting, default=default)
        k = orig_k
        continue
    if isinstance(value, list):
        value = ', '.join((json.dumps(val) if isinstance(val, list) else val for val in value))
        value = f'[{value}]'
    value = json.dumps(value)
    values.append(((k or '') + key, value))"
meta-dataset,https://github.com/google-research/meta-dataset/tree/master/meta_dataset/learners/optimization_learners.py,,gradient_descent_step$120,"for (v, dv) in zip(variables, grads):
    if not allow_grads_to_batch_norm_vars and ('offset' in v.name or 'scale' in v.name):
        updated_value = v
    else:
        updated_value = v - learning_rate * dv
        if get_update_ops:
            update_ops.append(tf.assign(v, updated_value))
    v_new.append(updated_value)","for e_target in zip(variables, grads):
    dv = e_target[1]
    v = e_target[0]
    if not allow_grads_to_batch_norm_vars and ('offset' in v.name or 'scale' in v.name):
        updated_value = v
    else:
        updated_value = v - learning_rate * dv
        if get_update_ops:
            update_ops.append(tf.assign(v, updated_value))
    v_new.append(updated_value)"
DALLE-pytorch,https://github.com/lucidrains/DALLE-pytorch/tree/master/dalle_pytorch/reversible.py,,route_args$8,"for (depth, ((f_args, g_args), routes)) in enumerate(zip(routed_args, router[key])):
    (new_f_args, new_g_args) = map(lambda route: {key: val} if route else {}, routes)
    routed_args[depth] = ({**f_args, **new_f_args}, {**g_args, **new_g_args})","for e_target in enumerate(zip(routed_args, router[key])):
    routes = e_target[1][1]
    g_args = e_target[1][0][1]
    f_args = e_target[1][0][0]
    depth = e_target[0]
    (new_f_args, new_g_args) = map(lambda route: {key: val} if route else {}, routes)
    routed_args[depth] = ({**f_args, **new_f_args}, {**g_args, **new_g_args})"
VideoSuperResolution,https://github.com/LoSealL/VideoSuperResolution/tree/master/VSR/Backend/TF/Models/Drrn.py,DRRN,build_loss$65,"for (grad, var) in opt.compute_gradients(loss):
    grads_and_vars.append((tf.clip_by_value(grad, -self.grad_clip / self.learning_rate, self.grad_clip / self.learning_rate), var))","for e_target in opt.compute_gradients(loss):
    var = e_target[1]
    grad = e_target[0]
    grads_and_vars.append((tf.clip_by_value(grad, -self.grad_clip / self.learning_rate, self.grad_clip / self.learning_rate), var))"
PaddleDetection,https://github.com/PaddlePaddle/PaddleDetection/tree/master/ppdet/metrics/mot_metrics.py,KITTIEvaluation,compute3rdPartyMetrics$644,"for (g, ign_g) in zip(seq_trajectories.values(), seq_ignored.values()):
    if all(ign_g):
        n_ignored_tr += 1
        n_ignored_tr_total += 1
        continue
    if all([this == -1 for this in g]):
        tmpML += 1
        self.ML += 1
        continue
    last_id = g[0]
    tracked = 1 if g[0] >= 0 else 0
    lgt = 0 if ign_g[0] else 1
    for f in range(1, len(g)):
        if ign_g[f]:
            last_id = -1
            continue
        lgt += 1
        if last_id != g[f] and last_id != -1 and (g[f] != -1) and (g[f - 1] != -1):
            tmpId_switches += 1
            self.id_switches += 1
        if f < len(g) - 1 and g[f - 1] != g[f] and (last_id != -1) and (g[f] != -1) and (g[f + 1] != -1):
            tmpFragments += 1
            self.fragments += 1
        if g[f] != -1:
            tracked += 1
            last_id = g[f]
    if len(g) > 1 and g[f - 1] != g[f] and (last_id != -1) and (g[f] != -1) and (not ign_g[f]):
        tmpFragments += 1
        self.fragments += 1
    tracking_ratio = tracked / float(len(g) - sum(ign_g))
    if tracking_ratio > 0.8:
        tmpMT += 1
        self.MT += 1
    elif tracking_ratio < 0.2:
        tmpML += 1
        self.ML += 1
    else:
        tmpPT += 1
        self.PT += 1","for e_target in zip(seq_trajectories.values(), seq_ignored.values()):
    ign_g = e_target[1]
    g = e_target[0]
    if all(ign_g):
        n_ignored_tr += 1
        n_ignored_tr_total += 1
        continue
    if all([this == -1 for this in g]):
        tmpML += 1
        self.ML += 1
        continue
    last_id = g[0]
    tracked = 1 if g[0] >= 0 else 0
    lgt = 0 if ign_g[0] else 1
    for f in range(1, len(g)):
        if ign_g[f]:
            last_id = -1
            continue
        lgt += 1
        if last_id != g[f] and last_id != -1 and (g[f] != -1) and (g[f - 1] != -1):
            tmpId_switches += 1
            self.id_switches += 1
        if f < len(g) - 1 and g[f - 1] != g[f] and (last_id != -1) and (g[f] != -1) and (g[f + 1] != -1):
            tmpFragments += 1
            self.fragments += 1
        if g[f] != -1:
            tracked += 1
            last_id = g[f]
    if len(g) > 1 and g[f - 1] != g[f] and (last_id != -1) and (g[f] != -1) and (not ign_g[f]):
        tmpFragments += 1
        self.fragments += 1
    tracking_ratio = tracked / float(len(g) - sum(ign_g))
    if tracking_ratio > 0.8:
        tmpMT += 1
        self.MT += 1
    elif tracking_ratio < 0.2:
        tmpML += 1
        self.ML += 1
    else:
        tmpPT += 1
        self.PT += 1"
py2neo,https://github.com/py2neo-org/py2neo/tree/master/py2neo/vendor/bottle.py,,parse_range_header$2568,"for (start, end) in ranges:
    try:
        if not start:
            (start, end) = (max(0, maxlen - int(end)), maxlen)
        elif not end:
            (start, end) = (int(start), maxlen)
        else:
            (start, end) = (int(start), min(int(end) + 1, maxlen))
        if 0 <= start < end <= maxlen:
            yield (start, end)
    except ValueError:
        pass","for e_target in ranges:
    end = e_target[1]
    start = e_target[0]
    try:
        if not start:
            (start, end) = (max(0, maxlen - int(end)), maxlen)
        elif not end:
            (start, end) = (int(start), maxlen)
        else:
            (start, end) = (int(start), min(int(end) + 1, maxlen))
        if 0 <= start < end <= maxlen:
            yield (start, end)
    except ValueError:
        pass"
taurus,https://github.com/Blazemeter/taurus/tree/master/bzt/cli.py,ConfigOverrider,__apply_mult_override$513,"for (k, v) in obj.items():
    obj[k] = self.__apply_mult_override(v, key, replace_value)","for e_target in obj.items():
    v = e_target[1]
    k = e_target[0]
    obj[k] = self.__apply_mult_override(v, key, replace_value)"
keras,https://github.com/keras-team/keras/tree/master/keras/layers/core/tf_op_layer.py,SlicingOpLambda,_call_wrapper$473,"for (key, value) in kwargs.items():
    value = _dict_to_slice(value)
    if isinstance(value, (list, tuple)):
        new_value = []
        for v in value:
            new_value.append(_dict_to_slice(v))
        value = new_value
    new_kwargs[key] = value","for e_target in kwargs.items():
    value = e_target[1]
    key = e_target[0]
    value = _dict_to_slice(value)
    if isinstance(value, (list, tuple)):
        new_value = []
        for v in value:
            new_value.append(_dict_to_slice(v))
        value = new_value
    new_kwargs[key] = value"
ansible,https://github.com/ansible/ansible/tree/master/test/support/network-integration/collections/ansible_collections/ansible/netcommon/plugins/filter/network.py,,parse_cli$83,"for (name, attrs) in iteritems(spec['keys']):
    value = attrs['value']
    try:
        variables = spec.get('vars', {})
        value = template(value, variables)
    except Exception:
        pass
    if 'start_block' in attrs and 'end_block' in attrs:
        start_block = re.compile(attrs['start_block'])
        end_block = re.compile(attrs['end_block'])
        blocks = list()
        lines = None
        block_started = False
        for line in output.split('\n'):
            match_start = start_block.match(line)
            match_end = end_block.match(line)
            if match_start:
                lines = list()
                lines.append(line)
                block_started = True
            elif match_end:
                if lines:
                    lines.append(line)
                    blocks.append('\n'.join(lines))
                block_started = False
            elif block_started:
                if lines:
                    lines.append(line)
        regex_items = [re.compile(r) for r in attrs['items']]
        objects = list()
        for block in blocks:
            if isinstance(value, Mapping) and 'key' not in value:
                items = list()
                for regex in regex_items:
                    match = regex.search(block)
                    if match:
                        item_values = match.groupdict()
                        item_values['match'] = list(match.groups())
                        items.append(item_values)
                    else:
                        items.append(None)
                obj = {}
                for (k, v) in iteritems(value):
                    try:
                        obj[k] = template(v, {'item': items}, fail_on_undefined=False)
                    except Exception:
                        obj[k] = None
                objects.append(obj)
            elif isinstance(value, Mapping):
                items = list()
                for regex in regex_items:
                    match = regex.search(block)
                    if match:
                        item_values = match.groupdict()
                        item_values['match'] = list(match.groups())
                        items.append(item_values)
                    else:
                        items.append(None)
                key = template(value['key'], {'item': items})
                values = dict([(k, template(v, {'item': items})) for (k, v) in iteritems(value['values'])])
                objects.append({key: values})
        return objects
    elif 'items' in attrs:
        regexp = re.compile(attrs['items'])
        when = attrs.get('when')
        conditional = '{%% if %s %%}True{%% else %%}False{%% endif %%}' % when
        if isinstance(value, Mapping) and 'key' not in value:
            values = list()
            for item in re_matchall(regexp, output):
                entry = {}
                for (item_key, item_value) in iteritems(value):
                    entry[item_key] = template(item_value, {'item': item})
                if when:
                    if template(conditional, {'item': entry}):
                        values.append(entry)
                else:
                    values.append(entry)
            obj[name] = values
        elif isinstance(value, Mapping):
            values = dict()
            for item in re_matchall(regexp, output):
                entry = {}
                for (item_key, item_value) in iteritems(value['values']):
                    entry[item_key] = template(item_value, {'item': item})
                key = template(value['key'], {'item': item})
                if when:
                    if template(conditional, {'item': {'key': key, 'value': entry}}):
                        values[key] = entry
                else:
                    values[key] = entry
            obj[name] = values
        else:
            item = re_search(regexp, output)
            obj[name] = template(value, {'item': item})
    else:
        obj[name] = value","for e_target in iteritems(spec['keys']):
    attrs = e_target[1]
    name = e_target[0]
    value = attrs['value']
    try:
        variables = spec.get('vars', {})
        value = template(value, variables)
    except Exception:
        pass
    if 'start_block' in attrs and 'end_block' in attrs:
        start_block = re.compile(attrs['start_block'])
        end_block = re.compile(attrs['end_block'])
        blocks = list()
        lines = None
        block_started = False
        for line in output.split('\n'):
            match_start = start_block.match(line)
            match_end = end_block.match(line)
            if match_start:
                lines = list()
                lines.append(line)
                block_started = True
            elif match_end:
                if lines:
                    lines.append(line)
                    blocks.append('\n'.join(lines))
                block_started = False
            elif block_started:
                if lines:
                    lines.append(line)
        regex_items = [re.compile(r) for r in attrs['items']]
        objects = list()
        for block in blocks:
            if isinstance(value, Mapping) and 'key' not in value:
                items = list()
                for regex in regex_items:
                    match = regex.search(block)
                    if match:
                        item_values = match.groupdict()
                        item_values['match'] = list(match.groups())
                        items.append(item_values)
                    else:
                        items.append(None)
                obj = {}
                for (k, v) in iteritems(value):
                    try:
                        obj[k] = template(v, {'item': items}, fail_on_undefined=False)
                    except Exception:
                        obj[k] = None
                objects.append(obj)
            elif isinstance(value, Mapping):
                items = list()
                for regex in regex_items:
                    match = regex.search(block)
                    if match:
                        item_values = match.groupdict()
                        item_values['match'] = list(match.groups())
                        items.append(item_values)
                    else:
                        items.append(None)
                key = template(value['key'], {'item': items})
                values = dict([(k, template(v, {'item': items})) for (k, v) in iteritems(value['values'])])
                objects.append({key: values})
        return objects
    elif 'items' in attrs:
        regexp = re.compile(attrs['items'])
        when = attrs.get('when')
        conditional = '{%% if %s %%}True{%% else %%}False{%% endif %%}' % when
        if isinstance(value, Mapping) and 'key' not in value:
            values = list()
            for item in re_matchall(regexp, output):
                entry = {}
                for (item_key, item_value) in iteritems(value):
                    entry[item_key] = template(item_value, {'item': item})
                if when:
                    if template(conditional, {'item': entry}):
                        values.append(entry)
                else:
                    values.append(entry)
            obj[name] = values
        elif isinstance(value, Mapping):
            values = dict()
            for item in re_matchall(regexp, output):
                entry = {}
                for (item_key, item_value) in iteritems(value['values']):
                    entry[item_key] = template(item_value, {'item': item})
                key = template(value['key'], {'item': item})
                if when:
                    if template(conditional, {'item': {'key': key, 'value': entry}}):
                        values[key] = entry
                else:
                    values[key] = entry
            obj[name] = values
        else:
            item = re_search(regexp, output)
            obj[name] = template(value, {'item': item})
    else:
        obj[name] = value"
salt,https://github.com/saltstack/salt/tree/master/salt/roster/clustershell.py,,targets$29,"for (host, addr) in host_addrs.items():
    addr = str(addr)
    ret[host] = copy.deepcopy(__opts__.get('roster_defaults', {}))
    for port in ports:
        try:
            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            sock.settimeout(float(__opts__['ssh_scan_timeout']))
            sock.connect((addr, port))
            sock.shutdown(socket.SHUT_RDWR)
            sock.close()
            ret[host].update({'host': addr, 'port': port})
        except OSError:
            pass","for e_target in host_addrs.items():
    addr = e_target[1]
    host = e_target[0]
    addr = str(addr)
    ret[host] = copy.deepcopy(__opts__.get('roster_defaults', {}))
    for port in ports:
        try:
            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            sock.settimeout(float(__opts__['ssh_scan_timeout']))
            sock.connect((addr, port))
            sock.shutdown(socket.SHUT_RDWR)
            sock.close()
            ret[host].update({'host': addr, 'port': port})
        except OSError:
            pass"
peewee,https://github.com/coleifer/peewee/tree/master//peewee.py,SqliteDatabase,_attach_databases$3463,"for (name, db) in self._attached.items():
    cursor.execute('ATTACH DATABASE ""%s"" AS ""%s""' % (db, name))","for e_target in self._attached.items():
    db = e_target[1]
    name = e_target[0]
    cursor.execute('ATTACH DATABASE ""%s"" AS ""%s""' % (db, name))"
galaxy,https://github.com/ansible/galaxy/tree/master/lib/galaxy/tools/parameters/basic.py,ColumnListParameter,get_column_list$1314,"for (i, col) in enumerate(dataset.metadata.column_types):
    if col == 'int' or col == 'float':
        this_column_list.append(str(i + 1))","for e_target in enumerate(dataset.metadata.column_types):
    col = e_target[1]
    i = e_target[0]
    if col == 'int' or col == 'float':
        this_column_list.append(str(i + 1))"
cloud-custodian,https://github.com/cloud-custodian/cloud-custodian/tree/master/tools/c7n_sentry/c7n_sentry/c7nsentry.py,,orgreplay$351,"for (fname, config) in a['config_files'].items():
    for p in config.get('policies', ()):
        if not p.get('mode'):
            continue
        b = bagger(region=r, assume_role=a['role'], log_group='/aws/lambda/custodian-%s' % p['name'])
        try:
            process_log_group(b)
        except ClientError as e:
            if e.response['Error']['Code']:
                log.info('account: %s region: %s group: %s not found', a['name'], r, b.log_group)
                continue","for e_target in a['config_files'].items():
    config = e_target[1]
    fname = e_target[0]
    for p in config.get('policies', ()):
        if not p.get('mode'):
            continue
        b = bagger(region=r, assume_role=a['role'], log_group='/aws/lambda/custodian-%s' % p['name'])
        try:
            process_log_group(b)
        except ClientError as e:
            if e.response['Error']['Code']:
                log.info('account: %s region: %s group: %s not found', a['name'], r, b.log_group)
                continue"
FedML,https://github.com/FedML-AI/FedML/tree/master/fedml_api/standalone/hierarchical_fl/client.py,Client,train$8,"for (x, labels) in self.local_training_data:
    (x, labels) = (x.to(self.device), labels.to(self.device))
    self.model.zero_grad()
    log_probs = self.model(x)
    loss = self.criterion(log_probs, labels)
    loss.backward()
    optimizer.step()","for e_target in self.local_training_data:
    labels = e_target[1]
    x = e_target[0]
    (x, labels) = (x.to(self.device), labels.to(self.device))
    self.model.zero_grad()
    log_probs = self.model(x)
    loss = self.criterion(log_probs, labels)
    loss.backward()
    optimizer.step()"
anime-downloader,https://github.com/anime-dl/anime-downloader/tree/master/anime_downloader/config.py,_Config,__init__$245,"for (key, val) in from_dict[gkey].items():
    if key not in to_be[gkey].keys():
        to_be[gkey][key] = val
    elif isinstance(from_dict[gkey][key], dict):
        update(key, to_be[gkey], from_dict[gkey])","for e_target in from_dict[gkey].items():
    val = e_target[1]
    key = e_target[0]
    if key not in to_be[gkey].keys():
        to_be[gkey][key] = val
    elif isinstance(from_dict[gkey][key], dict):
        update(key, to_be[gkey], from_dict[gkey])"
FastBERT,https://github.com/autoliuweijie/FastBERT/tree/master//run_fastbert_with_label_smooth.py,,main$207,"for (i, (input_ids_batch, label_ids_batch, mask_ids_batch)) in enumerate(batch_loader(batch_size, input_ids, label_ids, mask_ids)):
    input_ids_batch = input_ids_batch.to(device)
    label_ids_batch = label_ids_batch.to(device)
    mask_ids_batch = mask_ids_batch.to(device)
    with torch.no_grad():
        (loss, logits) = model(input_ids_batch, label_ids_batch, mask_ids_batch)
    logits = nn.Softmax(dim=1)(logits)
    if i == 0:
        logits_all = logits
    if i >= 1:
        logits_all = torch.cat((logits_all, logits), 0)","for e_target in enumerate(batch_loader(batch_size, input_ids, label_ids, mask_ids)):
    mask_ids_batch = e_target[1][2]
    label_ids_batch = e_target[1][1]
    input_ids_batch = e_target[1][0]
    i = e_target[0]
    input_ids_batch = input_ids_batch.to(device)
    label_ids_batch = label_ids_batch.to(device)
    mask_ids_batch = mask_ids_batch.to(device)
    with torch.no_grad():
        (loss, logits) = model(input_ids_batch, label_ids_batch, mask_ids_batch)
    logits = nn.Softmax(dim=1)(logits)
    if i == 0:
        logits_all = logits
    if i >= 1:
        logits_all = torch.cat((logits_all, logits), 0)"
salt,https://github.com/saltstack/salt/tree/master/tests/unit/utils/test_ssdp.py,SSDPBaseTestCase,test_base_protocol_settings$112,"for (key, value) in zip(v_keys, v_vals):
    assert base.DEFAULTS[key] == value","for e_target in zip(v_keys, v_vals):
    value = e_target[1]
    key = e_target[0]
    assert base.DEFAULTS[key] == value"
sofgan,https://github.com/apchenstu/sofgan/tree/master//run_seg_video.py,,main$120,"for (ind, item) in enumerate(files[:-1]):
    path = os.path.join(args.input, item)
    if os.path.isdir(path):
        continue
    cap = cv2.VideoCapture(path)
    if args.save_as_file:
        path_out = os.path.join(args.output, item[:-4] + '_seg')
        if not os.path.exists(path_out):
            os.mkdir(path_out)
    path_out_vis = os.path.join(args.output, 'vis')
    if not os.path.exists(path_out_vis):
        os.mkdir(path_out_vis)
    path_out_vis = os.path.join(path_out_vis, item[:-4] + '.avi')
    out = cv2.VideoWriter(path_out_vis, cv2.VideoWriter_fourcc(*'XVID'), 20, (512, 512))
    (count, flow_test_size) = (0, 512)
    (success, img) = cap.read()
    prvs_bbox = None
    while success:
        img = cv2.resize(img, (512, 512))
        seg = parsing_img(bisNet, img[..., ::-1], to_tensor)
        seg = np.round(seg).astype('uint8')
        dets = detector(img, 1)
        if len(dets) > 0:
            (seg[seg == 4], seg[seg == 5]) = (1, 1)
            shape = predictor(img, dets[0])
            bboxes = parse_68(shape.part)
            if prvs_bbox is not None:
                for k in range(len(bboxes)):
                    ratio = eye_aspect_ratio(bboxes[k])
                    if ratio < 0.18:
                        continue
                    diff = np.abs(bboxes[k] - prvs_bbox[k]) ** 2
                    diff = diff / max(np.max(diff), 1.0) * 0.8
                    bboxes[k] = np.round(diff * bboxes[k] + (1.0 - diff) * prvs_bbox[k]).astype('int')
            for (k, ind) in enumerate([4, 5]):
                cv2.fillConvexPoly(seg, bboxes[k], ind)
        seg = seg[20:512 - 20, 20:512 - 20]
        seg = cv2.resize(seg, (512, 512), interpolation=cv2.INTER_NEAREST)
        seg_vis = vis_condition_img(id_remap(seg))[..., ::-1]
        out.write(seg_vis.astype('uint8'))
        if args.save_as_file:
            cv2.imwrite(os.path.join(path_out, '%05d.png' % count), seg.astype('uint8'))
        (success, img) = cap.read()
        prvs_bbox = bboxes
        count += 1
    cap.release()
    out.release()","for e_target in enumerate(files[:-1]):
    item = e_target[1]
    ind = e_target[0]
    path = os.path.join(args.input, item)
    if os.path.isdir(path):
        continue
    cap = cv2.VideoCapture(path)
    if args.save_as_file:
        path_out = os.path.join(args.output, item[:-4] + '_seg')
        if not os.path.exists(path_out):
            os.mkdir(path_out)
    path_out_vis = os.path.join(args.output, 'vis')
    if not os.path.exists(path_out_vis):
        os.mkdir(path_out_vis)
    path_out_vis = os.path.join(path_out_vis, item[:-4] + '.avi')
    out = cv2.VideoWriter(path_out_vis, cv2.VideoWriter_fourcc(*'XVID'), 20, (512, 512))
    (count, flow_test_size) = (0, 512)
    (success, img) = cap.read()
    prvs_bbox = None
    while success:
        img = cv2.resize(img, (512, 512))
        seg = parsing_img(bisNet, img[..., ::-1], to_tensor)
        seg = np.round(seg).astype('uint8')
        dets = detector(img, 1)
        if len(dets) > 0:
            (seg[seg == 4], seg[seg == 5]) = (1, 1)
            shape = predictor(img, dets[0])
            bboxes = parse_68(shape.part)
            if prvs_bbox is not None:
                for k in range(len(bboxes)):
                    ratio = eye_aspect_ratio(bboxes[k])
                    if ratio < 0.18:
                        continue
                    diff = np.abs(bboxes[k] - prvs_bbox[k]) ** 2
                    diff = diff / max(np.max(diff), 1.0) * 0.8
                    bboxes[k] = np.round(diff * bboxes[k] + (1.0 - diff) * prvs_bbox[k]).astype('int')
            for (k, ind) in enumerate([4, 5]):
                cv2.fillConvexPoly(seg, bboxes[k], ind)
        seg = seg[20:512 - 20, 20:512 - 20]
        seg = cv2.resize(seg, (512, 512), interpolation=cv2.INTER_NEAREST)
        seg_vis = vis_condition_img(id_remap(seg))[..., ::-1]
        out.write(seg_vis.astype('uint8'))
        if args.save_as_file:
            cv2.imwrite(os.path.join(path_out, '%05d.png' % count), seg.astype('uint8'))
        (success, img) = cap.read()
        prvs_bbox = bboxes
        count += 1
    cap.release()
    out.release()"
centerX,https://github.com/JDAI-CV/centerX/tree/master/projects/speedup/pt_inference.py,,if_main_my$92,"for (img, img_bgr, bbox) in zip(images_path, images, bboxes):
    for (cat, score, x1, y1, x2, y2) in bbox:
        color = (255, 0, 0)
        if cat == 1:
            color = (0, 255, 0)
        font = cv2.FONT_HERSHEY_SIMPLEX
        cv2.rectangle(img_bgr, (int(x1), int(y1)), (int(x2), int(y2)), color, 2)
        cv2.putText(img_bgr, str(round(score, 3)), (int(x1), int(y1)), font, 2, (0, 0, 255), 2)
    cv2.imwrite(save_path + img, img_bgr)","for e_target in zip(images_path, images, bboxes):
    bbox = e_target[2]
    img_bgr = e_target[1]
    img = e_target[0]
    for (cat, score, x1, y1, x2, y2) in bbox:
        color = (255, 0, 0)
        if cat == 1:
            color = (0, 255, 0)
        font = cv2.FONT_HERSHEY_SIMPLEX
        cv2.rectangle(img_bgr, (int(x1), int(y1)), (int(x2), int(y2)), color, 2)
        cv2.putText(img_bgr, str(round(score, 3)), (int(x1), int(y1)), font, 2, (0, 0, 255), 2)
    cv2.imwrite(save_path + img, img_bgr)"
pfrl,https://github.com/pfnet/pfrl/tree/master/tests/agents_tests/basetest_training.py,_TestActorLearnerTrainingMixin,_test_actor_learner_training$185,"for (i, call) in enumerate(step_hook.call_args_list):
    (args, kwargs) = call
    assert args[0] is None
    assert args[1] is agent
    assert args[2] == (i + 1) * agent.update_interval","for e_target in enumerate(step_hook.call_args_list):
    call = e_target[1]
    i = e_target[0]
    (args, kwargs) = call
    assert args[0] is None
    assert args[1] is agent
    assert args[2] == (i + 1) * agent.update_interval"
selectivesearch,https://github.com/AlpacaDB/selectivesearch/tree/master/selectivesearch/selectivesearch.py,,_extract_neighbours$194,"for (cur, a) in enumerate(R[:-1]):
    for b in R[cur + 1:]:
        if intersect(a[1], b[1]):
            neighbours.append((a, b))","for e_target in enumerate(R[:-1]):
    a = e_target[1]
    cur = e_target[0]
    for b in R[cur + 1:]:
        if intersect(a[1], b[1]):
            neighbours.append((a, b))"
